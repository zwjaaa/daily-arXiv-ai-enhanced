<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 33]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AI Exchange Platforms](https://arxiv.org/abs/2510.17839)
*Johannes Schneider,Rene Abraham*

Main category: cs.SE

TL;DR: This paper develops a taxonomy for categorizing AI exchange platforms, examining their key dimensions and interaction patterns between research institutions and organizations.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of AI into organizational frameworks has created a need for structured platforms for AI model exchange, but a comprehensive framework to categorize and understand these platforms remains underexplored.

Method: The authors develop a taxonomy that provides a structured approach to categorize AI exchange platforms, examining key dimensions, characteristics, and interaction patterns.

Result: The taxonomy reveals interesting interaction patterns between public research institutions and organizations, including platforms that leverage peer review for quality control and provide mechanisms for online testing, deploying, and customization of models.

Conclusion: The paper serves as a critical resource for understanding AI model exchanges and provides a foundation for further research into their evolution, impact, and best practices, while highlighting the importance of adaptability and innovation in platform design.

Abstract: The rapid integration of Artificial Intelligence (AI) into organizational
technology frameworks has transformed how organizations engage with AI-driven
models, influencing both operational performance and strategic innovation. With
the advent of foundation models, the importance of structured platforms for AI
model exchange has become paramount for organizational efficacy and
adaptability. However, a comprehensive framework to categorize and understand
these platforms remains underexplored. To address this gap, our taxonomy
provides a structured approach to categorize AI exchange platforms, examining
key dimensions and characteristics, as well as revealing interesting
interaction patterns between public research institutions and organizations:
Some platforms leverage peer review as a mechanism for quality control, and
provide mechanisms for online testing, deploying, and customization of models.
Our paper is beneficial to practitioners seeking to understand challenges and
opportunities that arise from AI exchange platforms. For academics, the
taxonomy serves as a foundation for further research into the evolution,
impact, and best practices associated with AI model sharing and utilization in
different contexts. Additionally, our study provides insights into the evolving
role of AI in various industries, highlighting the importance of adaptability
and innovation in platform design. This paper serves as a critical resource for
understanding the dynamic interplay between technology, business models, and
user engagement in the rapidly growing domain of AI model exchanges pointing
also towards possible future evolution.

</details>


### [2] [Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming](https://arxiv.org/abs/2510.17842)
*Vinay Bamil*

Main category: cs.SE

TL;DR: Vibe coding is an AI-native programming paradigm where developers specify functional intent and qualitative descriptors ("vibe") that AI agents transform into executable software, with potential productivity gains but challenges in alignment, reproducibility, and security.


<details>
  <summary>Details</summary>
Motivation: Enable developers to generate software by conversing with AI systems using high-level functional intent and qualitative descriptors rather than writing code directly.

Method: Proposes a reference architecture with intent parser, semantic embedding engine, agentic code generator, and interactive feedback loop; formalizes vibe coding definition and compares with other programming paradigms.

Result: Describes a hypothetical implementation and examines reported productivity gains and democratizing effects, while also identifying vulnerabilities and potential slowdowns.

Conclusion: Vibe coding has implications for software engineering and human-AI collaboration but faces key challenges in alignment, reproducibility, bias, explainability, maintainability, and security that require future research.

Abstract: Recent advances in large language models have enabled developers to generate
software by conversing with artificial intelligence systems rather than writing
code directly. This paper introduces vibe coding, an emerging AI-native
programming paradigm in which a developer specifies high-level functional
intent along with qualitative descriptors of the desired "vibe" (tone, style,
or emotional resonance). An intelligent agent then transforms those
specifications into executable software. We formalize the definition of vibe
coding and propose a reference architecture that includes an intent parser, a
semantic embedding engine, an agentic code generator, and an interactive
feedback loop. A hypothetical implementation is described. We compare vibe
coding with declarative, functional, and prompt-based programming, and we
discuss its implications for software engineering, human-AI collaboration, and
responsible AI practice. Finally, we examine reported productivity gains and
democratizing effects, review recent studies that highlight vulnerabilities and
potential slowdowns, identify key challenges such as alignment,
reproducibility, bias, explainability, maintainability, and security, and
outline future directions and open research questions.

</details>


### [3] [Smart Contracts Formal Verification: A Systematic Literature Review](https://arxiv.org/abs/2510.17865)
*Rene Davila,Everardo Barcenas,Rocio Aldeco-Perez*

Main category: cs.SE

TL;DR: This paper surveys formal verification methods for smart contracts, identifies common errors in smart contract specifications, and proposes an alternative verification approach using description logic.


<details>
  <summary>Details</summary>
Motivation: Smart contracts often contain notable errors in their operation or specifications, which motivates the need for comprehensive formal verification methods to ensure they operate as specified.

Method: The study conducts a focused literature review of related works across various sources, examining specifications, verification tools, and experiments. Based on this survey, the paper proposes an alternative formal verification approach based on description logic.

Result: The survey identifies various existing formal verification methods and tools for smart contracts, highlighting their limitations and the prevalence of errors in smart contract specifications.

Conclusion: Description logic provides a promising alternative approach for formal verification of smart contracts, addressing the identified limitations of existing methods.

Abstract: Formal verification entails testing software to ensure it operates as
specified. Smart contracts are self-executing contracts with the terms of the
agreement directly written into lines of code. They run on blockchain platforms
and automatically enforce and execute the terms of an agreement when meeting
predefined conditions. However, Smart Contracts, as software models, often
contain notable errors in their operation or specifications. This observation
prompts us to conduct a focused study examining related works published across
various sources. These publications detail specifications, verification tools,
and relevant experiments. Subsequently, this survey proposes an alternative
formal verification based on description logic.

</details>


### [4] [UniCode: A Framework for Generating High Quality Competitive Coding Problems](https://arxiv.org/abs/2510.17868)
*Xinyue Zheng,Haowei Lin,Shaofei Cai,Zilong Zheng,Yitao Liang*

Main category: cs.SE

TL;DR: UniCode is a framework that automatically generates algorithmic problems and test cases using LLMs, addressing data contamination and scalability issues in competitive coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of static, human-authored problems in coding benchmarks, including data contamination and limited scalability.

Method: Uses LLMs with three diversification strategies (single problem extension, same-type fusion, cross-type fusion) and stress-driven test case synthesis with brute-force grounding and consensus-based validation.

Result: Created benchmark of 492 problems; evaluated 19 LLMs with top model o4-mini achieving only 70.3% pass rate, showing high challenge and discriminative power.

Conclusion: UniCode provides scalable, reliable solution for generating dynamic evaluation datasets in coding domain.

Abstract: The reliance of competitive coding benchmarks on static, human-authored
problems creates significant challenges, including data contamination and
limited scalability. To address these issues, we introduce UniCode, a novel
framework that automatically generates high-quality algorithmic problems
alongside robust, contamination-resistant test cases. Inspired by biological
evolution that creates better and diverse offspring, our framework leverages
Large Language Models (LLMs) to systematically diversify problems through three
strategies: single problem extension, same-type fusion, and cross-type fusion.
A key innovation is our stress-driven test case synthesis pipeline, which
generates reliable test suites without requiring a canonical ground-truth
solution. This pipeline combines brute-force grounding for small-scale inputs
with a consensus-based validation mechanism for large-scale inputs to ensure
high correctness and coverage. We demonstrate effectiveness of our framework by
curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.
The results reveal that UniCode is highly challenging and discriminative, with
the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our
framework provides a scalable and reliable solution for generating dynamic
evaluation datasets in coding domain.

</details>


### [5] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: A post-tool execution reflection system using LLM-based reflection and domain-specific RAG with troubleshooting documents to repair failed kubectl commands in Kubernetes management.


<details>
  <summary>Details</summary>
Motivation: Tool calls in agentic systems often fail due to semantic errors that can only be identified after execution, requiring a mechanism to repair these errors.

Method: Developed a reflection component combining LLM-based reflection with domain-specific RAG using tool documentation and troubleshooting documents, specifically tested with kubectl commands in Kubernetes.

Result: RAG-based reflection improved kubectl command success rate by 55% for evaluated models and increased correct query answers by 36% on average; troubleshooting documents improved pass rate by 10% over official documentation.

Conclusion: The RAG-based reflection approach effectively repairs tool execution failures, with troubleshooting documents providing significant additional value beyond standard documentation.

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [6] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: TritonRL is a specialized LLM for generating high-performance Triton kernels using supervised fine-tuning and reinforcement learning with robust reward mechanisms to prevent reward hacking.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLMs creates demand for automated, high-performance system kernels to accelerate development and deployment, but Triton kernel generation faces unique challenges including data scarcity and incomplete evaluation criteria.

Method: Combines supervised fine-tuning on curated datasets with reinforcement learning using robust, verifiable rewards and hierarchical reward assignment that detects reward hacking and guides both reasoning traces and code tokens through fine-grained verification.

Result: TritonRL achieves state-of-the-art correctness and speedup on KernelBench, surpassing all other Triton-specific models.

Conclusion: The RL-based training paradigm with robust reward mechanisms effectively enables automated generation of high-quality Triton kernels that can replace existing modules.

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [7] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: Systematic literature review of GenAI tools for code comprehension (2022-2024), revealing challenges like inaccurate explanations and novice prompt struggles, while classifying approaches and evaluating effectiveness.


<details>
  <summary>Details</summary>
Motivation: As programmers increasingly rely on GenAI assistants for code development, there's a growing need for programmers to comprehend GenAI-generated solutions for verification and integration. GenAI also presents new challenges and opportunities for computing education.

Method: Conducted a systematic literature review (SLR) of 31 studies published between 2022-2024, classifying GenAI-based approaches and tools, identifying research methods, and summarizing empirical evaluations.

Result: Despite potential, GenAI assistants often produce inaccurate or unclear explanations, and novice programmers struggle with crafting effective prompts, hindering their ability to leverage GenAI for code comprehension.

Conclusion: The review provides evidence-based guidance for computing educators on using GenAI for code comprehension and identifies future research directions in computing education research and practice.

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [8] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: SpecAgent improves code generation in software repositories by proactively exploring files during indexing to create speculative context, reducing latency while improving quality.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with realistic software repositories due to project-specific APIs and cross-file dependencies, and current retrieval-augmented methods face latency-quality tradeoffs.

Method: Proactive exploration of repository files during indexing to construct speculative context that anticipates future edits, using indexing-time asynchrony to mask latency.

Result: Achieves 9-11% absolute gains (48-58% relative) compared to best-performing baselines while significantly reducing inference latency.

Conclusion: SpecAgent's speculative context approach effectively addresses both latency and quality issues in repository-aware code generation, with a leakage-free benchmark enabling realistic evaluation.

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [9] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: Chart2Code is a new hierarchical benchmark for evaluating chart understanding and code generation in large multimodal models, featuring three difficulty levels with 2,023 tasks across 22 chart types.


<details>
  <summary>Details</summary>
Motivation: To create a user-driven benchmark that captures real-world chart2code scenarios and systematically scales task complexity, addressing the need for practical evaluation of multimodal models.

Method: Hierarchical benchmark with three levels: Level 1 (Chart Reproduction), Level 2 (Chart Editing with complex modifications), and Level 3 (Long-Table to Chart Generation). Uses multi-level evaluation metrics for code correctness and visual fidelity.

Result: Evaluation of 25 SoTA LMMs shows even the best model (GPT-5) performs poorly, averaging only 0.57 on code-based evaluation and 0.22 on chart-quality assessment for editing tasks, highlighting the benchmark's difficulty.

Conclusion: Chart2Code presents a challenging benchmark that will drive advances in multimodal reasoning and help develop more robust LMMs for practical chart generation tasks.

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [10] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: JunoBench is the first benchmark dataset of real-world crashes in Python-based ML notebooks, containing 111 curated crashes from Kaggle with verifiable fixes across popular ML libraries.


<details>
  <summary>Details</summary>
Motivation: There is a lack of debugging tools for ML code in notebooks due to missing benchmarks, despite Jupyter notebooks being widely used for ML prototyping.

Method: Created JunoBench by curating 111 reproducible crashes from public Kaggle notebooks, each with verifiable fixes, covering TensorFlow/Keras, PyTorch, Scikit-learn, Pandas, NumPy, and notebook-specific execution issues.

Result: JunoBench provides a unified execution environment where crashes and fixes can be reliably reproduced, supporting bug detection, localization, and repair for notebook-based ML development.

Conclusion: JunoBench facilitates the development of debugging tools tailored to the interactive nature of notebook-based ML development by providing realistic crashes and their resolutions.

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [11] [DIP-AI: A Discovery Framework for AI Innovation Projects](https://arxiv.org/abs/2510.18017)
*Mariana Crisostomo Martins,Lucas Elias Cardoso Rocha,Lucas Cordeiro Romao,Taciana Novo Kudo,Marcos Kalinowski,Renato de Freitas Bulcao-Neto*

Main category: cs.SE

TL;DR: DIP-AI is a discovery framework for AI innovation projects that combines ISO standards and Design Thinking to improve problem discovery, quality, and stakeholder satisfaction.


<details>
  <summary>Details</summary>
Motivation: Address the lack of support for problem discovery in AI innovation projects within Requirements Engineering, as current methods face challenges in data-intensive AI paradigms.

Method: Proposed DIP-AI framework combining elements of ISO 12207, 5338, and Design Thinking. Evaluated through industry-academia collaboration case study where participants applied the framework in practice.

Result: DIP-AI was found relevant and useful, particularly in facilitating problem discovery in AI projects. Participants provided positive feedback on problem discovery capability and acceptance.

Conclusion: DIP-AI contributes a practical framework for AI problem discovery that benefits both academia and industry, showing promise for improving early-stage exploration in AI innovation initiatives.

Abstract: Despite the increasing development of Artificial Intelligence (AI) systems,
Requirements Engineering (RE) activities face challenges in this new
data-intensive paradigm. We identified a lack of support for problem discovery
within AI innovation projects. To address this, we propose and evaluate DIP-AI,
a discovery framework tailored to guide early-stage exploration in such
initiatives. Based on a literature review, our solution proposal combines
elements of ISO 12207, 5338, and Design Thinking to support the discovery of AI
innovation projects, aiming at promoting higher quality deliveries and
stakeholder satisfaction. We evaluated DIP-AI in an industry-academia
collaboration (IAC) case study of an AI innovation project, in which
participants applied DIP-AI to the discovery phase in practice and provided
their perceptions about the approach's problem discovery capability,
acceptance, and suggestions. The results indicate that DIP-AI is relevant and
useful, particularly in facilitating problem discovery in AI projects. This
research contributes to academia by sharing DIP-AI as a framework for AI
problem discovery. For industry, we discuss the use of this framework in a real
IAC program that develops AI innovation projects.

</details>


### [12] [A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI](https://arxiv.org/abs/2510.18096)
*Esrat Ebtida Sakib,MD Ahnaf Akib,Md Muktadir Mazumder,Maliha Noushin Raida,Md. Mohsinul Kabir*

Main category: cs.SE

TL;DR: This paper presents an automated approach for identifying and classifying Non-Functional Requirements (NFRs) using Large Language Models, with Gemma-2 achieving the best performance metrics.


<details>
  <summary>Details</summary>
Motivation: Manual identification of NFRs from documentation is time-consuming and error-prone, necessitating automated solutions to ensure software meets performance, usability, and reliability expectations.

Method: Built an enhanced NFR dataset from Project Charters and Open Source Software Documentation, then compared classification performance of RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and Llama-3.1-8B using precision, recall, F1-score, and lime scores.

Result: Gemma-2 achieved the best results with precision 0.87, recall 0.89, F1-score 0.88, and lime hit score 78/80. Phi-3 followed closely with precision 0.85, recall 0.87, F1-score 0.86, and highest lime hit score 79.

Conclusion: The integration of enhanced contextual foundation improved model comprehension of technical aspects and user requirements, demonstrating the effectiveness of LLMs for automated NFR classification.

Abstract: Non-Functional Requirements (NFRs) play a critical role in determining the
overall quality and user satisfaction of software systems. Accurately
identifying and classifying NFRs is essential to ensure that software meets
performance, usability, and reliability expectations. However, manual
identification of NFRs from documentation is time-consuming and prone to
errors, necessitating automated solutions. Before implementing any automated
solution, a robust and comprehensive dataset is essential. To build such a
dataset, we collected NFRs from various Project Charters and Open Source
Software Documentation. This enhanced the technical depth and usability of an
already existing NFR dataset. We categorized NFRs into sub-classes and
identified needs using widely used Large Language Models to facilitate
automation. After classifying the NFRs, we compared the classification results
of the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and
Llama-3.1-8B using various evaluation metrics, including precision, recall,
F1-score, and lime scores. Among these models, Gemma-2 achieved the best
results with a precision of 0.87, recall of 0.89, and F1-score of 0.88,
alongside a lime hit score of 78 out of 80. Phi-3 closely followed with a
precision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit
score of 79. By improving the contextual foundation, this integration enhanced
the model's comprehension of technical aspects and user requirements.

</details>


### [13] [BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI](https://arxiv.org/abs/2510.18131)
*Chengquan Guo,Yuzhou Nie,Chulin Xie,Zinan Lin,Wenbo Guo,Bo Li*

Main category: cs.SE

TL;DR: BlueCodeAgent is an end-to-end blue teaming agent that uses automated red teaming to generate diverse risky instances, enabling multi-level defense for code security tasks like bias detection, malicious instruction detection, and vulnerable code detection.


<details>
  <summary>Details</summary>
Motivation: Current LLM code generation security research focuses mainly on red teaming (finding vulnerabilities), while blue teaming (defense) remains limited due to the need for effective semantic understanding to differentiate safe from unsafe code.

Method: Integrates red teaming to generate diverse risky instances, and blue teaming agent that uses these instances for risk detection through constitution and code analysis with agentic integration for multi-level defense, including dynamic analysis for vulnerable code detection.

Result: Achieves 12.7% average F1 score improvement across four datasets in three tasks, with particularly effective reduction of false positives in vulnerable code detection by integrating dynamic analysis.

Conclusion: Red teaming benefits blue teaming by continuously identifying new vulnerabilities to enhance defense performance, demonstrating the effectiveness of the integrated framework for code security.

Abstract: As large language models (LLMs) are increasingly used for code generation,
concerns over the security risks have grown substantially. Early research has
primarily focused on red teaming, which aims to uncover and evaluate
vulnerabilities and risks of CodeGen models. However, progress on the blue
teaming side remains limited, as developing defense requires effective semantic
understanding to differentiate the unsafe from the safe. To fill in this gap,
we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated
red teaming. Our framework integrates both sides: red teaming generates diverse
risky instances, while the blue teaming agent leverages these to detect
previously seen and unseen risk scenarios through constitution and code
analysis with agentic integration for multi-level defense. Our evaluation
across three representative code-related tasks--bias instruction detection,
malicious instruction detection, and vulnerable code detection--shows that
BlueCodeAgent achieves significant gains over the base models and safety
prompt-based defenses. In particular, for vulnerable code detection tasks,
BlueCodeAgent integrates dynamic analysis to effectively reduce false
positives, a challenging problem as base models tend to be over-conservative,
misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average
12.7\% F1 score improvement across four datasets in three tasks, attributed to
its ability to summarize actionable constitutions that enhance context-aware
risk detection. We demonstrate that the red teaming benefits the blue teaming
by continuously identifying new vulnerabilities to enhance defense performance.

</details>


### [14] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: TestPrune is an automated technique that leverages regression tests for bug reproduction and patch validation by minimizing test suites to overcome LLM context limits and reduce noise/costs in debugging pipelines.


<details>
  <summary>Details</summary>
Motivation: Real-world test suites are large and achieve high coverage but still miss bugs, while regression tests can serve dual purposes for debugging current versions and validating patches.

Method: Automatically minimizes regression test suites to small, relevant subsets that can be used for bug reproduction and patch validation in agentic bug repair pipelines.

Result: 6.2%-9.0% increase in issue reproduction rate and 9.4%-12.9% increase in issue resolution rate on SWE-Bench benchmarks, with minimal cost overhead of $0.02-$0.05 per instance.

Conclusion: TestPrune effectively improves debugging performance by strategically reusing regression tests, overcoming LLM context limitations while maintaining low costs.

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [15] [Ensuring Robustness in ML-enabled Software Systems: A User Survey](https://arxiv.org/abs/2510.18292)
*Hala Abdelkader,Mohamed Abdelrazek,Priya Rani,Rajesh Vasa,Jean-Guy Schneider*

Main category: cs.SE

TL;DR: ML-On-Rails protocol is a unified framework that addresses robustness challenges in ML systems through OOD detection, adversarial attack detection, input validation, explainability, and standardized communication using HTTP status codes.


<details>
  <summary>Details</summary>
Motivation: Traditional software engineering practices are insufficient for ML components due to silent failures, OOD data, and adversarial attacks. A practitioner survey revealed major robustness gaps in current ML systems.

Method: Proposed ML-On-Rails protocol integrating multiple safeguards and a model-to-software communication framework. Conducted practitioner survey to validate real-world challenges.

Result: Survey identified critical robustness issues and gaps in current solutions. ML-On-Rails protocol provides standardized approach to enhance system robustness and transparency.

Conclusion: There is a need for more support for ML engineers. Future work will refine the protocol based on survey insights and real-world applications to improve effectiveness.

Abstract: Ensuring robustness in ML-enabled software systems requires addressing
critical challenges, such as silent failures, out-of-distribution (OOD) data,
and adversarial attacks. Traditional software engineering practices, which rely
on predefined logic, are insufficient for ML components that depend on data and
probabilistic decision-making. To address these challenges, we propose the
ML-On-Rails protocol, a unified framework designed to enhance the robustness
and trustworthiness of ML-enabled systems in production. This protocol
integrates key safeguards such as OOD detection, adversarial attack detection,
input validation, and explainability. It also includes a model-to-software
communication framework using HTTP status codes to enhance transparency in
reporting model outcomes and errors. To align our approach with real-world
challenges, we conducted a practitioner survey, which revealed major robustness
issues, gaps in current solutions, and highlighted how a standardised protocol
such as ML-On-Rails can improve system robustness. Our findings highlight the
need for more support and resources for engineers working with ML systems.
Finally, we outline future directions for refining the proposed protocol,
leveraging insights from the survey and real-world applications to continually
enhance its effectiveness.

</details>


### [16] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: InspectCoder is an agentic program repair system that enables LLMs to conduct dynamic analysis through interactive debugger control, achieving significant improvements in repair accuracy and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based self-repair approaches rely on static semantic analysis or superficial execution logs, missing the in-depth runtime behaviors that expose bug root causes, and lack the interactive dynamic analysis capabilities that make human debugging effective.

Method: A dual-agent framework that enables strategic breakpoint placement, targeted state inspection, and incremental runtime experimentation within stateful debugger sessions. It adaptively inspects and perturbs relevant intermediate states at runtime, leveraging immediate process rewards from debugger feedback to guide multi-step reasoning.

Result: Achieved 5.10%-60.37% relative improvements in repair accuracy over the strongest baseline on BigCodeBench-R and LiveCodeBench-R benchmarks, with 1.67x-2.24x superior bug-fix efficiency. Also developed InspectWare, an open-source middleware for debugger abstraction.

Conclusion: The work demonstrates significant potential of LLM-driven dynamic analysis for automated software engineering, transforming LLM debugging from blind trial-and-error into systematic root cause diagnosis through interactive debugger systems.

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [17] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: This study analyzes differences between human-written and GenAI computational notebooks, finding that human notebooks show more structural diversity and innovation while GenAI achieves better code quality metrics.


<details>
  <summary>Details</summary>
Motivation: With the rise of generative AI, it's important to distinguish human-written vs GenAI content in competitive data science settings like Kaggle notebooks.

Method: Three case studies analyzing 25 code and documentation features in human-written Kaggle notebooks, plus comparison between human-written and GenAI notebooks.

Result: Gold medalists distinguished by longer documentation; GenAI notebooks have higher code quality but human notebooks show more structural diversity, complexity, and innovation.

Conclusion: The study provides groundwork for four research agendas to maximize human-AI collaboration in computational notebooks.

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [18] [Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study](https://arxiv.org/abs/2510.18448)
*Wenjing Dang,Kaixuan Li,Sen Chen,Zhenwei Zhuo,Lyuye Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: First large-scale study on vulnerability Proof-of-Concepts (PoCs) analyzing 470,921 PoCs from 13 platforms, revealing 78.9% of CVE vulnerabilities lack PoCs and existing reports miss 30% essential components.


<details>
  <summary>Details</summary>
Motivation: Research on PoCs significantly lags behind vulnerability studies due to challenges like PoC dispersion across platforms, diverse writing styles, and reproduction difficulties, creating a gap in understanding PoC availability and quality.

Method: 1) Collected 470,921 PoCs from 13 platforms to investigate availability; 2) Proposed component extraction method combining pattern-matching and BERT-NER model to extract 9 key components; 3) Manual reproduction of 150 vulnerabilities by 8 participants to analyze reproducibility.

Result: 78.9% of CVE vulnerabilities lack available PoCs; existing PoC reports miss about 30% essential components; identified various reasons for reproduction failures; proposed actionable strategies to enhance PoC usability.

Conclusion: Significant gaps exist in PoC availability and quality, highlighting the need for improved PoC reporting standards and practices to better support vulnerability validation and software security.

Abstract: The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its
existence, mitigating false positives, and illustrating the severity of the
security threat it poses. However, research on PoCs significantly lags behind
studies focusing on vulnerability data. This discrepancy can be directly
attributed to several challenges, including the dispersion of real-world PoCs
across multiple platforms, the diversity in writing styles, and the difficulty
associated with PoC reproduction. To fill this gap, we conduct the first
large-scale study on PoCs in the wild, assessing their report availability,
completeness, reproducibility. Specifically, 1) to investigate PoC reports
availability for CVE vulnerability, we collected an extensive dataset of
470,921 PoCs and their reports from 13 platforms, representing the broadest
collection of publicly available PoCs to date. 2) To assess the completeness of
PoC report at a fine-grained level, we proposed a component extraction method,
which combines pattern-matching techniques with a fine-tuned BERT-NER model to
extract 9 key components from PoC reports. 3) To evaluate the effectiveness of
PoCs, we recruited 8 participants to manually reproduce 150 sampled
vulnerabilities with 32 vulnerability types based on PoC reports, enabling an
in-depth analysis of PoC reproducibility and the factors influencing it. Our
findings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and
existing PoC reports typically miss about 30% of the essential components
required for effective vulnerability understanding and reproduction, with
various reasons identified for the failure to reproduce vulnerabilities using
available PoC reports. Finally, we proposed actionable strategies for
stakeholders to enhance the overall usability of vulnerability PoCs in
strengthening software security.

</details>


### [19] [Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research](https://arxiv.org/abs/2510.18456)
*Cristina Martinez Montes,Robert Feldt,Cristina Miguel Martos,Sofia Ouhbi,Shweta Premanandan,Daniel Graziotin*

Main category: cs.SE

TL;DR: This paper presents a reproducible method for integrating LLMs into thematic analysis workflows, with systematic evaluation showing LLMs can generate useful codes but have limitations in latent interpretation and theme boundaries.


<details>
  <summary>Details</summary>
Motivation: No reproducible methods exist for integrating LLMs into established qualitative approaches like thematic analysis, and existing studies lack systematic evaluation of LLM-generated qualitative outputs against established quality criteria.

Method: Designed and iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA, tested outputs from multiple LLMs against codes and themes produced by experienced researchers using 15 interviews on software engineers' well-being, and conducted blind evaluations with four expert evaluators using rubrics derived from Braun and Clarke's quality criteria.

Result: Evaluators preferred LLM-generated codes 61% of the time, finding them analytically useful for answering research questions. However, LLMs fragmented data unnecessarily, missed latent interpretations, and sometimes produced themes with unclear boundaries.

Conclusion: Provides a reproducible approach for integrating LLMs into qualitative analysis with guidelines for when and how LLMs can assist effectively while preserving methodological rigour, and clarifies when human interpretation remains essential.

Abstract: As artificial intelligence advances, large language models (LLMs) are
entering qualitative research workflows, yet no reproducible methods exist for
integrating them into established approaches like thematic analysis (TA), one
of the most common qualitative methods in software engineering research.
Moreover, existing studies lack systematic evaluation of LLM-generated
qualitative outputs against established quality criteria. We designed and
iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,
then tested outputs from multiple LLMs against codes and themes produced by
experienced researchers. Using 15 interviews on software engineers' well-being,
we conducted blind evaluations with four expert evaluators who applied rubrics
derived directly from Braun and Clarke's quality criteria. Evaluators preferred
LLM-generated codes 61% of the time, finding them analytically useful for
answering the research question. However, evaluators also identified
limitations: LLMs fragmented data unnecessarily, missed latent interpretations,
and sometimes produced themes with unclear boundaries. Our contributions are
threefold. First, a reproducible approach integrating refined, documented
prompts with an evaluation framework to operationalize Braun and Clarke's
reflexive TA. Second, an empirical comparison of LLM- and human-generated codes
and themes in software engineering data. Third, guidelines for integrating LLMs
into qualitative analysis while preserving methodological rigour, clarifying
when and how LLMs can assist effectively and when human interpretation remains
essential.

</details>


### [20] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: CodeRL+ integrates execution semantics alignment into RLVR training for code generation, enabling models to infer variable-level execution trajectories and providing direct learning signals for execution semantics.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR approaches rely on binary pass/fail test signals, which are inefficient for bridging the semantic gap between textual code patterns and functional correctness governed by formal execution semantics, especially for subtle logical errors.

Method: CodeRL+ constructs execution semantics alignment directly from on-policy rollouts, enabling models to infer variable-level execution trajectories. It integrates seamlessly with various RL algorithms and uses execution semantics as direct learning signals.

Result: CodeRL+ achieves 4.6% average relative improvement in pass@1 compared to post-training baselines, generalizes to other coding tasks with 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, and shows strong applicability across diverse RL algorithms and LLMs.

Conclusion: CodeRL+ effectively bridges the semantic gap between textual code representations and execution semantics, providing compelling evidence of strengthened alignment between code's textual representations and underlying execution semantics.

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [21] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: VAPU is an LLM-based multi-agent system that autonomously updates legacy applications through phased code updates, showing up to 22.5% better performance than traditional ZSL/OSL approaches.


<details>
  <summary>Details</summary>
Motivation: Legacy applications contain deprecated components causing compatibility, security, and reliability risks, but high resource costs prevent companies from updating them. There's a need for cost-effective autonomous solutions.

Method: Proposed VAPU system with multi-agent pipeline simulating software development team roles. Extended evaluation from single LLM to five LLMs with temperature parameters (0-1), tested on 20 open-source Python GitHub projects and compared to ZSL/OSL prompts.

Result: VAPU achieved similar error counts to ZSL/OSL but with higher fulfilled requirements, showing up to 22.5% increase in successful Python file updates. Low-temperature VAPU performed particularly well.

Conclusion: LLM-based multi-agent systems are capable solutions for autonomously updating legacy application components, providing a cost-effective alternative to manual updates.

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [22] [Mining Service Behavior for Stateful Service Emulation](https://arxiv.org/abs/2510.18519)
*Md Arafat Hossain,Jun Han,Muhammad Ashad Kabir,Steve Versteeg,Jean-Guy Schneider,Jiaojiao Jiang*

Main category: cs.SE

TL;DR: Proposes a service virtualization approach that considers service state to improve response generation accuracy for testing enterprise systems.


<details>
  <summary>Details</summary>
Motivation: Existing service virtualization methods fail to account for service state, reducing emulation accuracy and testing realism, especially for stateful services.

Method: Derives service models from interactions by uncovering contextual dependencies among messages and analyzing relationships between message data values.

Result: Evaluation shows notable improvements in accuracy and efficiency over existing approaches for both stateful and stateless services.

Conclusion: Considering service state in service virtualization significantly enhances response generation accuracy and testing environment realism.

Abstract: Enterprise software systems are increasingly integrating with diverse
services to meet expanding business demands. Testing these highly
interconnected systems presents a challenge due to the need for access to the
connected services. Service virtualization has emerged as a widely used
technique to derive service models from recorded interactions, for service
response generation during system testing. Various methods have been proposed
to emulate actual service behavior based on these interactions, but most fail
to account for the service's state, which reduces the accuracy of service
emulation and the realism of the testing environment, especially when dealing
with stateful services. This paper proposes an approach to deriving service
models from service interactions, which enhance the accuracy of response
generation by considering service state. This is achieved by uncovering
contextual dependencies among interaction messages and analyzing the
relationships between message data values. The approach is evaluated using
interaction traces collected from both stateful and stateless services, and the
results reveal notable enhancements in accuracy and efficiency over existing
approaches in service response generation.

</details>


### [23] [Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design](https://arxiv.org/abs/2510.18534)
*Uraz Odyurt,Richard Loendersloot,Tiedo Tinga*

Main category: cs.SE

TL;DR: Proposes a demonstrator requirements elaboration framework to address uncertainties in research project demonstrators, defining 5 hierarchical levels of demonstration connected to project expectations and industrial use-cases.


<details>
  <summary>Details</summary>
Motivation: Address challenges in research project organization where demonstrator coverage is often an afterthought, leading to confusion and mismatches between targeted and achievable results.

Method: Developed a demonstrator requirements elaboration framework with 5 hierarchical levels of demonstration, connected to work package interactions and industrial use-cases.

Result: Applied framework to two research projects (one early stage, one final stage) showing effectiveness, though complete validation requires 4-5 years of observation.

Conclusion: The proposed framework helps evaluate demonstration feasibility, make realistic adjustments, and assist in requirement description for software-intensive systems and industrial cyber-physical systems.

Abstract: One of the challenges apparent in the organisation of research projects is
the uncertainties around the subject of demonstrators. A precise and detailed
elicitation of the coverage for project demonstrators is often an afterthought
and not sufficiently detailed during proposal writing. This practice leads to
continuous confusion and a mismatch between targeted and achievable
demonstration of results, hindering progress. The reliance on the TRL scale as
a loose descriptor does not help either. We propose a demonstrator requirements
elaboration framework aiming to evaluate the feasibility of targeted
demonstrations, making realistic adjustments, and assist in describing
requirements. In doing so, we define 5 hierarchical levels of demonstration,
clearly connected to expectations, e.g., work package interaction, and also
connected to the project's industrial use-cases. The considered application
scope in this paper is the domain of software-intensive systems and industrial
cyber-physical systems. A complete validation is not accessible, as it would
require application of our framework at the start of a project and observing
the results at the end, taking 4-5 years. Nonetheless, we have applied it to
two research projects from our portfolio, one at the early and another at the
final stages, revealing its effectiveness.

</details>


### [24] [When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software](https://arxiv.org/abs/2510.18557)
*Jianjun Zhao*

Main category: cs.SE

TL;DR: Quantum programs have different semantics than classical programs, making classical abstraction mechanisms unsafe. This paper identifies conflicts between abstraction and quantum constraints, proposes design principles for sound abstraction, and suggests research directions.


<details>
  <summary>Details</summary>
Motivation: Classical abstraction mechanisms don't account for quantum-specific constraints like unitarity, entanglement, no-cloning theorem, and destructive measurement, which can lead to semantically invalid quantum programs despite syntactically valid abstractions.

Method: The paper identifies three classes of failure cases where naive abstraction breaks quantum semantics and proposes design principles for physically sound abstraction mechanisms.

Result: The analysis reveals fundamental conflicts between classical abstraction practices and quantum physical constraints, demonstrating that syntactically valid abstractions can violate quantum computation principles.

Conclusion: There is a need for systematic rethinking of abstraction in quantum software engineering based on quantum semantics, with proposed research directions including quantum-specific type systems, effect annotations, and contract-based module design.

Abstract: Abstraction is a fundamental principle in classical software engineering,
which enables modularity, reusability, and scalability. However, quantum
programs adhere to fundamentally different semantics, such as unitarity,
entanglement, the no-cloning theorem, and the destructive nature of
measurement, which introduce challenges to the safe use of classical
abstraction mechanisms. This paper identifies a fundamental conflict in quantum
software engineering: abstraction practices that are syntactically valid may
violate the physical constraints of quantum computation. We present three
classes of failure cases where naive abstraction breaks quantum semantics and
propose a set of design principles for physically sound abstraction mechanisms.
We further propose research directions, including quantum-specific type
systems, effect annotations, and contract-based module design. Our goal is to
initiate a systematic rethinking of abstraction in quantum software
engineering, based on quantum semantics and considering engineering
scalability.

</details>


### [25] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: WebDevJudge is a benchmark for evaluating LLM-as-a-judge in web development tasks, revealing significant gaps between LLM judges and human experts due to fundamental model limitations.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLM-as-a-judge in open-ended tasks with dynamic environments and complex interactions, which remains unexplored despite its strong performance on well-defined tasks.

Method: Created WebDevJudge benchmark with human preference labels over paired web implementations, using structured and query-grounded rubrics. Evaluated various evaluators including LLMs, MLLMs, and agentic workflows, investigating different paradigms and guidance mechanisms.

Result: Experiments revealed significant gap between LLM judges and human experts, with failures in recognizing functional equivalence, verifying task feasibility, and mitigating bias.

Conclusion: WebDevJudge presents a significant challenge to LLM-as-a-judge and offers insights to guide future research toward developing more reliable automated evaluators for complicated scenarios.

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [26] [A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation](https://arxiv.org/abs/2510.18590)
*Antonio Lamanna*

Main category: cs.SE

TL;DR: A comprehensive evaluation framework for Low-Code Development Platforms (LCDPs) using five key criteria and a weighted scoring model to improve platform selection decisions.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of LCDPs creates a need for systematic evaluation methodologies beyond marketing-driven comparisons, addressing the gap between platform marketing and rigorous context-specific evaluation.

Method: Proposes a framework based on five criteria (Business Process Orchestration, UI/UX Customization, Integration and Interoperability, Governance and Security, AI-Enhanced Automation) with a weighted scoring model for quantitative assessment.

Result: Empirical validation in enterprise environments demonstrates that this structured approach significantly improves decision-making outcomes and reduces platform lock-in or inadequate selection risks.

Conclusion: The framework provides organizations with a systematic methodology to quantitatively evaluate and compare low-code platforms based on specific requirements and strategic priorities.

Abstract: The rapid adoption of Low-Code Development Platforms (LCDPs) has created a
critical need for systematic evaluation methodologies that enable organizations
to make informed platform selection decisions. This paper presents a
comprehensive evaluation framework based on five key criteria: Business Process
Orchestration, UI/UX Customization, Integration and Interoperability,
Governance and Security, and AI-Enhanced Automation. We propose a weighted
scoring model that allows organizations to quantitatively assess and compare
different low-code platforms based on their specific requirements and strategic
priorities. The framework addresses the gap between marketing-driven platform
comparisons and rigorous, context-specific evaluation methodologies. Through
empirical validation in enterprise environments, we demonstrate how this
structured approach can significantly improve decision-making outcomes and
reduce the risk of platform lock-in or inadequate solution selection.

</details>


### [27] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: CUARewardBench is the first comprehensive benchmark for evaluating reward models on computer-using agents, addressing limitations of script-based verifiers through systematic assessment of outcome and process reward models across diverse software categories.


<details>
  <summary>Details</summary>
Motivation: Current script-based verifiers for computer-using agents have limited scalability and lack step-wise assessment capabilities, while reward models remain underexplored for CUA evaluation.

Method: Developed CUARewardBench with trajectories from 10 software categories and 7 agent architectures, expert annotations, and proposed Unanimous Prompt Ensemble (UPE) method using strict unanimous voting and strategic prompt configurations.

Result: UPE achieved 89.8% precision and 93.3% NPV for outcome reward models, and 81.7% precision and 85.1% NPV for process reward models, significantly outperforming single vision-language models and traditional ensemble approaches.

Conclusion: CUARewardBench enables systematic evaluation of CUA reward models, reveals critical limitations of current approaches, and demonstrates that UPE significantly enhances reward model reliability for computer-using agent evaluation.

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [28] [An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences](https://arxiv.org/abs/2510.18711)
*Bertha Ngereja,Magne Jørgensen*

Main category: cs.SE

TL;DR: Alternative funding and contracting approaches can better align with agile software development principles by addressing the rigidity of traditional methods, though they require careful implementation to manage risks.


<details>
  <summary>Details</summary>
Motivation: Traditional funding and contracting approaches are too rigid and conflict with agile principles, hindering collaboration and limiting profitability.

Method: A systematic literature review of 38 peer-reviewed empirical studies from SCOPUS, Web of Science, and Google Scholar, covering both private and public sectors.

Result: Identified four alternative funding and four alternative contracting approaches that led to higher client satisfaction, reduced contractor risk, and more efficient resource utilization.

Conclusion: Organizations should start with hybrid approaches that balance flexibility and control, then progressively transition to fully flexible approaches tailored to their specific context and readiness.

Abstract: Agile software development emphasizes flexibility and iterative processes,
which may conflict with the more linear, rigid, and time-consuming traditional
funding and contracting approaches. This review synthesizes real-life
experiences of using alternative (non-traditional) contracting and funding
approaches. The focus is on identifying approaches that align better with agile
principles and understanding the motivations, benefits, and challenges these
alternatives present. A systematic literature review was conducted in SCOPUS,
Web of Science, and Google Scholar, where we identified 38 relevant
peer-reviewed empirical studies from private and public sector contexts. Four
alternative funding and four alternative contracting approaches were
identified. Organizations were motivated to adopt these alternative approaches
because traditional approaches often proved too rigid, conflicted with agile
principles, hindered effective client-contractor collaboration, and limited
profitability. The benefits of these alternatives included higher client
satisfaction, reduced contractor risk, and more efficient resource utilization.
Adopting alternative funding and contracting approaches may promote flexibility
and efficiency in agile projects but also presents cultural and structural
challenges, increases the risk of scope creep and analysis paralysis, and
requires additional effort in terms of time and resources. The context of the
organization matters highly in selecting a suitable approach, such as the
organizational readiness in terms of its leaders, people, and systems. Thus,
instead of wholly adopting alternative approaches and introducing changes
abruptly, organizations may benefit from starting with hybrid approaches that
balance flexibility and control and progressively transition to fully flexible
approaches tailored to their needs

</details>


### [29] [Causally Perturbed Fairness Testing](https://arxiv.org/abs/2510.18719)
*Chengwen Du,Tao Chen*

Main category: cs.SE

TL;DR: CausalFT is a framework that uses causal inference to guide fairness testing by identifying causally relevant non-sensitive features, improving existing test generators' ability to reveal fairness bugs in AI systems.


<details>
  <summary>Details</summary>
Motivation: Current fairness testing approaches focus on test sample generators but ignore valuable data characteristics that could guide perturbation, limiting their effectiveness in revealing fairness bugs.

Method: Proposes CausalFT framework that uses causal inference to extract causally relevant non-sensitive features related to sensitive features, then injects this causal relationship into perturbation to guide test sample generation.

Result: Experiments on 1296 cases show CausalFT improves base generators in revealing fairness bugs in 93% of cases with acceptable overhead, outperforms correlation-based approaches in 64% of cases, and enhances bias resilience in nearly all cases.

Conclusion: CausalFT effectively bridges the gap in fairness testing by leveraging causal relationships to guide perturbation, significantly improving existing generators' ability to detect fairness bugs while maintaining efficiency.

Abstract: To mitigate unfair and unethical discrimination over sensitive features
(e.g., gender, age, or race), fairness testing plays an integral role in
engineering systems that leverage AI models to handle tabular data. A key
challenge therein is how to effectively reveal fairness bugs under an
intractable sample size using perturbation. Much current work has been focusing
on designing the test sample generators, ignoring the valuable knowledge about
data characteristics that can help guide the perturbation and hence limiting
their full potential. In this paper, we seek to bridge such a gap by proposing
a generic framework of causally perturbed fairness testing, dubbed CausalFT.
Through causal inference, the key idea of CausalFT is to extract the most
directly and causally relevant non-sensitive feature to its sensitive
counterpart, which can jointly influence the prediction of the label. Such a
causal relationship is then seamlessly injected into the perturbation to guide
a test sample generator. Unlike existing generator-level work, CausalFT serves
as a higher-level framework that can be paired with diverse base generators.
Extensive experiments on 1296 cases confirm that CausalFT can considerably
improve arbitrary base generators in revealing fairness bugs over 93% of the
cases with acceptable extra runtime overhead. Compared with a state-of-the-art
approach that ranks the non-sensitive features solely based on correlation,
CausalFT performs significantly better on 64% cases while being much more
efficient. Further, CausalFT can better improve bias resilience in nearly all
cases.

</details>


### [30] [ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](https://arxiv.org/abs/2510.18787)
*Quim Motger,Carlota Catot,Xavier Franch*

Main category: cs.SE

TL;DR: This paper analyzes datasets for LLM-based Requirements Engineering (LLM4RE), identifying fragmentation and poor characterization as key issues. It provides a systematic mapping study and characterization scheme to improve dataset visibility and reuse.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in Requirements Engineering limits LLM performance. Existing LLM4RE datasets are fragmented and poorly characterized, hindering reuse and comparability.

Method: Conducted a systematic mapping study to identify and analyze 62 publicly available datasets from 43 primary studies. Characterized datasets using descriptors like artifact type, granularity, RE stage, task, domain, and language.

Result: Found multiple research gaps: limited coverage for elicitation tasks, scarce datasets for management activities beyond traceability, and limited multilingual availability. Created a public catalogue and structured characterization scheme.

Conclusion: The research provides tools to support dataset selection, comparison, and reuse in LLM4RE. Future work will extend to grey literature and integration with open repositories.

Abstract: [Context] Large Language Models (LLMs) rely on domain-specific datasets to
achieve robust performance across training and inference stages. However, in
Requirements Engineering (RE), data scarcity remains a persistent limitation
reported in surveys and mapping studies. [Question/Problem] Although there are
multiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented
and poorly characterized, limiting reuse and comparability. This research
addresses the limited visibility and characterization of datasets used in
LLM4RE. We investigate which public datasets are employed, how they can be
systematically characterized, and which RE tasks and dataset descriptors remain
under-represented. [Ideas/Results] To address this, we conduct a systematic
mapping study to identify and analyse datasets used in LLM4RE research. A total
of 62 publicly available datasets are referenced across 43 primary studies.
Each dataset is characterized along descriptors such as artifact type,
granularity, RE stage, task, domain, and language. Preliminary findings show
multiple research gaps, including limited coverage for elicitation tasks,
scarce datasets for management activities beyond traceability, and limited
multilingual availability. [Contribution] This research preview offers a public
catalogue and structured characterization scheme to support dataset selection,
comparison, and reuse in LLM4RE research. Future work will extend the scope to
grey literature, as well as integration with open dataset and benchmark
repositories.

</details>


### [31] [FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews](https://arxiv.org/abs/2510.18799)
*Max Tiessler,Quim Motger*

Main category: cs.SE

TL;DR: FeClustRE is a framework that extracts features from mobile app reviews using hybrid parsing and LLM enrichment, organizes them through hierarchical clustering with auto-tuning, and generates semantic taxonomy labels to improve interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for extracting features from mobile app reviews struggle with noisy feedback, lack semantic depth, and produce flat feature lists without meaningful organization, hindering requirements analysis and cross-app comparison.

Method: Combines syntactic parsing with LLM enrichment for feature extraction, uses hierarchical clustering with auto-tuning to organize features, and employs LLM-based semantic labeling to generate meaningful taxonomy labels.

Result: Evaluated on public benchmarks for extraction correctness and generative AI assistant app reviews, demonstrating improved clustering quality, semantic coherence, and interpretability compared to existing approaches.

Conclusion: FeClustRE bridges user feedback and feature understanding through its hybrid framework, auto-tuning mechanism, and open-source implementation, enabling deeper insights into current and emerging requirements.

Abstract: [Context and motivation.] Extracting features from mobile app reviews is
increasingly important for multiple requirements engineering (RE) tasks.
However, existing methods struggle to turn noisy, ambiguous feedback into
interpretable insights. [Question/problem.] Syntactic approaches lack semantic
depth, while large language models (LLMs) often miss fine-grained features or
fail to structure them coherently. In addition, existing methods output flat
lists of features without semantic organization, limiting interpretation and
comparability. Consequently, current feature extraction approaches do not
provide structured, meaningful representations of app features. As a result,
practitioners face fragmented information that hinder requirement analysis,
prioritization, and cross-app comparison, among other use cases. [Principal
ideas/results.] In this context, we propose FeClustRE, a framework integrating
hybrid feature extraction, hierarchical clustering with auto-tuning and
LLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM
enrichment, organizes features into clusters, and automatically generates
meaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for
extraction correctness and on a sample study of generative AI assistant app
reviews for clustering quality, semantic coherence, and interpretability.
[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature
extraction and taxonomy generation, (2) an auto-tuning mechanism with a
comprehensive evaluation methodology, and (3) open-source and replicable
implementation. These contributions bridge user feedback and feature
understanding, enabling deeper insights into current and emerging requirements.

</details>


### [32] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: AToMIC is an automated framework that uses specialized LLMs to generate Gherkin scenarios, Page Objects, and executable UI test scripts from requirements and code changes, significantly reducing mobile acceptance testing effort.


<details>
  <summary>Details</summary>
Motivation: Mobile acceptance testing is a bottleneck in software development, especially for cross-platform frameworks like Flutter, requiring significant manual effort for creating and maintaining test artifacts.

Method: AToMIC leverages specialized Large Language Models to automatically generate Gherkin scenarios, Page Objects, and executable UI test scripts directly from requirements (JIRA tickets) and recent code changes.

Result: Applied to BMW's MyBMW app (170+ screen codebase), AToMIC produced executable test artifacts in under 5 minutes per feature. 93.3% of Gherkin scenarios were syntactically correct, 78.8% of PageObjects ran without edits, and 100% of UI tests executed successfully. Practitioners reported time savings of a full developer-day per feature.

Conclusion: AToMIC is a scalable, practical solution for streamlining acceptance test creation and maintenance in industrial mobile projects, with strong practitioner confidence and significant time savings.

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [33] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: EffiReasonTrans is a training framework that improves code translation accuracy while balancing inference latency through reasoning-augmented data and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between improved code translation accuracy from LLMs and increased inference latency that hinders real-world development workflows with human-in-the-loop inspection.

Method: Construct high-quality reasoning-augmented dataset using DeepSeek-R1, then employ two-stage training: supervised fine-tuning on reasoning-augmented samples followed by reinforcement learning to balance accuracy and latency.

Result: Improves translation accuracy up to +49.2% CA and +27.8% CodeBLEU while reducing generated tokens up to -19.3% and lowering inference latency up to -29.0% across six translation pairs.

Conclusion: EffiReasonTrans effectively balances translation accuracy and inference latency, demonstrating improved performance when integrated into agent-based frameworks, with complementary benefits from the two-stage training approach.

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>
