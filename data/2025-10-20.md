<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: Proposes using LLMs to generate snippet-alignment data for code translation and a two-stage training strategy that combines both program-alignment and snippet-alignment data to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Parallel corpora are crucial for code translation but limited. Existing augmentation methods mainly focus on program-alignment data, while snippet-alignment data enables more fine-grained alignment learning but is scarce.

Method: Leverages LLMs to automatically generate snippet-alignment data and proposes a two-stage training strategy that uses both program-alignment and generated snippet-alignment data.

Result: Experiments on TransCoder-test show consistent improvements over baseline, achieving maximum gain of 3.78% on pass@k metric.

Conclusion: The proposed data augmentation method and two-stage training strategy effectively enhance code translation performance by leveraging both types of parallel corpora.

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [2] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: CES is a task to evaluate LLMs' abilities in simulating program execution and using that reasoning in programming tasks, introducing coherence metrics to detect reasoning shortcuts and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs truly understand program execution or rely on shortcuts, and to systematically assess their reasoning consistency across different test cases.

Method: Proposes CES task with coherence metrics and reasoning consistency spectrum (strong, weak, random), evaluating 16 LLMs including reasoning models on HumanEval dataset.

Result: 81.42% coherent execution simulation on HumanEval, but only 46.92% correct predictions; frontier LLMs show most incoherent reasoning due to natural language shortcuts; reasoning consistency is mostly random (48.87%) or weak (45.37%).

Conclusion: LLMs barely incorporate execution reasoning into bug-related tasks, relying on pattern matching or shortcuts; CES can systematically vet suspicious success and reveals threats to generalizability in dealing with unseen bugs.

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [3] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: This study analyzes how community engagement (CE) impacts open-source software project dynamics and lifespan using GitHub data from 33,946 repositories, finding that CE metrics significantly correlate with project activity and that both initial engagement bursts and sustained high activity are crucial for longevity.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of community engagement on open-source software project dynamics and lifespan, as this relationship is underexplored despite its importance for project longevity.

Method: Analyzed 33,946 GitHub repositories, defined and operationalized community engagement using per-month metrics (issues, comments, watchers, stargazers), and used non-parametric tests and correlations to assess relationships across project quartiles.

Result: CE metrics significantly associate with project dynamics, with stronger correlations in highly engaged projects. Per-month CE rates are highest in younger projects but decline with age, though some long-lived projects maintain exceptionally high activity. Initial CE bursts are crucial for establishment, while sustained high engagement drives extreme longevity.

Conclusion: Community engagement dynamically drives OSS project longevity and development, with validated metrics providing insights into how diverse community activity patterns contribute to project success.

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [4] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: This paper evaluates 76 LLMs for code clone detection, finding no single best model but identifying CodeT5+110M, CuBERT and SPTCode as top performers. It also explores LLM ensembling approaches that significantly improve detection precision, achieving 46.91% on commercial datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in scalable clone detection for diverged clones and determine optimal LLM selection and ensemble efficacy for this task.

Method: Filtered 76 LLMs to suitable candidates, evaluated on BigCloneBench and commercial datasets. Explored ensembling methods with score normalization and compared maximum/sum vs averaging approaches.

Result: No uniformly best LLM emerged, but CodeT5+110M achieved 39.71% precision (twice CodeBERT's performance). Ensembling achieved 46.91% precision on commercial datasets, with statistical significance on larger datasets.

Conclusion: Smaller embedding sizes, smaller tokenizer vocabularies and tailored datasets benefit clone detection. Ensembling with proper score normalization significantly improves effectiveness, especially on large-scale datasets.

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [5] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: LLMs can generate code that improves performance over baseline in most cases, but human developers still outperform LLMs by a significant margin in creating optimal solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Large Language Models can generate not just functional code but actually fast and optimized code compared to human developers.

Method: Used a dataset of 65 real-world Java tasks where developers achieved significant speedups, employed automated pipeline with two leading LLMs under four prompt variations, and benchmarked against baseline and human-authored solutions.

Result: LLM-generated code improves performance over baseline in most cases, but human patches outperform LLM fixes by statistically significant margin. LLM solutions are semantically similar to developer ideas in ~2/3 cases and original in ~1/3 cases, but original ideas rarely yield substantial gains.

Conclusion: While LLMs can generate performance-improving code, they often fall short of finding truly optimal solutions compared to human developers, and their original optimization ideas rarely provide significant performance benefits.

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [6] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight is a framework that uses fuzzing-generated likely invariants to detect behavioral differences across program versions, helping identify potential bugs during code review.


<details>
  <summary>Details</summary>
Motivation: Manual code reviews often miss dynamic program behaviors due to time constraints and reliance on static inspection. Fuzzing produces rich behavioral data but remains inaccessible to reviewers without proper analysis mechanisms.

Method: FuzzSight leverages likely invariants from non-crashing fuzzing inputs to capture program behavior variations across versions, highlighting unexpected behavioral changes that may indicate bugs.

Result: FuzzSight detected 75% of regression bugs and up to 80% of vulnerabilities found by 24-hour fuzzing. It outperformed SAST with 10x higher detection rates and fewer false alarms.

Conclusion: FuzzSight demonstrates the value of combining fuzzing with invariant analysis for early-stage code review, bridging static inspection with dynamic behavioral insights to improve bug detection.

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [7] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp is an open-source tool that collects and synchronizes physiological and motion data from multiple wearable devices using a smartphone as a central hub.


<details>
  <summary>Details</summary>
Motivation: Limited access to large, high-quality public datasets and lack of control over data collection conditions hinder development of robust algorithms for wearable device data.

Method: Uses smartphone as central hub to receive data from Polar H10 chest strap and Wear OS smartwatch, with custom synchronization protocol and user-friendly interface to export synchronized CSV datasets.

Result: The tool effectively produces consistent and synchronized signals suitable for applications like human activity recognition and heart rate estimation.

Conclusion: Colepp facilitates generation of customizable, real-world datasets from multiple wearable devices, addressing challenges in wearable data collection.

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [8] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: This position paper proposes integrating Test-Driven Development (TDD) with LLM-generated code to improve correctness and reliability, especially in high-stakes domains like finance and science.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce code with hallucinations, logical inconsistencies, and errors, creating significant risks in domains where accuracy is critical. Spreadsheet users without formal programming training face serious consequences from such errors.

Method: A structured research framework that applies TDD principles to LLM-driven generation, using a 'test first' methodology to guide outputs. Includes experimental design with participant groups, evaluation metrics, and TDD-based prompting examples across various programming contexts.

Result: The framework is proposed but not yet empirically evaluated. The authors invite collaboration to refine and test the approach.

Conclusion: Integrating TDD with LLM generation can enhance computational thinking, prompt engineering skills, and user engagement, ultimately establishing responsible LLM integration in educational and professional practices.

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [9] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: This study examines how gender diversity, specifically women's participation, impacts React development patterns, showing women contribute more to feature enhancement and dependency management.


<details>
  <summary>Details</summary>
Motivation: To understand how greater gender diversity could fundamentally alter software development patterns, as current approaches often only superficially acknowledge women's existence without exploring their actual impact.

Method: Analyzed gender differences in robustness and innovation metrics, and shifts in contribution patterns leading up to major version releases over 11 years of the React project.

Result: Women contribute significantly more to feature enhancement and dependency management, and the exclusion of women is detrimental to software quality.

Conclusion: Increasing gender diversity could lead to more inclusive, innovative, and robust software development.

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [10] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz is an automated API fuzzing solution that discovers shared bugs across DL frameworks by leveraging historical bug data, API similarity matching, and LLM-generated test code.


<details>
  <summary>Details</summary>
Motivation: DL frameworks have similar APIs with overlapping parameters, making them vulnerable to shared bugs that can cascade across frameworks, but existing techniques don't adequately address this cross-framework vulnerability.

Method: Three-stage approach: 1) Collect historical bug data to identify buggy APIs, 2) Match buggy APIs with similar APIs across frameworks, 3) Use LLMs to synthesize test code leveraging historical bug patterns to trigger analogous bugs.

Result: Evaluated on TensorFlow, PyTorch, OneFlow, and Jittor. Improved code coverage by 39.92% (TensorFlow) and 98.20% (PyTorch) compared to state-of-the-art. Found 315 bugs (262 new), with 80 fixed and 52 assigned CNVD IDs.

Conclusion: MirrorFuzz effectively discovers shared bugs across DL frameworks through API similarity analysis and LLM-powered test generation, significantly outperforming existing methods in both coverage and bug detection.

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [11] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: EASELAN is an annotation framework that extends ELAN to handle multimodal and biosignals data, providing streamlined workflows from preparation to post-processing with integrated version control via GitHub.


<details>
  <summary>Details</summary>
Motivation: The growing demand for large, richly annotated multimodal datasets, especially those incorporating biosignals alongside traditional audiovisual channels, requires improved annotation workflows to handle rising complexity.

Method: EASELAN builds on the ELAN tool by adding components supporting all annotation pipeline stages: file preparation, channel setup, integrated GitHub version control, and simplified post-processing for biosignals integration.

Result: Successfully applied to a high-dimensional biosignals collection on human everyday activities (table setting) for cognitive robots within the EASE research center, demonstrating practical implementation.

Conclusion: The framework facilitates rich annotations for machine learning model training, with code and fully annotated Table Setting Database publicly available to foster biosignal research.

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [12] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: Community-based analytics can help OSS maintainers understand how their APIs are actually used by dependent projects, revealing that only 16% of API methods are actively used and 74% of used methods have test coverage.


<details>
  <summary>Details</summary>
Motivation: OSS maintainers lack continuous feedback on how their libraries are used in dependent projects, which could help them make better decisions about testing strategies, change impact, and library evolution.

Method: Empirical study of 10 popular Java libraries and their dependent ecosystems (50 projects each), plus proposed metrics for test suite evaluation and a survey of open-source practitioners.

Result: Only 16% of API methods are actively used by dependent projects, and only 74% of used API methods have partial or full test coverage in their library test suites.

Conclusion: Community-based analytics provide valuable insights for OSS maintainers to optimize testing strategies and guide library evolution based on actual API usage patterns in the ecosystem.

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>
