{"id": "2510.21902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21902", "abs": "https://arxiv.org/abs/2510.21902", "authors": ["Timoth\u00e9 Boulet", "Xavier Hinaut", "Cl\u00e9ment Moulin-Frier"], "title": "Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments", "comment": "10 pages, 7 figures", "summary": "Software Engineering Agents (SWE-Agents) have proven effective for\ntraditional software engineering tasks with accessible codebases, but their\nperformance for embodied tasks requiring well-designed information discovery\nremains unexplored. We present the first extended evaluation of SWE-Agents on\ncontroller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to\nsolve 20 diverse embodied tasks from the Minigrid environment. Our experiments\ncompare agent performance across different information access conditions: with\nand without environment source code access, and with varying capabilities for\ninteractive exploration. We quantify how different information access levels\naffect SWE-Agent performance for embodied tasks and analyze the relative\nimportance of static code analysis versus dynamic exploration for task solving.\nThis work establishes controller generation for embodied tasks as a crucial\nevaluation domain for SWE-Agents and provides baseline results for future\nresearch in efficient reasoning systems.", "AI": {"tldr": "First extended evaluation of Software Engineering Agents (SWE-Agents) on embodied task controller generation, showing how different information access levels affect performance in Minigrid environment tasks.", "motivation": "SWE-Agents have proven effective for traditional software engineering tasks, but their performance for embodied tasks requiring information discovery remains unexplored.", "method": "Adapted Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks from Minigrid environment, comparing performance across different information access conditions (with/without environment source code, varying interactive exploration capabilities).", "result": "Quantified how different information access levels affect SWE-Agent performance for embodied tasks and analyzed the relative importance of static code analysis versus dynamic exploration.", "conclusion": "Establishes controller generation for embodied tasks as crucial evaluation domain for SWE-Agents and provides baseline results for future research in efficient reasoning systems."}}
{"id": "2510.21903", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21903", "abs": "https://arxiv.org/abs/2510.21903", "authors": ["Xuhui Zhou", "Valerie Chen", "Zora Zhiruo Wang", "Graham Neubig", "Maarten Sap", "Xingyao Wang"], "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents", "comment": null, "summary": "Recent advances in coding agents have made them capable of planning, editing,\nrunning, and testing complex code bases. Despite their growing ability in\ncoding tasks, these systems still struggle to infer and track user intent,\nespecially when instructions are underspecified or context-dependent. To bridge\nthis gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary\nsoftware-engineering (SWE) agent with a lightweight theory-of-mind (ToM)\npartner agent dedicated to modeling the user's mental state. The ToM agent\ninfers user goals, constraints, and preferences from instructions and\ninteraction history, maintains a \\textbf{persistent memory} of the user, and\nprovides user-related suggestions to the SWE agent. In two software engineering\nbenchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task\nsuccess rates and user satisfaction. Notably, on the stateful SWE benchmark, a\nnewly introduced evaluation that provides agents with a user simulator along\nwith previous interaction histories, ToM-SWE achieves a substantially higher\ntask success rate of 59.7\\% compared to 18.1\\% for OpenHands, a\nstate-of-the-art SWE agent. Furthermore, in a three-week study with\nprofessional developers using ToM-SWE in their daily work, participants found\nit useful 86\\% of the time, underscoring the value of stateful user modeling\nfor practical coding agents.", "AI": {"tldr": "ToM-SWE introduces a dual-agent architecture with a software engineering agent and a theory-of-mind agent that models user mental states to improve coding agent performance on underspecified tasks.", "motivation": "Current coding agents struggle to infer and track user intent when instructions are underspecified or context-dependent, limiting their effectiveness in real-world software engineering scenarios.", "method": "A dual-agent architecture pairing a primary software engineering agent with a lightweight theory-of-mind partner agent that infers user goals, constraints, and preferences from instructions and interaction history, maintaining persistent memory of the user.", "result": "ToM-SWE achieved 59.7% task success rate on stateful SWE benchmark vs 18.1% for OpenHands, and in a 3-week study with professional developers, participants found it useful 86% of the time.", "conclusion": "Stateful user modeling through theory-of-mind agents significantly improves coding agent performance and user satisfaction in practical software engineering tasks."}}
{"id": "2510.21933", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21933", "abs": "https://arxiv.org/abs/2510.21933", "authors": ["Joao Correia", "Daniel Coutinho", "Marco Castelluccio", "Caio Barbosa", "Rafael de Mello", "Anita Sarma", "Alessandro Garcia", "Marco Gerosa", "Igor Steinmacher"], "title": "A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case", "comment": "13 pages", "summary": "The use of Large Language Models (LLMs) to support tasks in software\ndevelopment has steadily increased over recent years. From assisting developers\nin coding activities to providing conversational agents that answer newcomers'\nquestions. In collaboration with the Mozilla Foundation, this study evaluates\nthe effectiveness of Retrieval-Augmented Generation (RAG) in assisting\ndevelopers within the Mozilla Firefox project. We conducted an empirical\nanalysis comparing responses from human developers, a standard GPT model, and a\nGPT model enhanced with RAG, using real queries from Mozilla's developer chat\nrooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses\nbased on helpfulness, comprehensiveness, and conciseness. The results show that\nRAG-assisted responses were more comprehensive than human developers (62.50% to\n54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to\nenhance developer assistance. However, the RAG responses were not as concise\nand often verbose. The results show the potential to apply RAG-based tools to\nOpen Source Software (OSS) to minimize the load to core maintainers without\nlosing answer quality. Toning down retrieval mechanisms and making responses\neven shorter in the future would enhance developer assistance in massive\nprojects like Mozilla Firefox.", "AI": {"tldr": "RAG-enhanced GPT models can provide more comprehensive developer assistance than human developers in OSS projects, though they tend to be less concise.", "motivation": "To evaluate the effectiveness of Retrieval-Augmented Generation (RAG) in assisting developers within the Mozilla Firefox project and reduce the burden on core maintainers.", "method": "Empirical analysis comparing responses from human developers, standard GPT, and RAG-enhanced GPT using real queries from Mozilla's developer chat rooms, assessed by Mozilla experts on helpfulness, comprehensiveness, and conciseness.", "result": "RAG-assisted responses were more comprehensive than human developers (62.50% vs 54.17%) and almost as helpful (75.00% vs 79.17%), but less concise and often verbose.", "conclusion": "RAG-based tools show potential for enhancing developer assistance in OSS projects while reducing maintainer workload, though future improvements should focus on making responses more concise."}}
{"id": "2510.21966", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21966", "abs": "https://arxiv.org/abs/2510.21966", "authors": ["Musengamana Jean de Dieu", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Muhammad Waseem", "Arif Ali Khan", "Bangchao Wang", "Mst Shamima Aktar"], "title": "ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities", "comment": "42 pages, 14 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Stack Overflow (SO), a leading online community forum, is a rich source of\nsoftware development knowledge. However, locating architectural knowledge, such\nas architectural solutions remains challenging due to the overwhelming volume\nof unstructured content and fragmented discussions. Developers must manually\nsift through posts to find relevant architectural insights, which is\ntime-consuming and error-prone. This study introduces ArchISMiner, a framework\nfor mining architectural knowledge from SO. The framework comprises two\ncomplementary components: ArchPI and ArchISPE. ArchPI trains and evaluates\nmultiple models, including conventional ML/DL models, Pre-trained Language\nModels (PLMs), and Large Language Models (LLMs), and selects the\nbest-performing model to automatically identify Architecture-Related Posts\n(ARPs) among programming-related discussions. ArchISPE employs an indirect\nsupervised approach that leverages diverse features, including BERT embeddings\nand local TextCNN features, to extract architectural issue-solution pairs. Our\nevaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in\nARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,\nachieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.\nA user study further validated the quality (e.g., relevance and usefulness) of\nthe identified ARPs and the extracted issue-solution pairs. Moreover, we\napplied ArchISMiner to three additional forums, releasing a dataset of over 18K\narchitectural issue-solution pairs. Overall, ArchISMiner can help architects\nand developers identify ARPs and extract succinct, relevant, and useful\narchitectural knowledge from developer communities more accurately and\nefficiently. The replication package of this study has been provided at\nhttps://github.com/JeanMusenga/ArchISPE", "AI": {"tldr": "ArchISMiner is a framework that automatically identifies architecture-related posts and extracts architectural issue-solution pairs from Stack Overflow and other developer forums using ML/DL models and language models.", "motivation": "Finding architectural knowledge in Stack Overflow is challenging due to large volume of unstructured content and fragmented discussions, requiring manual effort that is time-consuming and error-prone.", "method": "Two-component framework: ArchPI uses ML/DL models, PLMs, and LLMs to identify Architecture-Related Posts; ArchISPE uses indirect supervised approach with BERT embeddings and TextCNN features to extract architectural issue-solution pairs.", "result": "ArchPI achieved F1-score of 0.960 in ARP detection; ArchISPE outperformed baselines with F1-scores of 0.883 for issues and 0.894 for solutions. Applied to three additional forums, creating dataset of 18K+ architectural issue-solution pairs.", "conclusion": "ArchISMiner helps architects and developers identify relevant architectural knowledge from developer communities more accurately and efficiently."}}
{"id": "2510.21993", "categories": ["cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.21993", "abs": "https://arxiv.org/abs/2510.21993", "authors": ["Yupeng Qi", "Ran Xu", "Xu Chu"], "title": "FeaGPT: an End-to-End agentic-AI for Finite Element Analysis", "comment": null, "summary": "Large language models (LLMs) are establishing new paradigms for engineering\napplications by enabling natural language control of complex computational\nworkflows. This paper introduces FeaGPT, the first framework to achieve\ncomplete geometry-mesh-simulation workflows through conversational interfaces.\nUnlike existing tools that automate individual FEA components, FeaGPT\nimplements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline\nthat transforms engineering specifications into validated computational results\nwithout manual intervention. The system interprets engineering intent,\nautomatically generates physics-aware adaptive meshes, configures complete FEA\nsimulations with proper boundary condition inference, and performs\nmulti-objective analysis through closed-loop iteration.\n  Experimental validation confirms complete end-to-end automation capability.\nIndustrial turbocharger cases (7-blade compressor and 12-blade turbine at\n\\SI{110000}{rpm}) demonstrate the system successfully transforms natural\nlanguage specifications into validated CalculiX simulations, producing\nphysically realistic results for rotating machinery analysis. Additional\nvalidation through 432 NACA airfoil configurations confirms scalability for\nparametric design exploration. These results demonstrate that natural language\ninterfaces can effectively democratize access to advanced computational\nengineering tools while preserving analytical rigor.", "AI": {"tldr": "FeaGPT is the first framework enabling complete geometry-mesh-simulation workflows through conversational interfaces, automating FEA from engineering specifications to validated results without manual intervention.", "motivation": "To democratize access to advanced computational engineering tools by enabling natural language control of complex FEA workflows, overcoming limitations of existing tools that only automate individual components.", "method": "Implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline that interprets engineering intent, generates physics-aware adaptive meshes, configures complete FEA simulations with boundary condition inference, and performs multi-objective analysis through closed-loop iteration.", "result": "Successfully transforms natural language specifications into validated CalculiX simulations for industrial turbocharger cases (7-blade compressor and 12-blade turbine at 110,000 rpm), producing physically realistic results. Additional validation with 432 NACA airfoil configurations confirms scalability for parametric design exploration.", "conclusion": "Natural language interfaces can effectively democratize access to advanced computational engineering tools while preserving analytical rigor, demonstrating complete end-to-end automation capability for FEA workflows."}}
{"id": "2510.22003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22003", "abs": "https://arxiv.org/abs/2510.22003", "authors": ["Stefan Julian Kooy", "Jean Paul Sebastian Piest", "Rob Henk Bemthuis"], "title": "Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review", "comment": "17 pages, 1 figure, 5 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Generative AI (GenAI) is reshaping enterprise architecture work in agile\nsoftware organizations, yet evidence on its effects remains scattered. We\nreport a systematic literature review (SLR), following established SLR\nprotocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies\nacross enterprise, solution, domain, business, and IT architect roles. GenAI\nmost consistently supports (i) design ideation and trade-off exploration; (ii)\nrapid creation and refinement of artifacts (e.g., code, models, documentation);\nand (iii) architectural decision support and knowledge retrieval. Reported\nrisks include opacity and bias, contextually incorrect outputs leading to\nrework, privacy and compliance concerns, and social loafing. We also identify\nemerging skills and competencies, including prompt engineering, model\nevaluation, and professional oversight, and organizational enablers around\nreadiness and adaptive governance. The review contributes with (1) a mapping of\nGenAI use cases and risks in agile architecting, (2) implications for\ncapability building and governance, and (3) an initial research agenda on\nhuman-AI collaboration in architecture. Overall, the findings inform\nresponsible adoption of GenAI that accelerates digital transformation while\nsafeguarding architectural integrity.", "AI": {"tldr": "Systematic review of 33 studies shows GenAI supports design ideation, artifact creation, and decision support in agile enterprise architecture, but faces risks like bias, incorrect outputs, and privacy concerns.", "motivation": "To systematically understand how Generative AI is reshaping enterprise architecture work in agile software organizations, as evidence on its effects remains scattered.", "method": "Systematic literature review following Kitchenham and PRISMA protocols, analyzing 1,697 records to identify 33 relevant studies across various architect roles.", "result": "GenAI consistently supports design ideation, rapid artifact creation, and architectural decision support. Identified risks include opacity/bias, incorrect outputs, privacy/compliance issues, and social loafing. Emerging skills include prompt engineering and model evaluation.", "conclusion": "The review provides a mapping of GenAI use cases and risks, implications for capability building and governance, and establishes a research agenda for human-AI collaboration in architecture to enable responsible adoption."}}
{"id": "2510.22210", "categories": ["cs.SE", "cs.AI", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22210", "abs": "https://arxiv.org/abs/2510.22210", "authors": ["Gwihwan Go", "Quan Zhang", "Chijin Zhou", "Zhao Wei", "Yu Jiang"], "title": "LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation", "comment": "13pages, 6 figures", "summary": "Automated unit test generation is essential for robust software development,\nyet existing approaches struggle to generalize across multiple programming\nlanguages and operate within real-time development. While Large Language Models\n(LLMs) offer a promising solution, their ability to generate high coverage test\ncode depends on prompting a concise context of the focal method. Current\nsolutions, such as Retrieval-Augmented Generation, either rely on imprecise\nsimilarity-based searches or demand the creation of costly, language-specific\nstatic analysis pipelines. To address this gap, we present LSPRAG, a framework\nfor concise-context retrieval tailored for real-time, language-agnostic unit\ntest generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)\nback-ends to supply LLMs with precise symbol definitions and references in real\ntime. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware\ncontext retrieval, requiring minimal per-language engineering effort. We\nevaluated LSPRAG on open-source projects spanning Java, Go, and Python.\nCompared to the best performance of baselines, LSPRAG increased line coverage\nby up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.", "AI": {"tldr": "LSPRAG is a framework that uses Language Server Protocol to provide precise context for LLM-based unit test generation, achieving significant coverage improvements across multiple programming languages.", "motivation": "Existing unit test generation approaches struggle with cross-language generalization and real-time operation, while current LLM solutions need precise context but rely on imprecise similarity searches or costly static analysis pipelines.", "method": "LSPRAG leverages off-the-shelf Language Server Protocol back-ends to supply LLMs with precise symbol definitions and references in real time, enabling language-aware context retrieval with minimal per-language engineering.", "result": "LSPRAG increased line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python compared to the best baseline performance.", "conclusion": "LSPRAG provides an effective language-agnostic solution for real-time unit test generation by reusing mature LSP servers, significantly outperforming existing approaches across multiple programming languages."}}
{"id": "2510.22224", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22224", "abs": "https://arxiv.org/abs/2510.22224", "authors": ["Guan-Yan Yang", "Farn Wang"], "title": "Taming Silent Failures: A Framework for Verifiable AI Reliability", "comment": "This preprint has been accepted by IEEE Reliability Magazine. 10\n  pages, 3 figures", "summary": "The integration of Artificial Intelligence (AI) into safety-critical systems\nintroduces a new reliability paradigm: silent failures, where AI produces\nconfident but incorrect outputs that can be dangerous. This paper introduces\nthe Formal Assurance and Monitoring Environment (FAME), a novel framework that\nconfronts this challenge. FAME synergizes the mathematical rigor of offline\nformal synthesis with the vigilance of online runtime monitoring to create a\nverifiable safety net around opaque AI components. We demonstrate its efficacy\nin an autonomous vehicle perception system, where FAME successfully detected\n93.5% of critical safety violations that were otherwise silent. By\ncontextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,\nwe provide reliability engineers with a practical, certifiable pathway for\ndeploying trustworthy AI. FAME represents a crucial shift from accepting\nprobabilistic performance to enforcing provable safety in next-generation\nsystems.", "AI": {"tldr": "FAME framework combines formal verification and runtime monitoring to detect silent AI failures in safety-critical systems, achieving 93.5% detection rate in autonomous vehicle testing.", "motivation": "Address silent failures in AI systems where confident but incorrect outputs pose safety risks in critical applications like autonomous vehicles.", "method": "Integrates offline formal synthesis with online runtime monitoring to create verifiable safety nets around opaque AI components.", "result": "Successfully detected 93.5% of critical safety violations in autonomous vehicle perception systems that would otherwise be silent failures.", "conclusion": "Provides a practical, certifiable pathway for trustworthy AI deployment, shifting from probabilistic performance to provable safety in next-generation systems."}}
{"id": "2510.22249", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22249", "abs": "https://arxiv.org/abs/2510.22249", "authors": ["Ibuki Nakamura", "Yutaro Kashiwa", "Bin Lin", "Hajimu Iida"], "title": "Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study", "comment": null, "summary": "Developers often opt for easier but non-optimal implementation to meet\ndeadlines or create rapid prototypes, leading to additional effort known as\ntechnical debt to improve the code later. Oftentimes, developers explicitly\ndocument the technical debt in code comments, referred to as Self-Admitted\nTechnical Debt (SATD). Numerous researchers have investigated the impact of\nSATD on different aspects of software quality and development processes.\nHowever, most of these studies focus on SATD in production code, often\noverlooking SATD in the test code or assuming that it shares similar\ncharacteristics with SATD in production code. In fact, a significant amount of\nSATD is also present in the test code, with many instances not fitting into\nexisting categories for the production code. This study aims to fill this gap\nand disclose the nature of SATD in the test code by examining its distribution\nand types. Moreover, the relation between its presence and test quality is also\nanalyzed. Our empirical study, involving 17,766 SATD comments (14,987 from\nproduction code, 2,779 from test code) collected from 50 repositories,\ndemonstrates that while SATD widely exists in test code, it is not directly\nassociated with test smells. Our study also presents comprehensive categories\nof SATD types in the test code, and machine learning models are developed to\nautomatically classify SATD comments based on their types for easier\nmanagement. Our results show that the CodeBERT-based model outperforms other\nmachine learning models in terms of recall and F1-score. However, the\nperformance varies on different types of SATD.", "AI": {"tldr": "This study analyzes Self-Admitted Technical Debt (SATD) in test code, revealing it has different characteristics than production code SATD and isn't directly linked to test smells. The research developed machine learning models to classify SATD types, with CodeBERT performing best.", "motivation": "Previous SATD research focused mainly on production code, overlooking test code SATD which has different characteristics and doesn't fit existing categories. There was a gap in understanding SATD distribution, types, and impact on test quality in test code.", "method": "Empirical study of 17,766 SATD comments (2,779 from test code) from 50 repositories. Analyzed distribution, types, and relationship with test quality. Developed machine learning models including CodeBERT to automatically classify SATD comments by type.", "result": "SATD widely exists in test code but isn't directly associated with test smells. Comprehensive categories of SATD types in test code were identified. CodeBERT-based model outperformed other ML models in recall and F1-score, though performance varied across different SATD types.", "conclusion": "SATD in test code has distinct characteristics from production code SATD and requires different management approaches. Machine learning models, particularly CodeBERT, can effectively classify SATD types for better management, though performance depends on the specific SATD type."}}
{"id": "2510.22254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22254", "abs": "https://arxiv.org/abs/2510.22254", "authors": ["Eric W. Bridgeford", "Iain Campbell", "Zijao Chen", "Zhicheng Lin", "Harrison Ritz", "Joachim Vandekerckhove", "Russell A. Poldrack"], "title": "Ten Simple Rules for AI-Assisted Coding in Science", "comment": "9 pages of content; 1 table; 1 page appendix", "summary": "While AI coding tools have demonstrated potential to accelerate software\ndevelopment, their use in scientific computing raises critical questions about\ncode quality and scientific validity. In this paper, we provide ten practical\nrules for AI-assisted coding that balance leveraging capabilities of AI with\nmaintaining scientific and methodological rigor. We address how AI can be\nleveraged strategically throughout the development cycle with four key themes:\nproblem preparation and understanding, managing context and interaction,\ntesting and validation, and code quality assurance and iterative improvement.\nThese principles serve to emphasize maintaining human agency in coding\ndecisions, establishing robust validation procedures, and preserving the domain\nexpertise essential for methodologically sound research. These rules are\nintended to help researchers harness AI's transformative potential for faster\nsoftware development while ensuring that their code meets the standards of\nreliability, reproducibility, and scientific validity that research integrity\ndemands.", "AI": {"tldr": "Ten practical rules for using AI coding tools in scientific computing that balance AI capabilities with scientific rigor, focusing on problem preparation, context management, testing, and code quality.", "motivation": "AI coding tools can accelerate software development but raise concerns about code quality and scientific validity in research contexts, requiring guidelines to ensure methodological rigor.", "method": "Proposes ten practical rules organized around four themes: problem preparation and understanding, managing context and interaction, testing and validation, and code quality assurance with iterative improvement.", "result": "A framework that helps researchers leverage AI's transformative potential while maintaining human agency, robust validation procedures, and domain expertise for methodologically sound research.", "conclusion": "These rules enable researchers to harness AI for faster software development while ensuring code meets standards of reliability, reproducibility, and scientific validity required for research integrity."}}
{"id": "2510.22318", "categories": ["cs.SE", "cs.AI", "K.3.2, D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22318", "abs": "https://arxiv.org/abs/2510.22318", "authors": ["Tuan-Phong Ngo", "Bao-Ngoc Duong", "Tuan-Anh Hoang", "Joshua Dwight", "Ushik Shrestha Khwakhali"], "title": "Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus", "comment": "7 pages, 3 figures, 3 tables", "summary": "Software testing is a critical component in the software engineering field\nand is important for software engineering education. Thus, it is vital for\nacademia to continuously improve and update educational methods to reflect the\ncurrent state of the field. The International Software Testing Qualifications\nBoard (ISTQB) certification framework is globally recognized and widely adopted\nin industry and academia. However, ISTQB-based learning has been rarely applied\nwith recent generative artificial intelligence advances. Despite the growing\ncapabilities of large language models (LLMs), ISTQB-based learning and\ninstruction with LLMs have not been thoroughly explored. This paper explores\nand evaluates how LLMs can complement the ISTQB framework for higher education.\nThe findings present four key contributions: (i) the creation of a\ncomprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28\nsample exams and 1,145 questions; (ii) the development of a domain-optimized\nprompt that enhances LLM precision and explanation quality on ISTQB tasks;\n(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and\n(iv) actionable insights and recommendations for integrating LLMs into software\ntesting education. These findings highlight the promise of LLMs in supporting\nISTQB certification preparation and offer a foundation for their broader use in\nsoftware engineering at higher education.", "AI": {"tldr": "This paper explores using Large Language Models (LLMs) to complement ISTQB certification framework for software testing education, including dataset creation, prompt optimization, systematic evaluation, and integration recommendations.", "motivation": "Software testing education needs continuous improvement to reflect current field state. ISTQB certification is globally recognized but rarely combined with recent generative AI advances. LLMs' potential for ISTQB-based learning remains unexplored.", "method": "Created comprehensive ISTQB-aligned dataset (28 exams, 1,145 questions over 10+ years), developed domain-optimized prompts, systematically evaluated state-of-the-art LLMs on ISTQB tasks.", "result": "Developed enhanced LLM precision and explanation quality through optimized prompts. Found LLMs show promise in supporting ISTQB certification preparation and software testing education.", "conclusion": "LLMs have significant potential to complement ISTQB framework in higher education. The research provides foundation for broader LLM integration in software engineering education and actionable recommendations for implementation."}}
{"id": "2510.22338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22338", "abs": "https://arxiv.org/abs/2510.22338", "authors": ["Aritra Mitra", "Srijoni Majumdar", "Anamitra Mukhopadhyay", "Partha Pratim Das", "Paul D Clough", "Partha Pratim Chakrabarti"], "title": "Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation", "comment": null, "summary": "Comments are very useful to the flow of code development. With the increasing\ncommonality of code, novice coders have been creating a significant amount of\ncodebases. Due to lack of commenting standards, their comments are often\nuseless, and increase the time taken to further maintain codes. This study\nintends to find the usefulness of large language models (LLMs) in these cases\nto generate potentially better comments. This study focuses on the feasibility\nof design documents as a context for the LLMs to generate more useful comments,\nas design documents are often used by maintainers to understand code when\ncomments do not suffice.", "AI": {"tldr": "This paper explores using LLMs to generate better code comments by leveraging design documents as context, addressing the issue of useless comments from novice coders that hinder code maintenance.", "motivation": "Novice coders create many codebases with useless comments due to lack of standards, which increases maintenance time. LLMs could potentially generate better comments to address this problem.", "method": "The study focuses on using design documents as context for LLMs to generate more useful comments, since maintainers often rely on design documents when comments are insufficient.", "result": "The paper investigates the feasibility of this approach but does not present specific experimental results in the abstract.", "conclusion": "Using design documents as context for LLMs shows promise for generating more useful code comments that could improve code maintenance efficiency."}}
{"id": "2510.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22409", "abs": "https://arxiv.org/abs/2510.22409", "authors": ["Shahidul Islam", "Md Nahidul Islam Opu", "Shaowei Wang", "Shaiful Chowdhury"], "title": "A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection", "comment": null, "summary": "Self-admitted technical debt (SATD) refers to comments in which developers\nexplicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD\nis known to significantly increase software maintenance effort. While extensive\nresearch has examined SATD in source code, its presence and impact in test code\nhave received no focused attention, leaving a significant gap in our\nunderstanding of how SATD manifests in testing contexts.\n  This study, the first of its kind, investigates SATD in test code by manually\nanalyzing 50,000 comments randomly sampled from 1.6 million comments across\n1,000 open-source Java projects. From this sample, after manual analysis and\nfiltering, we identified 615 SATD comments and classified them into 15 distinct\ncategories, building a taxonomy of test code SATD. To investigate whether test\ncode SATD can be detected automatically, we evaluated existing SATD detection\ntools, as well as both open-source and proprietary LLMs. Among the existing\ntools, MAT performed the best, albeit with moderate recall. To our surprise,\nboth open-source and proprietary LLMs exhibited poor detection accuracy,\nprimarily due to low precision. These results indicate that neither existing\napproaches nor current LLMs can reliably detect SATD in test code.\n  Overall, this work provides the first large-scale analysis of SATD in test\ncode, a nuanced understanding of its types, and the limitations of current SATD\ndetection methods. Our findings lay the groundwork for future research on test\ncode-specific SATD.", "AI": {"tldr": "First large-scale study of self-admitted technical debt (SATD) in test code, analyzing 50,000 comments from 1,000 Java projects to identify 615 SATD instances and create a taxonomy of 15 categories, while finding current detection tools and LLMs perform poorly.", "motivation": "While SATD in source code has been extensively studied, its presence and impact in test code has received no focused attention, leaving a significant gap in understanding how technical debt manifests in testing contexts.", "method": "Manually analyzed 50,000 comments randomly sampled from 1.6 million comments across 1,000 open-source Java projects, identified 615 SATD comments, classified them into 15 categories, and evaluated existing SATD detection tools and LLMs for automatic detection.", "result": "Identified 615 SATD comments and built a taxonomy of 15 distinct categories of test code SATD. Existing tools showed moderate recall (MAT performed best), while both open-source and proprietary LLMs exhibited poor detection accuracy primarily due to low precision.", "conclusion": "Current approaches and LLMs cannot reliably detect SATD in test code. This work provides foundational understanding of test code SATD types and detection limitations, laying groundwork for future research on test code-specific technical debt."}}
{"id": "2510.22457", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22457", "abs": "https://arxiv.org/abs/2510.22457", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "A Multifaceted View on Discrimination in Software Development Careers", "comment": "11 pages, 1 figure, 5 tables", "summary": "Conversations around diversity and inclusion in software engineering often\nfocus on gender and racial disparities. However, the State of the Developer\nNation 2025 survey with 8,717 participants revealed that other forms of\ndiscrimination are similarly prevalent but receive considerably less attention.\nThis includes discrimination based on age, political perspective, disabilities,\nor cognitive differences such as neurodivergence. We conducted a secondary\nanalysis of 800 open-ended survey responses to examine patterns of perceived\ndiscrimination, as well as related challenges and negative impacts. Our study\ncovers multiple identity facets, including age, gender, race, and disability.\nWe found that age- and gender-related discrimination was the most frequently\nreported workplace issue, but discrimination based on political and religious\nviews emerged as further notable concerns. Most of the participants who\nidentified as female cited gender as the primary source of discrimination,\noften accompanied by intersectional factors such as race, political views, age,\nor sexual orientation. Discrimination related to caregiving responsibilities\nwas reported by all gender identities. Regarding the negative impacts of\nworkplace issues, many participants described modifying their appearance or\nbehavior in response to gender biases. Gender also appeared to influence\nbroader career challenges, as women and non-binary respondents reported\nexperiencing almost all workplace issues at higher rates, particularly\ndiscrimination (35%) and mental health challenges (62%). Our goal is to raise\nawareness in the research community that discrimination in software development\nis multifaceted, and to encourage researchers to select and assess relevant\nfacets beyond age and gender when designing software engineering studies.", "AI": {"tldr": "The study reveals that discrimination in software engineering extends beyond gender and race to include age, political views, disabilities, and neurodivergence, with gender and age discrimination being most prevalent.", "motivation": "To highlight the multifaceted nature of discrimination in software engineering beyond commonly discussed gender and racial disparities, using survey data to uncover less visible forms of bias.", "method": "Secondary analysis of 800 open-ended responses from the State of the Developer Nation 2025 survey (8,717 total participants), examining patterns of perceived discrimination across multiple identity facets.", "result": "Age- and gender-related discrimination were most frequently reported, with political/religious discrimination also significant. Women and non-binary individuals reported higher rates of discrimination (35%) and mental health challenges (62%), often with intersectional factors. Caregiving discrimination affected all genders.", "conclusion": "Discrimination in software development is multifaceted, requiring researchers to consider diverse identity facets beyond age and gender in study design to better understand workplace challenges."}}
{"id": "2510.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22530", "abs": "https://arxiv.org/abs/2510.22530", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "comment": "11 pages, 8 figures, under review", "summary": "Fault Localization (FL) aims to identify root causes of program failures. FL\ntypically targets failures observed from test executions, and as such, often\ninvolves dynamic analyses to improve accuracy, such as coverage profiling or\nmutation testing. However, for large industrial software, measuring coverage\nfor every execution is prohibitively expensive, making the use of such\ntechniques difficult. To address these issues and apply FL in an industrial\nsetting, this paper proposes AutoCrashFL, an LLM agent for the localization of\ncrashes that only requires the crashdump from the Program Under Test (PUT) and\naccess to the repository of the corresponding source code. We evaluate\nAutoCrashFL against real-world crashes of SAP HANA, an industrial software\nproject consisting of more than 35 million lines of code. Experiments reveal\nthat AutoCrashFL is more effective in localization, as it identified 30%\ncrashes at the top, compared to 17% achieved by the baseline. Through thorough\nanalysis, we find that AutoCrashFL has attractive practical properties: it is\nrelatively more effective for complex bugs, and it can indicate confidence in\nits results. Overall, these results show the practicality of LLM agent\ndeployment on an industrial scale.", "AI": {"tldr": "AutoCrashFL is an LLM agent for fault localization that uses only crashdumps and source code access, achieving 30% top-1 accuracy on industrial crashes compared to 17% baseline.", "motivation": "Traditional fault localization techniques require expensive dynamic analyses like coverage profiling, which are impractical for large industrial software with millions of lines of code.", "method": "Proposes AutoCrashFL, an LLM agent that localizes crashes using only crashdump information and source code repository access, without requiring expensive dynamic analysis.", "result": "AutoCrashFL identified 30% of crashes at top position vs 17% baseline on SAP HANA (35M+ LOC), showing better performance on complex bugs and providing confidence indicators.", "conclusion": "LLM agents like AutoCrashFL demonstrate practical deployment potential for fault localization at industrial scale, offering effective crash analysis without expensive dynamic instrumentation."}}
{"id": "2510.22613", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22613", "abs": "https://arxiv.org/abs/2510.22613", "authors": ["Songhan Zhang", "Aoyang Fang", "Yifan Yang", "Ruiyi Cheng", "Xiaoying Tang", "Pinjia He"], "title": "DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices", "comment": null, "summary": "Cloud-native microservices enable rapid iteration and scalable deployment but\nalso create complex, fast-evolving dependencies that challenge reliable\ndiagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal\nfusion of logs, traces, and metrics, remain limited in capturing dynamic\nbehaviors and shifting service relationships. Three critical challenges\npersist: (i) inadequate modeling of cascading fault propagation, (ii)\nvulnerability to noise interference and concept drift in normal service\nbehavior, and (iii) over-reliance on service deviation intensity that obscures\ntrue root causes. To address these challenges, we propose DynaCausal, a dynamic\ncausality-aware framework for RCA in distributed microservice systems.\nDynaCausal unifies multi-modal dynamic signals to capture time-varying\nspatio-temporal dependencies through interaction-aware representation learning.\nIt further introduces a dynamic contrastive mechanism to disentangle true fault\nindicators from contextual noise and adopts a causal-prioritized pairwise\nranking objective to explicitly optimize causal attribution. Comprehensive\nevaluations on public benchmarks demonstrate that DynaCausal consistently\nsurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with\nabsolute gains from 0.25 to 0.46, and delivering both accurate and\ninterpretable diagnoses in highly dynamic microservice environments.", "AI": {"tldr": "DynaCausal is a dynamic causality-aware framework for root cause analysis in microservice systems that addresses challenges in modeling fault propagation, noise interference, and causal attribution through multi-modal dynamic signal fusion and contrastive learning.", "motivation": "Existing RCA approaches fail to capture dynamic behaviors and shifting service relationships in cloud-native microservices, with limitations in modeling cascading fault propagation, vulnerability to noise, and over-reliance on service deviation intensity.", "method": "Unifies multi-modal dynamic signals to capture time-varying spatio-temporal dependencies through interaction-aware representation learning, introduces dynamic contrastive mechanism to disentangle true fault indicators from noise, and adopts causal-prioritized pairwise ranking objective.", "result": "Achieves average AC@1 of 0.63 with absolute gains from 0.25 to 0.46, consistently surpassing state-of-the-art methods on public benchmarks.", "conclusion": "DynaCausal delivers both accurate and interpretable diagnoses in highly dynamic microservice environments by explicitly optimizing causal attribution."}}
{"id": "2510.22614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22614", "abs": "https://arxiv.org/abs/2510.22614", "authors": ["Roham Koohestani", "Agnia Sergeyuk", "David Gros", "Claudio Spiess", "Sergey Titov", "Prem Devanbu", "Maliheh Izadi"], "title": "Does In-IDE Calibration of Large Language Models work at Scale?", "comment": "Under Review", "summary": "The introduction of large language models into integrated development\nenvironments (IDEs) is revolutionizing software engineering, yet it poses\nchallenges to the usefulness and reliability of Artificial\nIntelligence-generated code. Post-hoc calibration of internal model confidences\naims to align probabilities with an acceptability measure. Prior work suggests\ncalibration can improve alignment, but at-scale evidence is limited. In this\nwork, we investigate the feasibility of applying calibration of code models to\nan in-IDE context. We study two aspects of the problem: (1) the technical\nmethod for implementing confidence calibration and improving the reliability of\ncode generation models, and (2) the human-centered design principles for\neffectively communicating reliability signal to developers. First, we develop a\nscalable and flexible calibration framework which can be used to obtain\ncalibration weights for open-source models using any dataset, and evaluate\nwhether calibrators improve the alignment between model confidence and\ndeveloper acceptance behavior. Through a large-scale analysis of over 24\nmillion real-world developer interactions across multiple programming\nlanguages, we find that a general, post-hoc calibration model based on\nPlatt-scaling does not, on average, improve the reliability of model confidence\nsignals. We also find that while dynamically personalizing calibration to\nindividual users can be effective, its effectiveness is highly dependent on the\nvolume of user interaction data. Second, we conduct a multi-phase design study\nwith 3 expert designers and 153 professional developers, combining\nscenario-based design, semi-structured interviews, and survey validation,\nrevealing a clear preference for presenting reliability signals via\nnon-numerical, color-coded indicators within the in-editor code generation\nworkflow.", "AI": {"tldr": "This paper investigates confidence calibration for AI code generation models in IDEs, finding that general post-hoc calibration doesn't improve reliability, while personalized calibration shows limited effectiveness. The study also reveals developer preference for color-coded reliability indicators over numerical ones.", "motivation": "Large language models in IDEs are revolutionizing software engineering but pose challenges for AI-generated code reliability. Post-hoc calibration aims to align model confidences with acceptability, but evidence at scale is limited.", "method": "Developed a scalable calibration framework for open-source models using any dataset. Analyzed 24+ million developer interactions across multiple languages. Conducted multi-phase design study with 3 expert designers and 153 developers using scenario-based design, interviews, and surveys.", "result": "General post-hoc calibration (Platt-scaling) doesn't improve reliability of model confidence signals on average. Personalized calibration to individual users can be effective but depends heavily on user interaction data volume. Developers prefer non-numerical, color-coded reliability indicators in IDE workflows.", "conclusion": "Standard calibration methods don't reliably improve code model confidence signals, while effective personalization requires substantial user data. Visual, non-numerical reliability indicators are preferred by developers for in-IDE code generation workflows."}}
{"id": "2510.22787", "categories": ["cs.SE", "cs.AI", "68T07", "I.2.11; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.22787", "abs": "https://arxiv.org/abs/2510.22787", "authors": ["Kamil Szczepanik", "Jaros\u0142aw A. Chudziak"], "title": "Collaborative LLM Agents for C4 Software Architecture Design Automation", "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59), 2026, Hawaii, USA.\n  The final published version will appear in the official conference\n  proceedings", "summary": "Software architecture design is a fundamental part of creating every software\nsystem. Despite its importance, producing a C4 software architecture model, the\npreferred notation for such architecture, remains manual and time-consuming. We\nintroduce an LLM-based multi-agent system that automates this task by\nsimulating a dialogue between role-specific experts who analyze requirements\nand generate the Context, Container, and Component views of the C4 model.\nQuality is assessed with a hybrid evaluation framework: deterministic checks\nfor structural and syntactic integrity and C4 rule consistency, plus semantic\nand qualitative scoring via an LLM-as-a-Judge approach. Tested on five\ncanonical system briefs, the workflow demonstrates fast C4 model creation,\nsustains high compilation success, and delivers semantic fidelity. A comparison\nof four state-of-the-art LLMs shows different strengths relevant to\narchitectural design. This study contributes to automated software architecture\ndesign and its evaluation methods.", "AI": {"tldr": "An LLM-based multi-agent system automates C4 software architecture model creation by simulating expert dialogues, with hybrid evaluation combining deterministic checks and LLM-as-a-Judge scoring.", "motivation": "Manual creation of C4 software architecture models is time-consuming despite being fundamental to software system design, creating a need for automation.", "method": "Multi-agent LLM system with role-specific experts analyzing requirements to generate Context, Container, and Component views, evaluated through hybrid framework with deterministic checks and LLM-as-a-Judge scoring.", "result": "The workflow enables fast C4 model creation with high compilation success and semantic fidelity across five canonical system briefs, with different LLMs showing varied architectural design strengths.", "conclusion": "This study advances automated software architecture design and evaluation methods, demonstrating effective LLM-based automation for C4 model generation."}}
{"id": "2510.22815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22815", "abs": "https://arxiv.org/abs/2510.22815", "authors": ["Vasudev Vikram", "Yuvraj Agarwal", "Rohan Padhye"], "title": "On the Freshness of Pinned Dependencies in Maven", "comment": null, "summary": "Library dependencies in software ecosystems play a crucial role in the\ndevelopment of software. As newer releases of these libraries are published,\ndevelopers may opt to pin their dependencies to a particular version. While\npinning may have benefits in ensuring reproducible builds and avoiding breaking\nchanges, it bears larger risks in using outdated dependencies that may contain\nbugs and security vulnerabilities. To understand the frequency and consequences\nof dependency pinning, we first define the concepts of stale and fresh pins,\nwhich are distinguished based on how outdated the dependency is relative to the\nrelease date of the project. We conduct an empirical study to show that over\n60% of consumers of popular Maven libraries contain stale pins to their\ndependencies, with some outdated versions over a year old. These pinned\nversions often miss out on security fixes; we find that 10% of all dependency\nupgrades in our dataset to the latest minor or patch version would reduce\nsecurity vulnerabilities.\n  We prototype an approach called Pin-Freshener that can encourage developers\nto freshen their pins by leveraging the insight that crowdsourced tests of peer\nprojects can provide additional signal for the safety of an upgrade. Running\nPin-Freshener on dependency upgrades shows that just 1-5 additional test suites\ncan provide 35-100% more coverage of a dependency, compared to that of a single\nconsumer test suite. Our evaluation on real-world pins to the top 500 popular\nlibraries in Maven shows that Pin-Freshener can provide an additional signal of\nat least 5 passing crowdsourced test suites to over 3,000 consumers to safely\nperform an upgrade that reduces security vulnerabilities. Pin-Freshener can\nprovide practical confidence to developers by offering additional signal beyond\ntheir own test suites, representing an improvement over current practices.", "AI": {"tldr": "The paper studies dependency pinning in software ecosystems, finding that over 60% of Maven consumers use stale dependencies that miss security fixes. It introduces Pin-Freshener, a tool that uses crowdsourced tests from peer projects to provide additional safety signals for dependency upgrades.", "motivation": "Dependency pinning ensures reproducible builds but risks using outdated versions with bugs and security vulnerabilities. The research aims to understand the prevalence and consequences of stale dependencies and provide a solution to encourage safer upgrades.", "method": "Defined stale vs fresh pins based on dependency outdatedness. Conducted empirical study on Maven libraries. Developed Pin-Freshener prototype that leverages crowdsourced tests from peer projects to provide additional safety signals for dependency upgrades.", "result": "Found over 60% of Maven consumers use stale pins, some over a year old. 10% of dependency upgrades would reduce security vulnerabilities. Pin-Freshener shows 1-5 additional test suites can provide 35-100% more coverage. Can provide safety signals for over 3,000 consumers to perform security-reducing upgrades.", "conclusion": "Pin-Freshener offers practical confidence to developers by providing additional safety signals beyond their own test suites, representing an improvement over current dependency management practices and helping reduce security vulnerabilities."}}
{"id": "2510.22986", "categories": ["cs.SE", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22986", "abs": "https://arxiv.org/abs/2510.22986", "authors": ["Junjie Huang", "Minghua He", "Jinyang Liu", "Yintong Huo", "Domenico Bianculli", "Michael R. Lyu"], "title": "CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs", "comment": null, "summary": "Log-based anomaly detection (LogAD) is critical for maintaining the\nreliability and availability of large-scale online service systems. While\nmachine learning, deep learning, and large language models (LLMs)-based methods\nhave advanced the LogAD, they often suffer from limited interpretability, high\ninference costs, and extensive preprocessing requirements, limiting their\npracticality for real-time, high-volume log analysis. In contrast, rule-based\nsystems offer efficiency and transparency, but require significant manual\neffort and are difficult to scale across diverse and evolving environments. In\nthis paper, We present CodeAD, a novel framework that automatically synthesizes\nlightweight Python rule functions for LogAD using LLMs. CodeAD introduces a\nhierarchical clustering and anchor-grounded sampling strategy to construct\nrepresentative contrastive log windows, enabling LLMs to discern discriminative\nanomaly patterns. To ensure robustness and generalizability, CodeAD employs an\nagentic workflow that iteratively generates, tests, repairs, and refines the\nrules until it meets correctness and abstraction requirements. The synthesized\nrules are interpretable, lightweight, and directly executable on raw logs,\nsupporting efficient and transparent online anomaly detection. Our\ncomprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)\ndemonstrate that CodeAD achieves an average absolute improvement of 3.6% F1\nscore over the state-of-the-art baselines, while processing large datasets up\nto 4x faster and at a fraction of the cost (total LLM invocation cost under 4\nUSD per dataset). These results highlight CodeAD as a practical and scalable\nsolution for online monitoring systems, enabling interpretable, efficient, and\nautomated LogAD in real-world environment.", "AI": {"tldr": "CodeAD is a framework that automatically generates lightweight Python rule functions for log-based anomaly detection using LLMs, achieving better performance and efficiency than existing methods.", "motivation": "Existing log-based anomaly detection methods suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, while rule-based systems require manual effort and are hard to scale.", "method": "Uses hierarchical clustering and anchor-grounded sampling to create contrastive log windows, then employs an agentic workflow to iteratively generate, test, repair, and refine Python rules using LLMs.", "result": "Achieves 3.6% F1 score improvement over state-of-the-art baselines, processes datasets 4x faster, with total LLM cost under $4 per dataset.", "conclusion": "CodeAD provides a practical, scalable solution for interpretable, efficient, and automated log-based anomaly detection in real-world environments."}}
{"id": "2510.23010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23010", "abs": "https://arxiv.org/abs/2510.23010", "authors": ["Ming-Tung Shen", "Yuh-Jzer Joung"], "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation", "comment": null, "summary": "Agentic code generation requires large language models (LLMs) capable of\ncomplex context management and multi-step reasoning. Prior multi-agent\nframeworks attempt to address these challenges through collaboration, yet they\noften suffer from rigid workflows and high reasoning recovery costs. To\novercome these limitations, we propose TALM (Tree-Structured Multi-Agent\nFramework with Long-Term Memory), a dynamic framework that integrates\nstructured task decomposition, localized re-reasoning, and long-term memory\nmechanisms. TALM employs an extensible tree-based collaboration structure. The\nparent-child relationships, when combined with a divide-and-conquer strategy,\nenhance reasoning flexibility and enable efficient error correction across\ndiverse task scopes. Furthermore, a long-term memory module enables semantic\nquerying and integration of prior knowledge, supporting implicit\nself-improvement through experience reuse. Experimental results on HumanEval,\nBigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently\ndelivers strong reasoning performance and high token efficiency, highlighting\nits robustness and practical utility in complex code generation tasks.", "AI": {"tldr": "TALM is a tree-structured multi-agent framework with long-term memory that improves code generation through dynamic task decomposition, localized reasoning, and experience reuse.", "motivation": "Existing multi-agent frameworks for code generation suffer from rigid workflows and high reasoning recovery costs, limiting their effectiveness in complex context management and multi-step reasoning tasks.", "method": "TALM uses an extensible tree-based collaboration structure with parent-child relationships, divide-and-conquer strategy, localized re-reasoning, and a long-term memory module for semantic querying and experience integration.", "result": "Experimental results on HumanEval, BigCodeBench, and ClassEval benchmarks show TALM delivers strong reasoning performance and high token efficiency in complex code generation tasks.", "conclusion": "TALM demonstrates robustness and practical utility in agentic code generation by enabling flexible reasoning, efficient error correction, and implicit self-improvement through experience reuse."}}
{"id": "2510.23055", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23055", "abs": "https://arxiv.org/abs/2510.23055", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek D\u0105browski"], "title": "From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks", "comment": null, "summary": "[Context and Motivation] Online user feedback provides valuable information\nto support requirements engineering (RE). However, analyzing online user\nfeedback is challenging due to its large volume and noise. Large language\nmodels (LLMs) show strong potential to automate this process and outperform\nprevious techniques. They can also enable new tasks, such as generating\nrequirements specifications.\n  [Question-Problem] Despite their potential, the use of LLMs to analyze user\nfeedback for RE remains underexplored. Existing studies offer limited empirical\nevidence, lack thorough evaluation, and rarely provide replication packages,\nundermining validity and reproducibility.\n  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on\nthree RE tasks: user request classification, NFR classification, and\nrequirements specification generation. Classification performance was measured\non two feedback datasets, and specification quality via human evaluation. LLMs\nachieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and\nmoderately high specification quality (mean ~ 3/5).\n  [Contributions] We newly explore lightweight LLMs for feedback-driven\nrequirements development. Our contributions are: (i) an empirical evaluation of\nlightweight LLMs on three RE tasks, (ii) a replication package, and (iii)\ninsights into their capabilities and limitations for RE.", "AI": {"tldr": "Evaluation of five lightweight open-source LLMs on three RE tasks using user feedback, showing moderate-to-high performance in classification and specification generation.", "motivation": "Online user feedback is valuable for RE but challenging to analyze due to volume and noise. LLMs show potential to automate this process and enable new tasks like requirements specification generation.", "method": "Evaluated five lightweight open-source LLMs on three RE tasks: user request classification, NFR classification, and requirements specification generation. Used two feedback datasets for classification and human evaluation for specification quality.", "result": "LLMs achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and moderately high specification quality (mean ~ 3/5).", "conclusion": "Lightweight LLMs show promise for feedback-driven requirements development, though limitations exist. The study provides empirical evidence, replication package, and insights into LLM capabilities for RE."}}
{"id": "2510.23068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23068", "abs": "https://arxiv.org/abs/2510.23068", "authors": ["Ella Dodor", "Cristina V. Lopes"], "title": "Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs", "comment": "11 pages, 9 figures, tool link:\n  https://github.com/ellacodee/CheckstylePlus", "summary": "Good code style improves program readability, maintainability, and\ncollaboration, and is an integral component of software quality. Developers,\nhowever, often cut corners when following style rules, leading to the wide\nadoption of tools such as linters in professional software development\nprojects. Traditional linters like Checkstyle operate using rigid, rule-based\nmechanisms that effectively detect many surface-level violations. However, in\nmost programming languages, there is a subset of style rules that require a\nmore nuanced understanding of code, and fall outside the scope of such static\nanalysis. In this paper, we propose Checkstyle+, a hybrid approach that\naugments Checkstyle with large language model (LLM) capabilities, to identify\nstyle violations that elude the conventional rule-based analysis. Checkstyle+\nis evaluated on a sample of 380 Java code files, drawn from a broader dataset\nof 30,800 real-world Java programs sourced from accepted Codeforces\nsubmissions. The results show that Checkstyle+ achieves superior performance\nover standard Checkstyle in detecting violations of the semantically nuanced\nrules.", "AI": {"tldr": "Checkstyle+ combines traditional rule-based linter Checkstyle with LLM capabilities to detect nuanced code style violations that conventional static analysis misses.", "motivation": "Traditional linters like Checkstyle use rigid rule-based mechanisms that miss semantically nuanced style rules requiring deeper code understanding.", "method": "Hybrid approach augmenting Checkstyle with large language model capabilities to identify style violations beyond conventional rule-based analysis.", "result": "Evaluated on 380 Java code files from 30,800 real-world programs, Checkstyle+ achieved superior performance over standard Checkstyle in detecting violations of semantically nuanced rules.", "conclusion": "The hybrid LLM-augmented approach effectively addresses limitations of traditional rule-based linters for detecting nuanced code style violations."}}
{"id": "2510.23350", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.5"], "pdf": "https://arxiv.org/pdf/2510.23350", "abs": "https://arxiv.org/abs/2510.23350", "authors": ["Alcino Cunha", "Nuno Macedo"], "title": "Validating Formal Specifications with LLM-generated Test Cases", "comment": null, "summary": "Validation is a central activity when developing formal specifications.\nSimilarly to coding, a possible validation technique is to define upfront test\ncases or scenarios that a future specification should satisfy or not.\nUnfortunately, specifying such test cases is burdensome and error prone, which\ncould cause users to skip this validation task. This paper reports the results\nof an empirical evaluation of using pre-trained large language models (LLMs) to\nautomate the generation of test cases from natural language requirements. In\nparticular, we focus on test cases for structural requirements of simple domain\nmodels formalized in the Alloy specification language. Our evaluation focuses\non the state-of-art GPT-5 model, but results from other closed- and open-source\nLLMs are also reported. The results show that, in this context, GPT-5 is\nalready quite effective at generating positive (and negative) test cases that\nare syntactically correct and that satisfy (or not) the given requirement, and\nthat can detect many wrong specifications written by humans.", "AI": {"tldr": "LLMs can effectively generate test cases from natural language requirements for Alloy specifications, with GPT-5 showing strong performance in creating syntactically correct test cases that detect human-written specification errors.", "motivation": "Validation of formal specifications is crucial but test case creation is burdensome and error-prone, causing users to skip this important validation task.", "method": "Empirical evaluation using pre-trained LLMs (primarily GPT-5) to automatically generate test cases from natural language requirements for Alloy specification language domain models.", "result": "GPT-5 effectively generates syntactically correct positive and negative test cases that satisfy (or violate) given requirements and can detect many wrong specifications written by humans.", "conclusion": "LLMs, particularly GPT-5, show promise in automating test case generation for formal specification validation, potentially reducing the burden of manual test creation."}}
{"id": "2510.23389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23389", "abs": "https://arxiv.org/abs/2510.23389", "authors": ["Edoardo Manino", "Bruno Farias", "Rafael S\u00e1 Menezes", "Fedor Shmarov", "Lucas C. Cordeiro"], "title": "Floating-Point Neural Network Verification at the Software Level", "comment": "Pre-print before submission to peer review", "summary": "The behaviour of neural network components must be proven correct before\ndeployment in safety-critical systems. Unfortunately, existing neural network\nverification techniques cannot certify the absence of faults at the software\nlevel. In this paper, we show how to specify and verify that neural networks\nare safe, by explicitly reasoning about their floating-point implementation. In\ndoing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural\nnetwork verification examples that cover activation functions, common layers,\nand full neural networks of up to 170K parameters. Our verification suite is\nwritten in plain C and is compatible with the format of the International\nCompetition on Software Verification (SV-COMP). Thanks to it, we can conduct\nthe first rigorous evaluation of eight state-of-the-art software verifiers on\nneural network code. The results show that existing automated verification\ntools can correctly solve an average of 11% of our benchmark, while producing\naround 3% incorrect verdicts. At the same time, a historical analysis reveals\nthat the release of our benchmark has already had a significantly positive\nimpact on the latter.", "AI": {"tldr": "The paper presents NeuroCodeBench 2.0, a benchmark for verifying neural network implementations in C, and evaluates eight software verifiers showing they correctly solve only 11% of cases while producing 3% incorrect verdicts.", "motivation": "Existing neural network verification techniques cannot certify the absence of faults at the software level, which is critical for safety-critical systems deployment.", "method": "Constructed NeuroCodeBench 2.0 with 912 neural network verification examples in plain C, covering activation functions, common layers, and full networks up to 170K parameters, compatible with SV-COMP format.", "result": "Evaluation of eight state-of-the-art software verifiers showed they correctly solve an average of 11% of benchmark cases while producing around 3% incorrect verdicts.", "conclusion": "The benchmark has already had a positive impact on verifier development, but current automated verification tools have limited effectiveness for neural network code verification."}}
{"id": "2510.23528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23528", "abs": "https://arxiv.org/abs/2510.23528", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Tracing Distribution Shifts with Causal System Maps", "comment": null, "summary": "Monitoring machine learning (ML) systems is hard, with standard practice\nfocusing on detecting distribution shifts rather than their causes. Root-cause\nanalysis often relies on manual tracing to determine whether a shift is caused\nby software faults, data-quality issues, or natural change. We propose ML\nSystem Maps -- causal maps that, through layered views, make explicit the\npropagation paths between the environment and the ML system's internals,\nenabling systematic attribution of distribution shifts. We outline the approach\nand a research agenda for its development and evaluation.", "AI": {"tldr": "Proposes ML System Maps - causal maps that enable systematic attribution of distribution shifts in ML systems through layered views showing propagation paths between environment and system internals.", "motivation": "Current ML system monitoring focuses on detecting distribution shifts but lacks systematic methods to identify root causes, relying on manual tracing to distinguish between software faults, data-quality issues, or natural changes.", "method": "Develops ML System Maps - causal maps with layered views that explicitly show propagation paths between the environment and ML system's internals, enabling systematic root-cause analysis.", "result": "The approach provides a framework for systematic attribution of distribution shifts in ML systems, moving beyond simple detection to understanding underlying causes.", "conclusion": "ML System Maps offer a promising approach for comprehensive ML system monitoring and root-cause analysis, with a research agenda outlined for further development and evaluation."}}
