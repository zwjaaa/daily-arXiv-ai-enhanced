<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec is an infrastructure that represents developers' eye movement transitions between syntactic elements using distributed representations to simplify eye-tracking analysis in program comprehension studies.


<details>
  <summary>Details</summary>
Motivation: Traditional eye-tracking studies require researchers to manually predefine analysis targets and metrics, which is time-consuming and leads to varying results depending on how Areas of Interest (AOIs) are defined.

Method: The system represents continuous eye fixations as transitions between syntactic elements using distributed representations, enabling semantic interpretation of eye movement patterns.

Result: The distributed representation approach facilitates the adoption of diverse data analysis methods with rich semantic interpretations for eye-tracking data.

Conclusion: eye2vec provides a more flexible and automated infrastructure for analyzing developers' eye movements during code reading, reducing manual effort and enabling richer semantic analysis.

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [2] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LLMs face challenges with verbose, noisy data in workflows. Treating token budget as attention budget and prioritizing task-relevant information through text reduction can improve efficiency and sustainability.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in data-intensive workflows but struggle with verbose, noisy data like logs and telemetry. Directly feeding such data is costly, unsustainable, and misaligned with tasks. Current efficiency efforts focus on model-level optimizations while input verbosity reduction remains underexplored.

Method: Propose treating LLM token budget as an attention budget and elevating task-aware text reduction as a core design principle. Position input-side reduction as attention allocation rather than compression, prioritizing information relevant to downstream tasks.

Result: The paper outlines research challenges for building benchmarks, designing adaptive reduction pipelines, and integrating token-budget-aware preprocessing into database and retrieval systems.

Conclusion: Channeling scarce attention resources toward meaningful signals in noisy data-intensive workflows enables scalable, accurate, and sustainable LLM-data integration.

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [3] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi is an LLM-powered framework that uses procedural knowledge from historical issue-fixing data to guide agents in solving repository-level software engineering issues, achieving 74.6% success rate on SWE-bench.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered agents struggle with complex repository-level issue resolution due to lack of procedural knowledge and reliance on brute-force computational exploration.

Method: Constructs procedural knowledge offline through hierarchical abstraction, then uses knowledge-driven scaling to analyze target issues from multiple perspectives using knowledge from similar historical issues.

Result: Achieves 74.6% bug resolution rate on SWE-bench Verified benchmark, outperforming five state-of-the-art techniques by 5.4% to 14.9%. Ablation studies confirm procedural knowledge is crucial for performance gains.

Conclusion: Procedural knowledge extraction and application is key to effective repository-level issue resolution, with 'design patterns & coding practices' being the most critical knowledge aspect, and different knowledge aspects playing varying roles across analysis, planning, and fixing stages.

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [4] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge is a framework that simplifies deploying and testing distributed multi-agent AI systems by decoupling application logic from deployment choices and automatically generating necessary code.


<details>
  <summary>Details</summary>
Motivation: Deploying and testing AI agents as distributed systems is challenging due to the rapidly evolving landscape of programming frameworks and protocols, requiring significant manual effort.

Method: DMAS-Forge decouples application logic from specific deployment choices and transparently generates the necessary glue code and configurations to spawn distributed multi-agent applications across diverse deployment scenarios.

Result: The framework enables minimal manual effort deployment of distributed multi-agent applications across various scenarios, with a prototype implementation presented.

Conclusion: DMAS-Forge addresses the complexity of deploying distributed AI agent systems and presents opportunities for future development in this area.

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [5] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor is a high-performance Python library for cardiac electrophysiology simulations using GPUs, built on PyTorch to accelerate simulations especially for large 3D meshes.


<details>
  <summary>Details</summary>
Motivation: Cardiac electrophysiology simulations typically require high-performance computing with many CPU cores, which are often inaccessible to many research groups and clinicians.

Method: Developed TorchCor, a Python library using finite element method on general-purpose GPUs, built on PyTorch framework.

Result: Significantly accelerates CEP simulations, particularly for large 3D meshes. Accuracy verified against analytical solutions and benchmark problems.

Conclusion: TorchCor provides accessible high-performance CEP simulation capabilities and is freely available for both academic and commercial use without restrictions.

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [6] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: Contextual information (version history and call graphs) significantly improves neural models' performance on code comprehension tasks like clone detection and code summarization.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for program comprehension rely solely on source code, missing important contextual signals like version history and structural relationships that could enhance understanding of code evolution and operation.

Method: Empirical study evaluating 5 neural models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) on two datasets (SeSaMe and CodeSearchNet) for clone detection and code summarization tasks, comparing code-only vs context-augmented settings with version history and call graphs.

Result: Context generally improves performance: version history consistently boosts clone detection (e.g., CodeT5 +15.92% F1) and summarization (e.g., GraphCodeBERT +5.56% METEOR), with combined contexts yielding up to +21.48% macro-F1 improvement. Human evaluation shows significant preference for context-augmented summaries.

Conclusion: Contextual signals have strong potential to enhance code comprehension and open new directions for optimizing contextual encoding in neural software engineering models.

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [7] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: SEMAP is a protocol-layer methodology that applies software engineering principles to multi-agent LLM systems, reducing failures in SE tasks through explicit contracts, structured messaging, and lifecycle-guided execution.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM systems for software engineering often fail due to under-specification, coordination misalignment, and inappropriate verification, lacking foundational SE structuring principles.

Method: SEMAP implements three core SE design principles: explicit behavioral contract modeling, structured messaging, and lifecycle-guided execution with verification, built atop Google's Agent-to-Agent infrastructure.

Result: SEMAP reduces failures significantly across SE tasks: up to 69.6% reduction in function-level code development, 56.7% in deployment-level development, 47.4% in Python vulnerability detection, and 28.2% in C/C++ vulnerability detection.

Conclusion: SEMAP effectively addresses core deficiencies in multi-agent LLM systems for software engineering by applying structured SE principles, demonstrating substantial failure reduction across various development and security tasks.

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [8] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: iCodeReviewer is an automated secure code review system using LLMs with a mixture-of-prompts architecture and routing algorithm to improve security issue detection with 63.98% F1 score and 84% acceptance rate.


<details>
  <summary>Details</summary>
Motivation: Current automated secure code review approaches (static analysis, deep learning, prompting) have limited precision, coverage, and lack comprehensive evaluation, creating a need for more effective solutions.

Method: Uses large language models with mixture-of-prompts architecture where multiple prompt experts check specific security issues, plus routing algorithm to activate only necessary experts based on code features.

Result: Achieved 63.98% F1 score for security issue identification and localization, with 84% acceptance rate for generated review comments in production environments.

Conclusion: iCodeReviewer effectively addresses limitations of existing approaches by combining LLMs with specialized prompt experts and intelligent routing, demonstrating practical value in real-world deployment.

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [9] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: This paper presents an LLM-assisted scoping review of verbalization techniques in software engineering research, examining themes and evaluating LLM effectiveness for interdisciplinary reviews.


<details>
  <summary>Details</summary>
Motivation: To understand how software developers think and behave, and to explore the feasibility of using LLMs for large-scale interdisciplinary literature reviews.

Method: Used GPT-based LLM screening pipeline to assess relevance of 9,000+ papers from SE-psychology intersection, focusing on verbal data methods, with human validation of LLM outputs.

Result: Found high consistency between GPT and human reviewers (13% disagreement rate), identified prominent SE craft themes with underrepresented human-centered topics, and observed one-way knowledge flow from psychology to SE.

Conclusion: LLMs are effective for interdisciplinary review processes, verbalization techniques provide valuable insights into developer cognition, and there's an opportunity for more balanced knowledge exchange between SE and psychology.

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [10] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: The paper investigates Vibe Coding (VC) - an intuitive, affect-driven programming paradigm that contrasts with conventional AI-assisted development approaches like GitHub Copilot, using interviews with practitioners to identify key themes and conceptualizing VC as co-drifting rather than co-piloting.


<details>
  <summary>Details</summary>
Motivation: To explore how emerging GenAI capabilities enable new intuitive programming practices that diverge from conventional approaches, and understand the implications of this Vibe Coding paradigm for software development culture.

Method: Conducted five semi-structured interview sessions with ten experienced software practitioners to identify thematic dimensions of Vibe Coding.

Result: Identified five key themes: creativity, sustainability, future of programming, collaboration, and criticism. Conceptualized VC as co-drifting (contrasting with co-piloting), showing it reconfigures developer roles and blurs boundaries between professionals and non-developers.

Conclusion: Vibe Coding represents a meaningful shift in programming culture that enables novel expression and rapid prototyping but introduces challenges in reproducibility, scalability, and inclusivity, warranting further HCI and software engineering research.

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [11] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: Examines how major global events like Black Friday impact cloud benchmarking performance variability.


<details>
  <summary>Details</summary>
Motivation: To investigate if major global events affect cloud performance benchmark results, building on previous findings about daily/weekly performance patterns.

Method: Extended previous study by examining performance benchmarks during Black Friday, comparing results to normal periods.

Result: Found that performance variability exists at application level but is less pronounced than commonly assumed; identified subtle daily/weekly patterns.

Conclusion: Major global events can impact cloud benchmarking performance, confirming the need to consider temporal factors in cloud performance experiments.

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [12] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: SysMLv2 enables domain-specific language creation for Digital Twin evolution management, demonstrated through DarTwin DSL development, though current tooling has graphical notation limitations.


<details>
  <summary>Details</summary>
Motivation: To evaluate SysMLv2's new domain-specific language extension capabilities and apply them to Digital Twin evolution management by formalizing the existing DarTwin notation.

Method: Developed DarTwin DSL through SysMLv2 to formalize Digital Twin evolution notation, evaluated using concrete use cases and existing tooling capabilities.

Result: Successfully created DarTwin DSL enabling wide application of evolution templates using SysMLv2 tools, but identified limitations in current graphical notation capabilities.

Conclusion: SysMLv2 effectively facilitates DSL creation for Digital Twin evolution management, integrating systematic approaches with systems engineering, though tooling improvements are needed for better graphical support.

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [13] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ is a benchmark for code-diff understanding with three tasks: apply, anti-apply, and diff generation, using real commits from CommitPackFT.


<details>
  <summary>Details</summary>
Motivation: Reliable handling of code diffs is crucial for agents that edit and refactor repositories at scale, requiring a standardized benchmark for evaluation.

Method: The benchmark consists of triples (old code, new code, diff) from real commits, with automatic metrics and evaluation protocol. It studies unified diff format and compares different diff representations.

Result: Different diff formats perform better depending on use case and model size. Search-replace format works well for larger models in diff generation but not for diff analysis and smaller models.

Conclusion: Diff-XYZ provides a reusable foundation for assessing and improving diff handling in LLMs, aiding future development of diff formats and code editing models.

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [14] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: Developed and validated two domain-specific empathy scales for software engineering: EmpathiSEr-P (practitioner empathy) and EmpathiSEr-U (practitioner empathy towards users), addressing the lack of validated instruments for measuring empathy in SE contexts.


<details>
  <summary>Details</summary>
Motivation: Existing generic empathy scales from psychology and healthcare are not meaningful for software practitioners as they don't account for role-specific, domain-bound expressions of empathy in SE contexts like understanding technical constraints or user frustrations.

Method: Rigorous multi-phase methodology including expert evaluation, cognitive interviews, and two practitioner surveys to develop scales covering three empathy dimensions: cognitive empathy, affective empathy, and empathic responses.

Result: Created the first psychometrically validated empathy scales specifically tailored to software engineering, providing instruments to assess empathy in software teams and user interactions.

Conclusion: The EmpathiSEr scales fill a critical gap by offering validated tools for researchers and practitioners to measure empathy and design empathy-enhancing interventions in software engineering contexts.

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [15] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: Empirical evaluation shows that simplified energy models for web sustainability reporting (like Digst and DIMPACT) have systematic deviations from actual energy consumption, varying by website category, device type, and task characteristics.


<details>
  <summary>Details</summary>
Motivation: To evaluate the accuracy and precision of widely adopted simplified energy and carbon models (Digst framework and DIMPACT model) for web-based services sustainability reporting, as their reliability remains underexplored.

Method: Conducted empirical study measuring actual energy consumption during realistic user interactions with shopping, booking, navigation, and news websites using predefined user flows executed on four laptop platforms.

Result: The constant-power approximation (P * t) commonly used in models diverges substantially from measured energy, with deviations being systematic rather than random, varying by website category, device type, and task characteristics.

Conclusion: There is a need for category-aware and device-reflective power parameters in reproducible sustainability reporting frameworks to improve accuracy of energy consumption estimates.

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [16] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: This systematic literature review synthesizes research on runtime composition in dynamic Systems of Systems (SoSs), identifying key challenges, solution strategies, tools, and evaluation methods through analysis of 80 primary studies.


<details>
  <summary>Details</summary>
Motivation: Modern SoSs operate in dynamic environments where runtime composition is crucial for adaptability, but the literature lacks cohesive synthesis of this topic.

Method: Conducted a Systematic Literature Review (SLR) screening 1,774 studies from 2019-2024 and selecting 80 primary studies for thematic analysis.

Result: Identified four challenge categories (modeling/analysis, resilient operations, system orchestration, CS heterogeneity) and seven solution areas. Service-oriented frameworks dominate tools, while simulation platforms support evaluation. Key gaps include interoperability issues and lack of standardized benchmarks.

Conclusion: Reveals tensions between autonomy vs coordination and modeling-reality gap. Calls for standardized metrics, scalable decentralized architectures, and cross-domain frameworks to guide development of dynamically composable SoSs.

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


### [17] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: LLMs can effectively generate formal functional contracts (preconditions and postconditions) from natural language hints in code, improving software verification by reducing false alarms compared to postcondition-only generation.


<details>
  <summary>Details</summary>
Motivation: Address the gap in automatic software verification adoption due to lack of formal specifications in real-world code, and reduce false alarms that occur when using LLM-generated postconditions alone.

Method: Introduce NL2Contract task using LLMs to translate natural language into formal functional contracts (preconditions and postconditions), with evaluation metrics for soundness, bug discriminative power, and usability in verification.

Result: LLMs effectively generate sound functional contracts that discriminate buggy from correct behavior, and verifiers with these contracts produce fewer false alarms than with postconditions alone. LLM-inferred preconditions align well with developer intentions.

Conclusion: NL2Contract enables effective use of automatic software verifiers to catch real-world bugs by generating comprehensive functional contracts from natural language, overcoming limitations of postcondition-only approaches.

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>
