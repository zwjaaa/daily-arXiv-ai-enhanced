{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezz\u00e8"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites.", "AI": {"tldr": "E-Test is an approach that uses LLMs to identify untested execution scenarios from production data and generates test cases to enhance test suites, achieving better performance than existing methods.", "motivation": "Test suites are imperfect and finding new test cases that explore untested execution scenarios is challenging and labor-intensive, especially for large test suites over time.", "method": "E-Test identifies untested executions from production scenarios and generates new test cases using Large Language Models (LLMs) to pinpoint scenarios not covered by current test suites.", "result": "Evaluation on 1,975 scenarios from open-source Java projects shows E-Test achieves F1-score of 0.55, significantly outperforming state-of-the-art approaches (max F1=0.34) and vanilla LLMs (max F1=0.39).", "conclusion": "E-Test effectively enhances test suites by targeting untested execution scenarios and reduces manual effort for test suite maintenance."}}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed.", "AI": {"tldr": "This paper introduces Spreadsheet Operations Documentation (SOD), an AI task that generates human-readable explanations from spreadsheet operations, and evaluates five LLMs on a benchmark of 111 spreadsheet code snippets.", "motivation": "Lack of systematic documentation methods for spreadsheets hinders automation, collaboration, and knowledge transfer, risking loss of institutional knowledge.", "method": "Created a benchmark of 111 spreadsheet manipulation code snippets with natural language summaries, then evaluated five LLMs (GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, Gemma2-9B) using BLEU, GLEU, ROUGE-L, and METEOR metrics.", "result": "LLMs can generate accurate spreadsheet documentation, making SOD a feasible prerequisite step toward enhancing reproducibility, maintainability, and collaborative workflows.", "conclusion": "SOD is a promising approach for spreadsheet documentation, though challenges remain that need to be addressed."}}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "KGACG is a knowledge-guided multi-agent framework for automated application-level code generation that transforms requirements and design documents into executable code through collaborative agents with feedback loops.", "motivation": "Existing multi-agent frameworks perform inadequately in large-scale application-level software code generation, failing to ensure reasonable organizational structures and maintainable code generation processes.", "method": "KGACG uses three collaborative agents (Code Organization & Planning Agent, Coding Agent, Testing Agent) with feedback mechanisms to transform software requirements and architectural design into executable code.", "result": "Demonstrated through a Java Tank Battle game case study, showing the collaborative process of agents while facing challenges in application-level code generation.", "conclusion": "KGACG advances the automation of application-level software development by providing a structured multi-agent approach with knowledge guidance and feedback mechanisms."}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "A novel method for generating realistic software bugs by having SWE agents introduce features that unintentionally break tests, producing more realistic training data than intentional bug injection methods.", "motivation": "High quality bugs are essential for training effective language model-based software engineering agents, but current methods generate unrealistic bugs through intentional perturbations rather than reflecting real development processes.", "method": "Instruct SWE agents to introduce features into codebases where they may unintentionally break tests, creating bugs that more closely resemble human-authored edits through natural development workflows.", "result": "The generated bugs provide more efficient training data, outperforming other datasets by 2% with only half the training data (1.2k vs. 3k bugs). FrogBoss (32B) achieved 54.6% pass@1 and FrogMini (14B) achieved 45.3% pass@1 on SWE-bench Verified.", "conclusion": "The approach generates more realistic bugs that lead to state-of-the-art performance in software engineering agents, demonstrating the importance of realistic bug generation methods for effective model training."}}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel B\u00f6hme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT.", "AI": {"tldr": "MuoFuzz is a greybox fuzzer that learns optimal mutator sequences by modeling conditional probabilities between mutators, achieving higher code coverage and bug detection than existing fuzzers.", "motivation": "The authors hypothesize that the order in which mutators are applied to seed inputs significantly impacts greybox fuzzer effectiveness, suggesting current approaches that use fixed or isolated mutator selection are suboptimal.", "method": "MuoFuzz learns conditional probabilities that the next mutator will yield interesting inputs given the previous mutator, then samples from these probabilities using random walks to generate mutator sequences.", "result": "MuoFuzz achieves the highest code coverage on FuzzBench and MAGMA benchmarks, finding four bugs missed by AFL++ and one missed by both AFL++ and MOPT.", "conclusion": "Learning and applying optimal mutator sequences significantly improves greybox fuzzing effectiveness compared to fixed or isolated mutator selection approaches."}}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research.", "AI": {"tldr": "FAIGMOE is a conceptual framework for GenAI adoption that addresses the unique challenges of midsize organizations (resource constraints) and enterprises (organizational complexity), filling a gap in existing technology adoption models.", "motivation": "Existing technology adoption frameworks (TAM, TOE, DOI) lack specificity for GenAI implementation across different organizational contexts, creating a critical gap in adoption literature.", "method": "FAIGMOE synthesizes technology adoption theory, organizational change management, and innovation diffusion into four interconnected phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization.", "result": "The framework provides scalable guidance on readiness assessment, strategic alignment, risk governance, technical architecture, and change management, incorporating GenAI-specific considerations like prompt engineering and hallucination management.", "conclusion": "FAIGMOE offers the first comprehensive conceptual framework for GenAI adoption across midsize and enterprise organizations, providing actionable implementation protocols and governance templates that require future empirical validation."}}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption", "AI": {"tldr": "Downgrading from artifact-based build tools like Bazel to language-specific alternatives incurs significant performance costs despite perceived maintainability benefits.", "motivation": "To understand the implications of build tool downgrades, particularly why teams abandon faster artifact-based tools for easier-to-maintain alternatives and what performance costs this entails.", "method": "Case study of Kubernetes project's downgrade from Bazel to Go Build, reproducing and analyzing full/incremental builds during downgrade period, then replicating study on four other projects that downgraded from Bazel.", "result": "Bazel builds are faster (23.06-38.66s vs 75.19s for full builds) but consume more memory (81.42-351.07MB) and impose greater CPU load at higher parallelism. Downgrading increases CI resource costs by up to 76%. Similar patterns observed in other projects.", "conclusion": "Abandoning artifact-based build tools tends to incur considerable performance costs for large projects, highlighting the need to balance maintainability benefits against performance trade-offs."}}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE.", "AI": {"tldr": "A model-driven reengineering process for migrating PL/SQL monolithic code from Oracle Forms to Java using KDM models and TDD-like approach with three validation types.", "motivation": "To address the migration needs of enterprises moving from RAD platforms like Oracle Forms to modern technologies, specifically converting PL/SQL code to Java.", "method": "Model-driven reengineering using KDM models to represent legacy code, with TDD-like approach for incremental development of model transformations and three types of validations for generated code.", "result": "Development of a migration tool that successfully converts PL/SQL code from Forms triggers and program units to Java code separated in multiple tiers.", "conclusion": "Model-driven engineering techniques can be effectively applied to reengineering scenarios, providing a systematic approach for migrating legacy RAD applications to modern technologies."}}
{"id": "2510.20211", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20211", "abs": "https://arxiv.org/abs/2510.20211", "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"], "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents", "comment": null, "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally,\ncloud consoles, command-line interfaces (CLI), and SDKs are the tools of\nchoice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have\nquickly gained popularity. Unlike conventional tools, IaC~frameworks encode the\ninfrastructure in a \"source-of-truth\" configuration. They are capable of\nautomatically carrying out modifications to the cloud -- deploying, updating,\nor destroying resources -- to bring the actual infrastructure into alignment\nwith the IaC configuration. However, when IaC is used alongside consoles, CLIs,\nor SDKs, it loses visibility into external changes, causing infrastructure\ndrift, where the configuration becomes outdated, and later IaC operations may\nundo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates\nout-of-band changes back into the IaC program. Our key insight is that\ninfrastructure changes eventually all occur via cloud API invocations -- the\nlowest layer for cloud management operations. NSync gleans insights from API\ntraces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update\nthe IaC configuration to capture the changes). It employs an agentic\narchitecture that leverages LLMs to infer high-level intents from noisy API\nsequences, synthesize targeted IaC updates using specialized tools, and\ncontinually improve through a self-evolving knowledge base of past\nreconciliations. We further introduce a novel evaluation pipeline for injecting\nrealistic drifts into cloud infrastructure and assessing reconciliation\nperformance. Experiments across five real-world Terraform projects and 372\ndrift scenarios show that NSync outperforms the baseline both in terms of\naccuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$\nimprovement).", "AI": {"tldr": "NSync is an automated system that reconciles infrastructure drift in Infrastructure-as-Code (IaC) by detecting non-IaC changes from cloud API traces and updating IaC configurations to capture those changes, using LLMs to infer intents and synthesize updates.", "motivation": "When IaC frameworks are used alongside traditional cloud management tools (consoles, CLIs, SDKs), they lose visibility into external changes, causing infrastructure drift where configurations become outdated and subsequent IaC operations may undo valid updates or trigger errors.", "method": "NSync uses an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. It detects drift from cloud API traces and reconciles it by updating IaC configurations.", "result": "Experiments across five real-world Terraform projects and 372 drift scenarios show NSync outperforms the baseline in accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47\u00d7 improvement).", "conclusion": "NSync effectively addresses infrastructure drift in IaC environments by automatically detecting and reconciling out-of-band changes through API trace analysis and LLM-powered intent inference, significantly improving reconciliation accuracy and efficiency."}}
{"id": "2510.20340", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20340", "abs": "https://arxiv.org/abs/2510.20340", "authors": ["Serena Cofano", "Daniel Williams", "Aman Sharma", "Martin Monperrus"], "title": "Classport: Designing Runtime Dependency Introspection for Java", "comment": null, "summary": "Runtime introspection of dependencies, i.e., the ability to observe which\ndependencies are currently used during program execution, is fundamental for\nSoftware Supply Chain security. Yet, Java has no support for it. We solve this\nproblem with Classport, a system that embeds dependency information into Java\nclass files, enabling the retrieval of dependency information at runtime. We\nevaluate Classport on six real-world projects, demonstrating the feasibility in\nidentifying dependencies at runtime. Runtime dependency introspection with\nClassport opens important avenues for runtime integrity checking.", "AI": {"tldr": "Classport enables runtime introspection of Java dependencies by embedding dependency information into class files, allowing identification of used dependencies during program execution.", "motivation": "Java lacks support for runtime dependency introspection, which is fundamental for Software Supply Chain security.", "method": "Classport embeds dependency information directly into Java class files, enabling retrieval of dependency information at runtime.", "result": "Evaluation on six real-world projects demonstrates feasibility in identifying dependencies at runtime.", "conclusion": "Runtime dependency introspection with Classport opens important avenues for runtime integrity checking."}}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities", "AI": {"tldr": "Software platforms act as structure preserving systems with consistent interfaces and behaviors that remain stable under specific transformations called symmetries.", "motivation": "To explore how architectural robustness in software platforms emerges from enforcing structural regularities and symmetries.", "method": "Analyzing software platforms as structure preserving systems that maintain consistent interfaces and behaviors under specific transformations.", "result": "The paper demonstrates that architectural robustness is achieved through the enforcement of structural regularities and symmetries in software platforms.", "conclusion": "Enforcing structural regularities and symmetries in software platforms leads to architectural robustness, making them more reliable and maintainable."}}
{"id": "2510.20403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20403", "abs": "https://arxiv.org/abs/2510.20403", "authors": ["Santiago Gil", "Ecem E. Ba\u015f", "Christian D. Jensen", "Sebastian Engelsgaard", "Giuseppe Abbiati", "Cl\u00e1udio Gomes"], "title": "FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards", "comment": "6 pages, Proceedings of the 2025 Annual Modeling and Simulation\n  Conference (ANNSIM)", "summary": "Distributed co-simulation plays a key role in enabling collaborative modeling\nand simulation by different stakeholders while protecting their Intellectual\nProperty (IP). Although IP protection is provided implicitly by co-simulation,\nthere is no consensus in the guidelines to conduct distributed co-simulation of\ncontinuous-time or hybrid systems with no exposure to potential hacking\nattacks. We propose an approach for distributed co-simulation on top of UniFMU\nwith enhanced cybersecurity and IP protection mechanisms, ensuring that the\nconnection is initiated by the client and the models and binaries live on\ntrusted platforms. We showcase the functionality of this approach using two\nco-simulation demos in four different network settings and analyze the\ntrade-off between IP-protected distribution and performance efficiency in these\nsettings.", "AI": {"tldr": "Proposes a distributed co-simulation approach using UniFMU with enhanced cybersecurity and IP protection, ensuring client-initiated connections and trusted platform model storage.", "motivation": "Distributed co-simulation enables collaborative modeling while protecting IP, but lacks consensus on cybersecurity guidelines for continuous-time/hybrid systems against hacking attacks.", "method": "Developed approach using UniFMU with enhanced cybersecurity mechanisms, client-initiated connections, and trusted platform model storage. Tested with two co-simulation demos in four network settings.", "result": "Successfully demonstrated functionality in various network configurations and analyzed trade-offs between IP-protected distribution and performance efficiency.", "conclusion": "The proposed approach provides enhanced cybersecurity and IP protection for distributed co-simulation while maintaining functionality across different network environments."}}
{"id": "2510.20514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20514", "abs": "https://arxiv.org/abs/2510.20514", "authors": ["Lea Salome Brugger", "Xavier Denis", "Peter M\u00fcller"], "title": "Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia", "comment": null, "summary": "Deductive verification is an effective method to ensure that a given system\nexposes the intended behavior. In spite of its proven usefulness and\nfeasibility in selected projects, deductive verification is still not a\nmainstream technique. To pave the way to widespread use, we present a study\ninvestigating the factors enabling successful applications of deductive\nverification and the underlying issues preventing broader adoption. We\nconducted semi-structured interviews with 30 practitioners of verification from\nboth industry and academia and systematically analyzed the collected data\nemploying a thematic analysis approach. Beside empirically confirming familiar\nchallenges, e.g., the high level of expertise needed for conducting formal\nproofs, our data reveal several underexplored obstacles, such as proof\nmaintenance, insufficient control over automation, and usability concerns. We\nfurther use the results from our data analysis to extract enablers and barriers\nfor deductive verification and formulate concrete recommendations for\npractitioners, tool builders, and researchers, including principles for\nusability, automation, and integration with existing workflows.", "AI": {"tldr": "Study identifies barriers and enablers for deductive verification adoption through interviews with 30 practitioners, revealing challenges like proof maintenance and automation control, and providing recommendations for different stakeholders.", "motivation": "Despite proven usefulness, deductive verification is not mainstream. The study aims to understand factors enabling successful applications and issues preventing broader adoption.", "method": "Conducted semi-structured interviews with 30 verification practitioners from industry and academia, followed by systematic thematic analysis of collected data.", "result": "Confirmed known challenges (e.g., high expertise requirements) and revealed underexplored obstacles like proof maintenance, insufficient automation control, and usability concerns.", "conclusion": "Extracted enablers and barriers for deductive verification, formulated concrete recommendations for practitioners, tool builders, and researchers regarding usability, automation, and workflow integration principles."}}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness.", "AI": {"tldr": "Systematic evaluation of LLMs for statement-level code fault localization, testing open-source and closed-source models on HumanEval-Java and Defects4J datasets with various prompting strategies.", "motivation": "LLMs show strong code repair capabilities but their effectiveness depends on upstream fault localization, which lacks comprehensive evaluation. This study addresses this gap.", "method": "Evaluated Qwen2.5-coder-32b-instruct, DeepSeek-V3, GPT-4.1 mini, and Gemini-2.5-flash on fault localization using standard prompts, few-shot examples, and chain-of-reasoning strategies.", "result": "Bug report context significantly improves performance. Few-shot learning shows diminishing returns, and chain-of-thought effectiveness depends on model's reasoning capabilities. Performance varies across accuracy, time efficiency, and cost dimensions.", "conclusion": "Study reveals performance characteristics and trade-offs of different LLMs in fault localization, providing insights for improving fault localization effectiveness and highlighting current LLM strengths."}}
{"id": "2510.20679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20679", "abs": "https://arxiv.org/abs/2510.20679", "authors": ["Jonas Klauke", "Tom Ohlmer", "Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Eric Bodden"], "title": "A Soundness and Precision Benchmark for Java Debloating Tools", "comment": "Preprint - accepted at the ACM Workshop on Software Supply Chain\n  Offensive Research and Ecosystem Defenses (SCORED '25)", "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software.", "AI": {"tldr": "Deblometer is a micro-benchmark with 59 test cases to evaluate Java debloating tools' soundness and precision. Evaluation of Deptrim, JShrink, and ProGuard reveals all tools remove required constructs, with dynamic class loading causing unsoundness across all tools.", "motivation": "Modern software has excessive dependencies (avg 36 per project, 80% transitive), with only 24.9% required at runtime. Many unused program constructs add unnecessary code, creating need for debloating tools that balance precision and soundness.", "method": "Developed Deblometer - a micro-benchmark with 59 test cases covering Java language features. Each test case has manually curated ground truth specifying necessary vs bloated classes, methods, and fields to measure soundness and precision.", "result": "All three tools (Deptrim, JShrink, ProGuard) remove required program constructs, causing changed semantics or execution crashes. Dynamic class loading introduces unsoundness in all tools. Deptrim retains more bloated constructs, ProGuard removes more required constructs, JShrink has annotation support issues.", "conclusion": "Current debloating tools have significant soundness issues, especially with dynamic class loading and annotations. There's a need to improve these tools to ensure stable and reliable debloated software."}}
{"id": "2510.20692", "categories": ["cs.SE", "cs.AI", "cs.FL", "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.20692", "abs": "https://arxiv.org/abs/2510.20692", "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"], "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization", "comment": "20 pages, 7 figures", "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies.", "AI": {"tldr": "LLMs show promise for access control policy analysis but have limitations in automated policy generation, achieving high syntactic correctness but low semantic equivalence (45.8-93.7% accuracy).", "motivation": "Cloud computing's growing use requires complex access control policies that are error-prone to write manually and difficult to analyze. LLMs' success in code tasks suggests potential for automating policy generation and analysis.", "method": "Investigated LLMs for policy synthesis and introduced semantic-based request summarization using LLMs to characterize allowed requests. Compared reasoning vs non-reasoning LLMs for policy generation accuracy.", "result": "LLMs generate syntactically correct policies but have permissiveness issues: 45.8% accuracy for non-reasoning LLMs vs 93.7% for reasoning LLMs. LLMs show promise when combined with symbolic approaches for policy analysis.", "conclusion": "LLMs face significant hurdles for automated policy generation but demonstrate promising results for policy analysis when integrated with symbolic methods, particularly for understanding existing policies."}}
