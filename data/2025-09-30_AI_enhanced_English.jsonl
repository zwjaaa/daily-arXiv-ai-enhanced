{"id": "2509.22908", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22908", "abs": "https://arxiv.org/abs/2509.22908", "authors": ["Sergiu Bursuc", "Theodore Ehrenborg", "Shaowei Lin", "Lacramioara Astefanoaei", "Ionel Emilian Chiosa", "Jure Kukovec", "Alok Singh", "Oliver Butterley", "Adem Bizid", "Quinn Dougherty", "Miranda Zhao", "Max Tan", "Max Tegmark"], "title": "A benchmark for vericoding: formally verified program synthesis", "comment": "25 pages, 1 figure; data available at\n  https://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "summary": "We present and test the largest benchmark for vericoding, LLM-generation of\nformally verified code from formal specifications - in contrast to vibe coding,\nwhich generates potentially buggy code from a natural language description. Our\nbenchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in\nVerus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find\nvericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny\nusing off-the-shelf LLMs. Adding natural-language descriptions does not\nsignificantly improve performance. We also find that LLM progress has improved\nprogress on pure Dafny verification from 68% to 96% over the past year. The\nbenchmark and vericoding results are shared at\nhttps://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "AI": {"tldr": "Largest benchmark for vericoding (formal code generation from specifications) with 12,504 specs across Dafny, Verus/Rust, and Lean. Success rates: 82% in Dafny, 44% in Verus/Rust, 27% in Lean using off-the-shelf LLMs.", "motivation": "To establish a comprehensive benchmark for evaluating LLM performance in generating formally verified code from formal specifications, distinguishing it from natural language-based coding.", "method": "Created benchmark with 12,504 formal specifications (3,029 Dafny, 2,334 Verus/Rust, 7,141 Lean) including 6,174 new problems. Tested off-the-shelf LLMs on vericoding tasks and measured success rates.", "result": "Vericoding success rates: 82% in Dafny, 44% in Verus/Rust, 27% in Lean. Natural language descriptions didn't significantly improve performance. Pure Dafny verification improved from 68% to 96% over past year.", "conclusion": "Vericoding is viable with current LLMs, with significant performance differences across verification languages. The benchmark enables systematic evaluation of formal code generation capabilities."}}
{"id": "2509.22978", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22978", "abs": "https://arxiv.org/abs/2509.22978", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chayanee Junplong", "Akara Supratak"], "title": "Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer", "comment": null, "summary": "Recent studies highlight various machine learning (ML)-based techniques for\ncode clone detection, which can be integrated into developer tools such as\nstatic code analysis. With the advancements brought by ML in code\nunderstanding, ML-based code clone detectors could accurately identify and\nclassify cloned pairs, especially semantic clones, but often operate as black\nboxes, providing little insight into the decision-making process. Post hoc\nexplainers, on the other hand, aim to interpret and explain the predictions of\nthese ML models after they are made, offering a way to understand the\nunderlying mechanisms driving the model's decisions. However, current post hoc\ntechniques require white-box access to the ML model or are computationally\nexpensive, indicating a need for advanced post hoc explainers. In this paper,\nwe propose a novel approach that leverages the in-context learning capabilities\nof large language models to elucidate the predictions made by the ML-based code\nclone detectors. We perform a study using ChatGPT-4 to explain the code clone\nresults inferred by GraphCodeBERT. We found that our approach is promising as a\npost hoc explainer by giving the correct explanations up to 98% and offering\ngood explanations 95% of the time. However, the explanations and the code line\nexamples given by the LLM are useful in some cases. We also found that lowering\nthe temperature to zero helps increase the accuracy of the explanation. Lastly,\nwe list the insights that can lead to further improvements in future work. This\nstudy paves the way for future studies in using LLMs as a post hoc explainer\nfor various software engineering tasks.", "AI": {"tldr": "This paper proposes using large language models (LLMs) like ChatGPT-4 as post hoc explainers for ML-based code clone detectors, achieving high accuracy in explaining predictions without requiring white-box model access.", "motivation": "ML-based code clone detectors are effective but operate as black boxes, lacking interpretability. Current post hoc explanation techniques require white-box access or are computationally expensive, creating a need for better explainability methods.", "method": "Leveraged ChatGPT-4's in-context learning capabilities to explain predictions made by GraphCodeBERT code clone detector. Explored the impact of temperature settings on explanation accuracy.", "result": "The approach achieved correct explanations up to 98% of the time and good explanations 95% of the time. Lowering temperature to zero increased explanation accuracy. However, the explanations and code line examples were only useful in some cases.", "conclusion": "LLMs show promise as post hoc explainers for code clone detection and other software engineering tasks. Future work should focus on improving the usefulness of explanations and code examples provided by LLMs."}}
{"id": "2509.23261", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23261", "abs": "https://arxiv.org/abs/2509.23261", "authors": ["Fei Gu", "Zi Liang", "Hongzong LI", "Jiahao MA"], "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "comment": null, "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems.", "AI": {"tldr": "AI-assisted programming creates a Matthew effect where LLM-generated code succeeds more in popular languages/frameworks, potentially reinforcing existing hierarchies and reducing ecosystem diversity.", "motivation": "To investigate how AI-assisted programming impacts software development dynamics and ecosystem evolution, beyond just code generation quality.", "method": "Large-scale experiments on thousands of algorithmic programming tasks and hundreds of framework selection tasks to analyze LLM performance across different programming languages and frameworks.", "result": "Found a strong Matthew effect - LLM-generated code has higher success rates in more popular programming languages and frameworks, suggesting AI systems reinforce existing popularity hierarchies.", "conclusion": "AI-assisted programming may accelerate convergence around dominant tools while hindering diversity and innovation in programming ecosystems."}}
{"id": "2509.23297", "categories": ["cs.SE", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.23297", "abs": "https://arxiv.org/abs/2509.23297", "authors": ["Anthony Savidis", "Christos Vasilopoulos"], "title": "Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics", "comment": null, "summary": "Software visualization seeks to represent software artifacts graphical-ly in\ntwo or three dimensions, with the goal of enhancing comprehension, anal-ysis,\nmaintenance, and evolution of the source code. In this context, visualiza-tions\nemploy graphical forms such as dependency structures, treemaps, or time-lines\nthat incorporate repository histories. These visualizations allow software\nengineers to identify structural patterns, detect complexity hotspots, and\ninfer system behaviors that are difficult to perceive directly from source\ntext. By adopting metaphor-based approaches, visualization tools provide\nmacroscopic overviews while enabling focused inspection of specific program\nelements, thus offering an accessible means of understanding large-scale\nsystems. The contri-bution of our work lies in three areas. First, we introduce\na configurable group-ing mechanism that supports flexible organization of code\nelements based on arbitrary relationships. Second, we combine fine-grained and\ncoarse-grained software metrics to provide a multi-level perspective on system\nproperties. Third, we present an interactive visualization engine that allows\ndevelopers to dynamically adjust rendering attributes. Collectively, these\nadvances provide a more adaptable and insightful approach to source code\ncomprehension.", "AI": {"tldr": "The paper presents a configurable software visualization system with flexible grouping, multi-level metrics, and interactive rendering to enhance source code comprehension.", "motivation": "To improve software comprehension, analysis, maintenance, and evolution by providing better visualization tools that can reveal structural patterns and complexity hotspots that are hard to see directly from source code.", "method": "Developed a configurable grouping mechanism for code elements, combined fine-grained and coarse-grained software metrics, and created an interactive visualization engine with dynamic rendering adjustment capabilities.", "result": "Created a more adaptable and insightful approach to source code visualization that provides macroscopic overviews while enabling focused inspection of specific program elements.", "conclusion": "The three contributions (configurable grouping, multi-level metrics, and interactive visualization) collectively provide a more effective framework for understanding large-scale software systems through visualization."}}
{"id": "2509.23469", "categories": ["cs.SE", "68N30", "D.2.8"], "pdf": "https://arxiv.org/pdf/2509.23469", "abs": "https://arxiv.org/abs/2509.23469", "authors": ["Mykola Kuz", "Ivan Yaremiy", "Hanna Yaremii", "Mykola Pikuliak", "Ihor Lazarovych", "Mykola Kozlenko", "Denys Vekeryk"], "title": "Methods for evaluating software accessibility", "comment": "10 pages, 4 figures, 1 table", "summary": "The development and enhancement of methods for evaluating software\naccessibility is a relevant challenge in modern software engineering, as\nensuring equal access to digital services is a key factor in improving their\nefficiency and inclusivity. The increasing digitalization of society\nnecessitates the creation of software that complies with international\naccessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these\nstandards helps eliminate barriers to software use for individuals with diverse\nphysical, sensory, and cognitive needs. Despite advancements in regulatory\nframeworks, existing accessibility evaluation methodologies are often\ngeneralized and fail to account for the specific needs of different user\ncategories or the unique ways they interact with digital systems. This\nhighlights the need for the development of new, more detailed methods for\ndefining metrics that influence the quality of user interaction with software\nproducts. Building a classification and mathematical model and developing\naccessibility assessment methods for software based on it. A method for\nassessing the quality subcharacteristic \"Accessibility\", which is part of the\n\"Usability\" quality characteristic, has been developed. This enabled the\nanalysis of a website's inclusivity for individuals with visual impairments,\nand the formulation of specific recommendations for further improvements, which\nis a crucial step toward creating an inclusive digital environment. Comparing\nto standardized approaches, a more detailed and practically oriented\naccessibility assessment methodology has been proposed. Using this methodology,\nan analysis of the accessibility of the main pages of Vasyl Stefanyk\nPrecarpathian National University's website was conducted, and improvements\nwere suggested to enhance its inclusivity.", "AI": {"tldr": "The paper presents a new detailed methodology for assessing software accessibility, specifically focusing on visual impairments, and applies it to analyze a university website.", "motivation": "Existing accessibility evaluation methods are too generalized and fail to address specific user needs and interaction patterns, necessitating more detailed assessment approaches.", "method": "Developed a classification and mathematical model for accessibility assessment, creating a method to evaluate the \"Accessibility\" subcharacteristic within the \"Usability\" quality characteristic.", "result": "Applied the methodology to analyze Vasyl Stefanyk Precarpathian National University's website, identifying accessibility issues and providing specific improvement recommendations.", "conclusion": "The proposed methodology offers a more detailed and practical approach to accessibility assessment compared to standardized methods, contributing to creating more inclusive digital environments."}}
{"id": "2509.23586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23586", "abs": "https://arxiv.org/abs/2509.23586", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "comment": "20 pages, 4 figures", "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems.", "AI": {"tldr": "AgentDiet is a trajectory reduction approach that reduces computational costs in LLM-based multi-turn agent systems by removing useless, redundant, and expired information from agent trajectories while maintaining performance.", "motivation": "Multi-turn LLM agent systems face high computational costs due to growing trajectories, but efficiency is largely neglected in existing studies and products.", "method": "Design AgentDiet - a simple yet effective trajectory reduction approach that automatically identifies and removes waste information (useless, redundant, expired) from agent trajectories.", "result": "AgentDiet reduces input tokens by 39.9%-59.7% and final computational cost by 21.1%-35.9% while maintaining the same agent performance on two LLMs and two benchmarks.", "conclusion": "Trajectory reduction is a promising direction for improving efficiency in agent systems without compromising performance."}}
{"id": "2509.23645", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23645", "abs": "https://arxiv.org/abs/2509.23645", "authors": ["A S M Shahadat Hossain", "Colin Brown", "David Koop", "Tanu Malik"], "title": "Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks", "comment": "10 pages", "summary": "Computational reproducibility refers to obtaining consistent results when\nrerunning an experiment. Jupyter Notebook, a web-based computational notebook\napplication, facilitates running, publishing, and sharing computational\nexperiments along with their results. However, rerunning a Jupyter Notebook may\nnot always generate identical results due to various factors, such as\nrandomness, changes in library versions, or variations in the computational\nenvironment. This paper introduces the Similarity-based Reproducibility Index\n(SRI) -- a metric for assessing the reproducibility of results in Jupyter\nNotebooks. SRI employs novel methods developed based on similarity metrics\nspecific to different types of Python objects to compare rerun outputs against\noriginal outputs. For every cell generating an output in a rerun notebook, SRI\nreports a quantitative score in the range [0, 1] as well as some qualitative\ninsights to assess reproducibility. The paper also includes a case study in\nwhich the proposed metric is applied to a set of Jupyter Notebooks,\ndemonstrating how various similarity metrics can be leveraged to quantify\ncomputational reproducibility.", "AI": {"tldr": "The paper introduces SRI, a metric for assessing reproducibility in Jupyter Notebooks using similarity metrics for different Python objects to compare rerun outputs against originals.", "motivation": "Jupyter Notebooks facilitate sharing computational experiments but rerunning them may not produce identical results due to randomness, library changes, or environmental variations.", "method": "Developed Similarity-based Reproducibility Index (SRI) using novel similarity metrics specific to different Python object types to compare rerun outputs with original outputs.", "result": "SRI provides quantitative scores (0-1) and qualitative insights for each output-generating cell in rerun notebooks, demonstrated through a case study on multiple Jupyter Notebooks.", "conclusion": "The proposed SRI metric effectively quantifies computational reproducibility in Jupyter Notebooks by leveraging various similarity metrics to compare outputs."}}
{"id": "2509.23675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23675", "abs": "https://arxiv.org/abs/2509.23675", "authors": ["Xinyue Zuo", "Yifan Zhang", "Hongshu Wang", "Yufan Cai", "Zhe Hou", "Jing Sun", "Jin Song Dong"], "title": "PAT-Agent: Autoformalization for Model Checking", "comment": "Accepted in ASE 2025 (International Conference on Automated Software\n  Engineering)", "summary": "Recent advances in large language models (LLMs) offer promising potential for\nautomating formal methods. However, applying them to formal verification\nremains challenging due to the complexity of specification languages, the risk\nof hallucinated output, and the semantic gap between natural language and\nformal logic. We introduce PAT-Agent, an end-to-end framework for natural\nlanguage autoformalization and formal model repair that combines the generative\ncapabilities of LLMs with the rigor of formal verification to automate the\nconstruction of verifiable formal models. In PAT-Agent, a Planning LLM first\nextracts key modeling elements and generates a detailed plan using semantic\nprompts, which then guides a Code Generation LLM to synthesize syntactically\ncorrect and semantically faithful formal models. The resulting code is verified\nusing the Process Analysis Toolkit (PAT) model checker against user-specified\nproperties, and when discrepancies occur, a Repair Loop is triggered to\niteratively correct the model using counterexamples. To improve flexibility, we\nbuilt a web-based interface that enables users, particularly non-FM-experts, to\ndescribe, customize, and verify system behaviors through user-LLM interactions.\nExperimental results on 40 systems show that PAT-Agent consistently outperforms\nbaselines, achieving high verification success with superior efficiency. The\nablation studies confirm the importance of both planning and repair components,\nand the user study demonstrates that our interface is accessible and supports\neffective formal modeling, even for users with limited formal methods\nexperience.", "AI": {"tldr": "PAT-Agent is an end-to-end framework that combines LLMs with formal verification to automate formal model construction and repair, achieving high verification success with superior efficiency.", "motivation": "Applying LLMs to formal verification is challenging due to specification language complexity, hallucinated output risks, and the semantic gap between natural language and formal logic.", "method": "Uses a Planning LLM to extract modeling elements and generate detailed plans, then a Code Generation LLM to synthesize formal models, verified by PAT model checker with iterative repair using counterexamples.", "result": "Outperforms baselines on 40 systems, achieves high verification success with superior efficiency. Ablation studies confirm importance of planning and repair components.", "conclusion": "The framework enables effective formal modeling for non-experts through user-friendly interface, demonstrating accessibility and practical utility."}}
{"id": "2509.23679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23679", "abs": "https://arxiv.org/abs/2509.23679", "authors": ["Zeqin Liao", "Yuhong Nan", "Zixu Gao", "Henglong Liang", "Sicheng Hao", "Jiajing Wu", "Zibin Zheng"], "title": "Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse", "comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering. The final version is available at\n  10.1109/TSE.2025.3613470", "summary": "Developers of smart contracts pervasively reuse subcontracts to improve\ndevelopment efficiency. Like any program language, such subcontract reuse may\nunexpectedly include, or introduce vulnerabilities to the end-point smart\ncontract. Unfortunately, automatically detecting such issues poses several\nunique challenges. Particularly, in most cases, smart contracts are compiled as\nbytecode, whose class-level information (e.g., inheritance, virtual function\ntable), and even semantics (e.g., control flow and data flow) are fully\nobscured as a single smart contract after compilation.\n  In this paper, we propose Satellite, a new bytecode-level static analysis\nframework for subcontract misuse vulnerability (SMV) detection in smart\ncontracts. Satellite incorporates a series of novel designs to enhance its\noverall effectiveness.. Particularly, Satellite utilizes a transfer learning\nmethod to recover the inherited methods, which are critical for identifying\nsubcontract reuse in smart contracts. Further, Satellite extracts a set of\nfine-grained method-level features and performs a method-level comparison, for\nidentifying the reuse part of subcontract in smart contracts. Finally,\nSatellite summarizes a set of SMV indicators according to their types, and\nhence effectively identifies SMVs. To evaluate Satellite, we construct a\ndataset consisting of 58 SMVs derived from real-world attacks and collect\nadditional 56 SMV patterns from SOTA studies. Experiment results indicate that\nSatellite exhibits good performance in identifying SMV, with a precision rate\nof 84.68% and a recall rate of 92.11%. In addition, Satellite successfully\nidentifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting\na total amount of digital assets worth 201,358 USD.", "AI": {"tldr": "Satellite is a bytecode-level static analysis framework that detects subcontract misuse vulnerabilities in smart contracts by recovering inherited methods through transfer learning, performing method-level comparison, and using SMV indicators.", "motivation": "Smart contract developers frequently reuse subcontracts for efficiency, but this can introduce vulnerabilities. Bytecode compilation obscures class-level information and semantics, making automatic detection challenging.", "method": "Uses transfer learning to recover inherited methods, extracts fine-grained method-level features, performs method-level comparison to identify subcontract reuse, and employs SMV indicators for vulnerability detection.", "result": "Achieved 84.68% precision and 92.11% recall on test dataset. Identified 14 new/unknown SMVs in 10,011 real-world contracts affecting $201,358 worth of digital assets.", "conclusion": "Satellite effectively detects subcontract misuse vulnerabilities in smart contracts with high accuracy and has practical value in identifying real-world security threats."}}
{"id": "2509.23806", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23806", "abs": "https://arxiv.org/abs/2509.23806", "authors": ["Chih-Duo Hong", "Yu Wang", "Yao-Chen Chang", "Fang Yu"], "title": "Influence-Guided Concolic Testing of Transformer Robustness", "comment": null, "summary": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing.", "AI": {"tldr": "Influence-guided concolic testing for Transformer classifiers uses SHAP-based influence ranking to efficiently find label-flip inputs, with solver-compatible attention semantics and scheduling heuristics enabling practical testing on modern architectures.", "motivation": "To make concolic testing feasible for contemporary Transformer models by addressing the challenges of constraint solving on modern architectures and improving search efficiency for finding adversarial inputs.", "method": "Developed an influence-guided concolic tester that ranks path predicates by SHAP-based influence estimates, created solver-compatible pure-Python semantics for multi-head self-attention, and introduced practical scheduling heuristics to manage constraint growth.", "result": "Influence guidance finds label-flip inputs more efficiently than FIFO baseline, maintains steady progress on deeper networks, and reveals recurring compact decision logic shared across attacks through SHAP-based critical decision path analysis.", "conclusion": "Influence signals provide useful search bias for symbolic exploration, and solver-friendly attention semantics with lightweight scheduling make concolic testing feasible for Transformer models, offering utility for debugging and model auditing."}}
{"id": "2509.23812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23812", "abs": "https://arxiv.org/abs/2509.23812", "authors": ["Dianshu Liao", "Xin Yin", "Shidong Pan", "Chao Ni", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "comment": null, "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers.", "AI": {"tldr": "JUnitGenie is a path-sensitive framework that combines code knowledge with LLMs to generate high-coverage unit tests, outperforming existing methods by 29-31% in coverage and finding real bugs.", "motivation": "Existing unit test generation approaches are path-insensitive and struggle with deep control-flow structures, leading to inadequate coverage for complex execution paths.", "method": "Extracts code knowledge from Java projects and distills it into structured prompts to guide LLMs in context-aware unit test generation.", "result": "Achieved 29.60% and 31.00% improvement in branch and line coverage respectively over baselines, and generated tests that uncovered real-world bugs confirmed by developers.", "conclusion": "The path-sensitive approach combining code knowledge with LLMs significantly improves test coverage and bug detection capabilities in unit test generation."}}
{"id": "2509.23824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23824", "abs": "https://arxiv.org/abs/2509.23824", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "comment": null, "summary": "The rise of blockchain has brought smart contracts into mainstream use,\ncreating a demand for smart contract generation tools. While large language\nmodels (LLMs) excel at generating code in general-purpose languages, their\neffectiveness on Solidity, the primary language for smart contracts, remains\nunderexplored. Solidity constitutes only a small portion of typical LLM\ntraining data and differs from general-purpose languages in its\nversion-sensitive syntax and limited flexibility. These factors raise concerns\nabout the reliability of existing LLMs for Solidity code generation.\nCritically, existing evaluations, focused on isolated functions and synthetic\ninputs, fall short of assessing models' capabilities in real-world contract\ndevelopment.\n  To bridge this gap, we introduce SolContractEval, the first contract-level\nbenchmark for Solidity code generation. It comprises 124 tasks drawn from real\non-chain contracts across nine major domains. Each task input, consisting of\ncomplete context dependencies, a structured contract framework, and a concise\ntask prompt, is independently annotated and cross-validated by experienced\ndevelopers. To enable precise and automated evaluation of functional\ncorrectness, we also develop a dynamic evaluation framework based on historical\ntransaction replay. Building on SolContractEval, we perform a systematic\nevaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the\nhighest overall performance, though evaluated models underperform relative to\ntheir capabilities on class-level generation tasks in general-purpose\nprogramming languages. Second, current models perform better on tasks that\nfollow standard patterns but struggle with complex logic and inter-contract\ndependencies. Finally, they exhibit limited understanding of Solidity-specific\nfeatures and contextual dependencies.", "AI": {"tldr": "SolContractEval is the first contract-level benchmark for Solidity code generation, evaluating LLMs on real-world smart contracts across 9 domains. Claude-3.7-Sonnet performs best but models struggle with complex logic, inter-contract dependencies, and Solidity-specific features.", "motivation": "Existing LLM evaluations for Solidity code generation are inadequate as they focus on isolated functions and synthetic inputs, failing to assess real-world contract development capabilities. Solidity constitutes only a small portion of LLM training data and has version-sensitive syntax with limited flexibility.", "method": "Created SolContractEval benchmark with 124 tasks from real on-chain contracts across 9 domains, with complete context dependencies and structured contract frameworks. Developed dynamic evaluation framework using historical transaction replay for automated functional correctness assessment. Evaluated 6 mainstream LLMs systematically.", "result": "Claude-3.7-Sonnet achieved the highest overall performance, but all models underperformed compared to their capabilities on general-purpose programming languages. Models performed better on standard patterns but struggled with complex logic and inter-contract dependencies. Limited understanding of Solidity-specific features and contextual dependencies was observed.", "conclusion": "Current LLMs have significant limitations in Solidity code generation for real-world smart contract development, particularly with complex logic, dependencies, and domain-specific features. The SolContractEval benchmark provides a more realistic evaluation framework for future model improvements."}}
{"id": "2509.23835", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23835", "abs": "https://arxiv.org/abs/2509.23835", "authors": ["Yukai Zhao", "Menghan Wu", "Xing Hu", "Xin Xia"], "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration.", "AI": {"tldr": "HFUZZER is a phrase-based fuzzing framework that tests LLMs for package hallucinations in code generation, identifying non-existent packages that could be exploited in supply chain attacks.", "motivation": "LLMs face security risks from package hallucinations where they recommend non-existent packages, creating vulnerabilities for software supply chain attacks. Current research lacks testing frameworks specifically for package hallucinations.", "method": "HFUZZER uses phrase-based fuzzing technology to guide LLMs in generating diverse coding tasks. It extracts phrases from package information or coding tasks to ensure relevance between phrases and generated code.", "result": "HFUZZER triggers package hallucinations across all tested LLMs, identifying 2.60x more unique hallucinated packages than mutational fuzzing. For GPT-4o alone, it found 46 unique hallucinated packages, revealing hallucinations occur in both code generation and environment configuration.", "conclusion": "Package hallucinations are a widespread security issue in LLMs for code generation, and HFUZZER effectively identifies these vulnerabilities, demonstrating the need for robust testing frameworks to defend against potential supply chain attacks."}}
{"id": "2509.23961", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23961", "abs": "https://arxiv.org/abs/2509.23961", "authors": ["Sheikh Md Mushfiqur Rahman", "Nasir Eisty"], "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization", "comment": null, "summary": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications.", "AI": {"tldr": "Proposes a Learning-Based Testing method that integrates hypothesis and mutation testing to efficiently prioritize adversarial test cases for DNNs, outperforming existing approaches in fault detection speed and effectiveness.", "motivation": "Existing test prioritization methods (coverage-based or confidence-based) often fail to efficiently identify the most fault-revealing inputs in DNNs, limiting their practical effectiveness for critical applications requiring resilience against adversarial inputs.", "method": "Integrates Learning-Based Testing with hypothesis and mutation testing to select a subset of adversarial inputs with high likelihood of exposing model faults, without relying on architecture-specific characteristics or formal verification.", "result": "The LBT method consistently surpasses baseline approaches in prioritizing fault-revealing inputs and accelerating fault detection, uncovering all potential faults significantly faster across various datasets, model architectures, and adversarial attack techniques.", "conclusion": "Beyond improving fault detection, the method preserves input diversity and provides effective guidance for model retraining, establishing it as a powerful and practical solution for adversarial test prioritization in real-world DNN applications."}}
{"id": "2509.24032", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24032", "abs": "https://arxiv.org/abs/2509.24032", "authors": ["Jialun Zhang", "Merve G\u00fclmez", "Thomas Nyman", "Gang Tan"], "title": "SandCell: Sandboxing Rust Beyond Unsafe Code", "comment": null, "summary": "Rust is a modern systems programming language that ensures memory safety by\nenforcing ownership and borrowing rules at compile time. While the unsafe\nkeyword allows programmers to bypass these restrictions, it introduces\nsignificant risks. Various approaches for isolating unsafe code to protect safe\nRust from vulnerabilities have been proposed, yet these methods provide only\nfixed isolation boundaries and do not accommodate expressive policies that\nrequire sandboxing both safe and unsafe code. This paper presents SandCell for\nflexible and lightweight isolation in Rust by leveraging existing syntactic\nboundaries. SandCell allows programmers to specify which components to sandbox\nwith minimal annotation effort, enabling fine-grained control over isolation.\nThe system also introduces novel techniques to minimize overhead when\ntransferring data between sandboxes. Our evaluation demonstrates SandCell's\neffectiveness in preventing vulnerabilities across various Rust applications\nwhile maintaining reasonable performance overheads.", "AI": {"tldr": "SandCell is a flexible isolation system for Rust that allows fine-grained sandboxing of both safe and unsafe code with minimal annotations, preventing vulnerabilities while maintaining reasonable performance.", "motivation": "Rust's unsafe keyword bypasses memory safety guarantees, creating risks. Existing isolation approaches provide only fixed boundaries and cannot sandbox both safe and unsafe code with expressive policies.", "method": "Leverages existing syntactic boundaries in Rust, allows programmers to specify sandboxed components with minimal annotations, and introduces techniques to minimize data transfer overhead between sandboxes.", "result": "Effectively prevents vulnerabilities across various Rust applications while maintaining reasonable performance overheads.", "conclusion": "SandCell provides flexible, lightweight isolation for Rust that accommodates expressive policies and fine-grained control over sandboxing both safe and unsafe code."}}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues.", "AI": {"tldr": "PerfBench is a new benchmark with 81 real-world performance bug-fixing tasks from .NET repositories, featuring a novel evaluation harness that allows agents to generate their own performance benchmarks and validate fixes by comparing execution metrics.", "motivation": "Existing benchmarks focus on functional correctness but fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs, which waste computational resources without causing functional failures.", "method": "Developed PerfBench with tasks derived from actual developer fixes linked to performance-related issues, verified by human experts. Created a novel evaluation harness that allows agents to generate performance benchmarks and validates fixes by comparing execution metrics between developer fix and agent fix.", "result": "Current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only ~3% success rate. OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions, achieved ~20% success rate.", "conclusion": "Proper instructions for benchmarking changes and tooling for benchmark output processing significantly improve agent performance, but room for improvement remains. PerfBench provides a challenging test set for advancing agent capabilities in fixing performance issues."}}
{"id": "2509.24148", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24148", "abs": "https://arxiv.org/abs/2509.24148", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "comment": null, "summary": "Test-Driven Development (TDD) is a widely adopted software engineering\npractice that requires developers to create and execute tests alongside code\nimplementation, ensuring that software behavior is continuously validated and\nrefined. In the era of vibe coding, where developers increasingly delegate code\nwriting to large language models (LLMs) by specifying high-level intentions,\nTDD becomes even more crucial, as test cases serve as executable specifications\nthat explicitly define and verify intended functionality beyond what\nnatural-language descriptions and code context can convey. While vibe coding\nunder TDD is promising, there are three main challenges: (1) selecting a small\nyet effective test suite to improve the generation accuracy and control the\nexecution workload, (2) retrieving context such as relevant code effectively,\nand (3) systematically using test feedback for effective code refinement. To\naddress these challenges, we introduce TENET, an LLM agent for generating\nfunctions in complex real-world repositories under the TDD setting. TENET\nfeatures three components: (1) a novel test harness mechanism that selects a\nconcise test suite to maximize diversity of target usage scenarios; (2) a\ntailored agent toolset that performs efficient retrieval of relevant code with\ninteractive debugging; and (3) a reflection-based refinement workflow that\niteratively analyzes failures, replenishes context, and applies code\nrefinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval\nbenchmarks, outperforming the best agentic baselines by 9.49 and 2.17\npercentage points, respectively. In addition, this is the first study of\ntest-driven code generation with repository-level context, examining how\ndifferent aspects of test suites affect the performance of LLM agents under the\nTDD setting.", "AI": {"tldr": "TENET is an LLM agent that addresses challenges in test-driven development for vibe coding by selecting diverse test suites, efficiently retrieving code context, and using reflection-based refinement, achieving state-of-the-art performance on code generation benchmarks.", "motivation": "In the era of vibe coding where developers use LLMs for code generation, TDD becomes crucial as tests serve as executable specifications. However, challenges exist in test suite selection, context retrieval, and systematic test feedback usage.", "method": "TENET features three components: (1) test harness mechanism for concise diverse test suite selection, (2) tailored agent toolset for efficient code retrieval with interactive debugging, and (3) reflection-based refinement workflow that analyzes failures and applies iterative code refinement.", "result": "TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points respectively.", "conclusion": "This is the first study of test-driven code generation with repository-level context, demonstrating how test suite aspects affect LLM agent performance in TDD settings, with TENET providing an effective solution for test-driven vibe coding."}}
{"id": "2509.24215", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.24215", "abs": "https://arxiv.org/abs/2509.24215", "authors": ["Wenxuan Wang", "Yongjiang Wu", "Junyuan Zhang", "Shuqing Li", "Yun Peng", "Wenting Chen", "Shuai Wang", "Michael R. Lyu"], "title": "Metamorphic Testing for Audio Content Moderation Software", "comment": "Accepted by ASE 2025", "summary": "The rapid growth of audio-centric platforms and applications such as WhatsApp\nand Twitter has transformed the way people communicate and share audio content\nin modern society. However, these platforms are increasingly misused to\ndisseminate harmful audio content, such as hate speech, deceptive\nadvertisements, and explicit material, which can have significant negative\nconsequences (e.g., detrimental effects on mental health). In response,\nresearchers and practitioners have been actively developing and deploying audio\ncontent moderation tools to tackle this issue. Despite these efforts, malicious\nactors can bypass moderation systems by making subtle alterations to audio\ncontent, such as modifying pitch or inserting noise. Moreover, the\neffectiveness of modern audio moderation tools against such adversarial inputs\nremains insufficiently studied. To address these challenges, we propose MTAM, a\nMetamorphic Testing framework for Audio content Moderation software.\nSpecifically, we conduct a pilot study on 2000 audio clips and define 14\nmetamorphic relations across two perturbation categories: Audio Features-Based\nand Heuristic perturbations. MTAM applies these metamorphic relations to toxic\naudio content to generate test cases that remain harmful while being more\nlikely to evade detection. In our evaluation, we employ MTAM to test five\ncommercial textual content moderation software and an academic model against\nthree kinds of toxic content. The results show that MTAM achieves up to 38.6%,\n18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing\ncommercial moderation software provided by Gladia, Assembly AI, Baidu,\nNextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when\ntesting the state-of-the-art algorithms from the academy.", "AI": {"tldr": "MTAM is a metamorphic testing framework for audio content moderation software that generates adversarial audio test cases to evaluate the robustness of moderation systems against evasion attacks.", "motivation": "Audio platforms are increasingly misused to spread harmful content, and existing moderation tools can be bypassed through subtle audio modifications, but their effectiveness against such adversarial inputs is not well studied.", "method": "Proposed MTAM framework with 14 metamorphic relations across Audio Features-Based and Heuristic perturbation categories, applied to toxic audio content to generate test cases that remain harmful but evade detection.", "result": "MTAM achieved error finding rates up to 38.6%, 18.3%, 35.1%, 16.7%, and 51.1% for commercial software from Gladia, Assembly AI, Baidu, Nextdata, and Tencent respectively, and up to 45.7% for academic models.", "conclusion": "MTAM effectively identifies vulnerabilities in audio content moderation systems by generating adversarial test cases that bypass detection while maintaining harmful content, highlighting the need for more robust moderation tools."}}
{"id": "2509.24344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24344", "abs": "https://arxiv.org/abs/2509.24344", "authors": ["Theo Koraag", "Niklas Wagner", "Felix Dobslaw", "Lucas Gren"], "title": "Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs", "comment": null, "summary": "Context: Large Language Models (LLMs) enable automation of complex natural\nlanguage processing across domains, but research on domain-specific\napplications like Finance remains limited. Objectives: This study explored\nopen-source and commercial LLMs for financial report analysis and commentary\ngeneration, focusing on software engineering challenges in implementation.\nMethods: Using Design Science Research methodology, an exploratory case study\niteratively designed and evaluated two LLM-based systems: one with local\nopen-source models in a multi-agent workflow, another using commercial GPT-4o.\nBoth were assessed through expert evaluation of real-world financial reporting\nuse cases. Results: LLMs demonstrated strong potential for automating financial\nreporting tasks, but integration presented significant challenges. Iterative\ndevelopment revealed issues including prompt design, contextual dependency, and\nimplementation trade-offs. Cloud-based models offered superior fluency and\nusability but raised data privacy and external dependency concerns. Local\nopen-source models provided better data control and compliance but required\nsubstantially more engineering effort for reliability and usability.\nConclusion: LLMs show strong potential for financial reporting automation, but\nsuccessful integration requires careful attention to architecture, prompt\ndesign, and system reliability. Implementation success depends on addressing\ndomain-specific challenges through tailored validation mechanisms and\nengineering strategies that balance accuracy, control, and compliance.", "AI": {"tldr": "LLMs show strong potential for automating financial reporting tasks but face significant implementation challenges including prompt design, contextual dependency, and trade-offs between cloud-based models (better fluency) vs local models (better data control).", "motivation": "Limited research exists on domain-specific LLM applications in Finance, despite LLMs' ability to automate complex natural language processing tasks across domains.", "method": "Design Science Research methodology with exploratory case study, iteratively designing and evaluating two LLM-based systems: one with local open-source models in multi-agent workflow, another using commercial GPT-4o, assessed through expert evaluation of real-world financial reporting use cases.", "result": "LLMs demonstrated strong potential for financial reporting automation but integration presented significant challenges. Cloud-based models offered superior fluency and usability but raised data privacy concerns, while local models provided better data control but required more engineering effort.", "conclusion": "LLMs show strong potential for financial reporting automation, but successful integration requires careful attention to architecture, prompt design, and system reliability, with tailored validation mechanisms and engineering strategies balancing accuracy, control, and compliance."}}
{"id": "2509.24347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24347", "abs": "https://arxiv.org/abs/2509.24347", "authors": ["Junjie Meng", "Jie An", "Yong Li", "Andrea Turrini", "Fanjiang Xu", "Naijun Zhan", "Miaomiao Zhang"], "title": "Efficient Decomposition Identification of Deterministic Finite Automata from Examples", "comment": null, "summary": "The identification of deterministic finite automata (DFAs) from labeled\nexamples is a cornerstone of automata learning, yet traditional methods focus\non learning monolithic DFAs, which often yield a large DFA lacking simplicity\nand interoperability. Recent work addresses these limitations by exploring DFA\ndecomposition identification problems (DFA-DIPs), which model system behavior\nas intersections of multiple DFAs, offering modularity for complex tasks.\nHowever, existing DFA-DIP approaches depend on SAT encodings derived from\nAugmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due\nto their inherent redundancy.\n  In this work, we advance DFA-DIP research through studying two variants: the\ntraditional Pareto-optimal DIP and the novel states-optimal DIP, which\nprioritizes a minimal number of states. We propose a novel framework that\nbridges DFA decomposition with recent advancements in automata representation.\nOne of our key innovations replaces APTA with 3-valued DFA (3DFA) derived\ndirectly from labeled examples. This compact representation eliminates\nredundancies of APTA, thus drastically reducing variables in the improved SAT\nencoding. Experimental results demonstrate that our 3DFA-based approach\nachieves significant efficiency gains for the Pareto-optimal DIP while enabling\na scalable solution for the states-optimal DIP.", "AI": {"tldr": "This paper proposes a novel framework for DFA decomposition identification problems (DFA-DIPs) that replaces Augmented Prefix Tree Acceptors (APTAs) with 3-valued DFAs (3DFAs) to eliminate redundancies and improve scalability.", "motivation": "Traditional DFA learning methods produce monolithic DFAs that lack simplicity and interoperability. While recent DFA-DIP approaches offer modularity through decomposition, they suffer from scalability limitations due to redundant SAT encodings derived from APTAs.", "method": "The authors introduce a framework that bridges DFA decomposition with modern automata representation by replacing APTA with 3DFA derived directly from labeled examples. This compact representation reduces variables in SAT encoding and enables efficient solutions for both Pareto-optimal and states-optimal DIP variants.", "result": "Experimental results show that the 3DFA-based approach achieves significant efficiency gains for Pareto-optimal DIP and enables scalable solutions for the states-optimal DIP variant.", "conclusion": "The proposed 3DFA-based framework overcomes scalability limitations of existing DFA-DIP approaches by eliminating redundancies in automata representation, making DFA decomposition more practical for complex system modeling."}}
{"id": "2509.24352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24352", "abs": "https://arxiv.org/abs/2509.24352", "authors": ["Minghua He", "Tong Jia", "Chiming Duan", "Pei Xiao", "Lingzhe Zhang", "Kangjin Wang", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?", "comment": "Accepted by ASE 2025 (NIER Track)", "summary": "Log-based software reliability maintenance systems are crucial for sustaining\nstable customer experience. However, existing deep learning-based methods\nrepresent a black box for service providers, making it impossible for providers\nto understand how these methods detect anomalies, thereby hindering trust and\ndeployment in real production environments. To address this issue, this paper\ndefines a trustworthiness metric, diagnostic faithfulness, for models to gain\nservice providers' trust, based on surveys of SREs at a major cloud provider.\nWe design two evaluation tasks: attention-based root cause localization and\nevent perturbation. Empirical studies demonstrate that existing methods perform\npoorly in diagnostic faithfulness. Consequently, we propose FaithLog, a\nfaithful log-based anomaly detection system, which achieves faithfulness\nthrough a carefully designed causality-guided attention mechanism and\nadversarial consistency learning. Evaluation results on two public datasets and\none industrial dataset demonstrate that the proposed method achieves\nstate-of-the-art performance in diagnostic faithfulness.", "AI": {"tldr": "This paper proposes FaithLog, a faithful log-based anomaly detection system that addresses the black-box nature of existing deep learning methods by introducing diagnostic faithfulness as a trustworthiness metric.", "motivation": "Existing deep learning-based log analysis methods are black boxes that prevent service providers from understanding how anomalies are detected, hindering trust and deployment in production environments.", "method": "FaithLog achieves faithfulness through a causality-guided attention mechanism and adversarial consistency learning, with evaluation tasks including attention-based root cause localization and event perturbation.", "result": "Empirical studies show existing methods perform poorly in diagnostic faithfulness, while FaithLog achieves state-of-the-art performance in diagnostic faithfulness on two public datasets and one industrial dataset.", "conclusion": "FaithLog successfully addresses the trustworthiness issue in log-based anomaly detection by providing diagnostic faithfulness, enabling better deployment in real production environments."}}
{"id": "2509.24364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24364", "abs": "https://arxiv.org/abs/2509.24364", "authors": ["Minghua He", "Chiming Duan", "Pei Xiao", "Tong Jia", "Siyu Yu", "Lingzhe Zhang", "Weijie Hong", "Jin Han", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning", "comment": "ASE 2025 (Research Track)", "summary": "Log-based fault diagnosis is essential for maintaining software system\navailability. However, existing fault diagnosis methods are built using a\ntask-independent manner, which fails to bridge the gap between anomaly\ndetection and root cause localization in terms of data form and diagnostic\nobjectives, resulting in three major issues: 1) Diagnostic bias accumulates in\nthe system; 2) System deployment relies on expensive monitoring data; 3) The\ncollaborative relationship between diagnostic tasks is overlooked. Facing this\nproblems, we propose a novel end-to-end log-based fault diagnosis method,\nChimera, whose key idea is to achieve end-to-end fault diagnosis through\nbidirectional interaction and knowledge transfer between anomaly detection and\nroot cause localization. Chimera is based on interactive multi-task learning,\ncarefully designing interaction strategies between anomaly detection and root\ncause localization at the data, feature, and diagnostic result levels, thereby\nachieving both sub-tasks interactively within a unified end-to-end framework.\nEvaluation on two public datasets and one industrial dataset shows that Chimera\noutperforms existing methods in both anomaly detection and root cause\nlocalization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,\nrespectively. It has been successfully deployed in production, serving an\nindustrial cloud platform.", "AI": {"tldr": "Chimera is an end-to-end log-based fault diagnosis method that bridges anomaly detection and root cause localization through bidirectional interaction and knowledge transfer, outperforming existing methods.", "motivation": "Existing fault diagnosis methods are task-independent, creating gaps between anomaly detection and root cause localization that lead to diagnostic bias, expensive monitoring requirements, and overlooked task collaboration.", "method": "Based on interactive multi-task learning with carefully designed interaction strategies at data, feature, and diagnostic result levels between anomaly detection and root cause localization within a unified end-to-end framework.", "result": "Evaluation on two public and one industrial dataset shows Chimera outperforms existing methods with improvements of 2.92%-5.00% in anomaly detection and 19.01%-37.09% in root cause localization.", "conclusion": "Chimera successfully achieves end-to-end fault diagnosis through bidirectional interaction between diagnostic tasks and has been deployed in production for an industrial cloud platform."}}
{"id": "2509.24380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24380", "abs": "https://arxiv.org/abs/2509.24380", "authors": ["Shuiguang Deng", "Hailiang Zhao", "Ziqi Wang", "Guanjie Cheng", "Peng Chen", "Wenzhuo Qian", "Zhiwei Ling", "Jianwei Yin", "Albert Y. Zomaya", "Schahram Dustdar"], "title": "Agentic Services Computing", "comment": null, "summary": "The rise of LLM-powered agents is driving a fundamental transformation in\nservices computing: from static, request-response functions to dynamic,\ngoal-oriented, and autonomous multi-agent ecosystems. In response to this\nshift, we introduce Agentic Service Computing (ASC), a new paradigm that\nreimagines services as intelligent, self-adaptive, and socially embedded\nentities. This comprehensive survey presents a lifecycle-driven framework for\nASC, structured around four core phases: Design, Deployment, Operation, and\nEvolution. We systematically analyze ASC through four foundational research\ndimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous\nDecision-Making and Task Execution, (3) Multi-Agent Collaboration and\nOrganization, and (4) Evaluation, Value Alignment, and Trustworthiness. We\nexamine how these dimensions are instantiated, integrated, and continuously\nadapted across the service lifecycle. Our synthesis reveals that agentic\nservices are not merely assembled but orchestrated: contextual awareness\nenables robust deployment; autonomous reasoning supports real-time operation;\ncollaborative structures emerge and evolve through interaction; and\ntrustworthiness must be upheld as a cross-cutting, lifelong imperative. We\nfurther identify and discuss emerging trends shaping the future of ASC. By\nintegrating classical principles of services computing with advances in\nLLM-based multi-agent systems, this work establishes a holistic and\nforward-looking foundation for ASC. It provides a unified reference for\nresearchers and practitioners aiming to develop adaptive, accountable, and\nhuman-centered intelligent services.", "AI": {"tldr": "Agentic Service Computing (ASC) is a new paradigm that transforms services into intelligent, autonomous entities through a lifecycle framework of Design, Deployment, Operation, and Evolution, analyzed across four research dimensions.", "motivation": "The rise of LLM-powered agents is transforming services computing from static request-response functions to dynamic, goal-oriented multi-agent ecosystems, requiring a new paradigm for intelligent services.", "method": "The paper presents a lifecycle-driven framework for ASC with four phases and systematically analyzes it through four research dimensions: Perception/Context/Environment Modeling, Autonomous Decision-Making, Multi-Agent Collaboration, and Evaluation/Trustworthiness.", "result": "The synthesis reveals that agentic services are orchestrated rather than assembled, with contextual awareness enabling robust deployment, autonomous reasoning supporting real-time operation, collaborative structures emerging through interaction, and trustworthiness as a lifelong imperative.", "conclusion": "ASC integrates classical services computing principles with LLM-based multi-agent systems to establish a holistic foundation for developing adaptive, accountable, and human-centered intelligent services, providing a unified reference for researchers and practitioners."}}
{"id": "2509.24419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24419", "abs": "https://arxiv.org/abs/2509.24419", "authors": ["Yuanhe Zhang", "Zhiquan Yang", "Shengyi Pan", "Zhongxin Liu"], "title": "Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement", "comment": null, "summary": "Unit testing is critical for ensuring software quality and software system\nstability. The current practice of manually maintaining unit tests suffers from\nlow efficiency and the risk of delayed or overlooked fixes. Therefore, an\nautomated approach is required to instantly update unit tests, with the\ncapability to both repair and enhance unit tests. However, existing automated\ntest maintenance methods primarily focus on repairing broken tests, neglecting\nthe scenario of enhancing existing tests to verify new functionality.\nMeanwhile, due to their reliance on rule-based context collection and the lack\nof verification mechanisms, existing approaches struggle to handle complex code\nchanges and often produce test cases with low correctness. To address these\nchallenges, we propose TESTUPDATER, a novel LLM based approach that enables\nautomated just-in-time test updates in response to production code changes.\nTESTUPDATER first leverages the LLM to analyze code changes and identify\nrelevant context, which it then extracts and filters. Then, through carefully\ndesigned prompts, TESTUPDATER guides the LLM step by step to handle various\ntypes of code changes and introduce new dependencies, enabling both test repair\nand enhancement. Finally, we introduce an error-type-aware iterative refinement\nmechanism that executes the LLM-updated tests and repairs failures, which\nsignificantly improves the overall correctness of test updates. Since existing\ntest repair datasets lack scenarios of test enhancement, we further construct a\nnew benchmark, UPDATES4J, with 195 real-world samples from 7 projects.\nExperimental results show that TESTUPDATER achieves a compilation pass rate of\n94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method\nSYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits\n12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.", "AI": {"tldr": "TESTUPDATER is an LLM-based approach for automated unit test maintenance that handles both test repair and enhancement in response to code changes, using context analysis, step-by-step prompting, and iterative refinement to achieve high correctness.", "motivation": "Manual unit test maintenance is inefficient and error-prone, while existing automated methods only focus on test repair and struggle with complex code changes due to rule-based context collection and lack of verification mechanisms.", "method": "Uses LLM to analyze code changes and extract relevant context, then guides the LLM through step-by-step prompts to handle various code changes and dependencies for both test repair and enhancement, with error-type-aware iterative refinement to fix failures.", "result": "Achieves 94.4% compilation pass rate and 86.7% test pass rate, outperforming SYNTER by 15.9% and 20.0% respectively, with 12.9% higher branch coverage and 15.2% greater line coverage.", "conclusion": "TESTUPDATER effectively addresses the limitations of existing test maintenance methods by enabling automated just-in-time test updates for both repair and enhancement scenarios, significantly improving correctness and coverage compared to state-of-the-art approaches."}}
{"id": "2509.24485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24485", "abs": "https://arxiv.org/abs/2509.24485", "authors": ["Vlad Stirbu", "Mateen Ahmed Abbasi", "Teerath Das", "Jesse Haimi", "Niko Iljin", "Pyry Kotilainen", "Petrus Lipsanen", "Niko M\u00e4kitalo", "Maiju Sipil\u00e4", "Venla Veijalainen", "Tommi Mikkonen"], "title": "Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development", "comment": null, "summary": "Generative AI (GenAI) has significantly influenced software engineering.\nAssociated tools have created a shift in software engineering, where\nspecialized agents, based on user-provided prompts, are replacing human\ndevelopers. In this paper, we propose a framework for GenAI native development\nthat we call \\textit{shift-up}, which helps software teams focus on high-value\nwork while being supported by GenAI. Furthermore, we also present a preliminary\nstudy testing these ideas with current GenAI tools. Towards the end of the\npaper, we propose future research goals to study shift-up in more detail.", "AI": {"tldr": "Proposes a \"shift-up\" framework for GenAI native development to help software teams focus on high-value work while being supported by GenAI tools.", "motivation": "Generative AI is transforming software engineering by replacing human developers with specialized AI agents, creating a need for frameworks that help teams adapt to this shift.", "method": "Proposes a shift-up framework for GenAI native development and conducts a preliminary study testing these ideas with current GenAI tools.", "result": "The paper presents initial findings from testing the shift-up framework with existing GenAI tools.", "conclusion": "Proposes future research goals to study the shift-up framework in more detail and further explore GenAI's role in software engineering."}}
{"id": "2509.24498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24498", "abs": "https://arxiv.org/abs/2509.24498", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Zelin Su", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Man Ho Lam", "Shuzheng Gao", "Yuchong Xie", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat", "comment": "10 pages", "summary": "The WeChat mini-game ecosystem faces rampant intellectual property theft to\nother platforms via secondary development, yet existing JavaScript obfuscation\ntools are ill-equipped for large-scale applications, suffering from prohibitive\nprocessing times, severe runtime performance degradation, and unsustainable\ncode size inflation. This paper introduces JSProtect, a high-throughput\nparallelized obfuscation framework designed to overcome these fundamental\nlimitations. At the core of our framework is the Parallel-Aware Scope Analysis\n(PASA) algorithm, which enables two key optimizations: independent code\npartitioning for multi-core processing and independent namespace management\nthat aggressively reuses short identifiers to combat code bloat. Our evaluation\ndemonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining\n100\\% semantic equivalence while controlling code size inflation to as low as\n20\\% compared to over 1,000\\% with baseline tools. Furthermore, it preserves\nnear-native runtime performance and provides superior security effectiveness\nagainst both static analysis tools and large language models. This work\npresents a new paradigm for industrial-scale JavaScript protection that\neffectively balances robust security with high performance and scalability.", "AI": {"tldr": "JSProtect is a parallelized JavaScript obfuscation framework that addresses scalability issues in existing tools, processing 20MB codebases in minutes with minimal performance impact and only 20% code size increase.", "motivation": "Current JavaScript obfuscation tools are inadequate for large-scale applications like WeChat mini-games, suffering from slow processing, severe performance degradation, and unsustainable code bloat when protecting against intellectual property theft.", "method": "Uses Parallel-Aware Scope Analysis (PASA) algorithm for independent code partitioning across multiple cores and aggressive identifier reuse to minimize code size inflation.", "result": "Processes 20MB codebases in minutes with 100% semantic equivalence, controls code size inflation to 20% (vs 1000%+ in baselines), preserves near-native performance, and provides superior security against static analysis and LLMs.", "conclusion": "JSProtect establishes a new paradigm for industrial-scale JavaScript protection that effectively balances robust security with high performance and scalability."}}
{"id": "2509.24507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24507", "abs": "https://arxiv.org/abs/2509.24507", "authors": ["Qinglin Wang", "Zhihong Sun", "Ruyun Wang", "Tao Huang", "Zhi Jin", "Ge Li", "Chen Lyu"], "title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "comment": "Accepted by the 40th IEEE/ACM Automated Software Engineering\n  Conference (ASE 2025)", "summary": "Large Language Models (LLMs) can translate natural language requirements into\ncode, yet empirical analyses of representative models reveal that semantic\nerrors-programs that compile but behave incorrectly-constitute the majority of\nobserved faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc\nrepair pipelines detect such faults only after execution, incurring latency,\nrelying on incomplete test suites, and often mis-localizing the defect. Since\nsemantic drift originates in the autoregressive decoding process, intervening\nwhile the code is being generated is a direct way to stop error propagation.\nConstrained-decoding approaches such as ROCODE attempt this, but still wait\nuntil the entire program runs to obtain feedback and use entropy heuristics\nthat do not truly capture semantics. A more effective solution must inject\nsemantic signals-early and precisely-into the decoding process.We present\nSemGuard, a semantic-evaluator-driven framework that performs real-time,\nline-level semantic supervision. To train the evaluator, we build SemDiff, the\nfirst dataset with fine-grained annotations that mark the exact line where a\ncorrect and an incorrect implementation diverge. The evaluator, once embedded\nin the LLM's decoder, flags deviations on partial code, rolls back to the\nfaulty line, and guides regeneration-without executing the program or requiring\ntest cases. Across four benchmarks, SemGuard consistently outperforms\nstate-of-the-art baselines. It lowers the semantic error rate by 19.86% on\nSemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world\nLiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP\nand for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating\nmodel- and language-agnostic effectiveness.", "AI": {"tldr": "SemGuard is a framework that performs real-time semantic supervision during code generation to prevent semantic errors in LLM-generated code, outperforming existing methods across multiple benchmarks.", "motivation": "Current LLMs generate code with high rates of semantic errors (programs that compile but behave incorrectly), and existing repair approaches detect faults only after execution, causing latency and relying on incomplete test suites.", "method": "SemGuard embeds a semantic evaluator trained on SemDiff dataset into the LLM's decoder to flag deviations in partial code, roll back to faulty lines, and guide regeneration without program execution or test cases.", "result": "Across four benchmarks, SemGuard lowers semantic error rate by 19.86% on SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on LiveCodeBench with CodeLlama-7B, demonstrating model- and language-agnostic effectiveness.", "conclusion": "SemGuard provides an effective solution for real-time semantic supervision during code generation, significantly reducing semantic errors without requiring program execution or test cases."}}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications.", "AI": {"tldr": "MSG is an automated specification generation tool for Move smart contracts that demonstrates LLMs can effectively handle non-mainstream languages, achieving 84% success rate and identifying overlooked clauses.", "motivation": "Existing LLM-based specification tools focus on mainstream languages like C and Java, leaving verification-oriented languages like Move underexplored.", "method": "MSG uses an agentic, modular design that explicitly leverages specification language features and incorporates feedback from the verification toolchain.", "result": "MSG generates verifiable specifications for 84% of tested Move functions, identifies clauses overlooked by experts, and produces 57% more verifiable clauses than conventional designs. Verification toolchain feedback increases verifiable specifications by 30%.", "conclusion": "LLMs exhibit robust capabilities for non-mainstream languages, and agentic modular designs with verification feedback significantly improve specification generation quality."}}
{"id": "2509.24637", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24637", "abs": "https://arxiv.org/abs/2509.24637", "authors": ["Zhensu Sun", "Chengran Yang", "Chao Peng", "Pengfei Gao", "Xiaoning Du", "Li Li", "David Lo"], "title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "comment": "10 pages", "summary": "Large Language Models (LLMs) have significantly advanced code completion, yet\nthey often fail when the developer's intent is underspecified in the code\ncontext. To address this, developers usually add natural language instructions\n(e.g., comments) into the code context to clarify their intent. However,\nexisting code LLMs applied for code completion systems merely undergo a\nfill-in-the-middle (FIM) pre-training, which struggles to leverage this\ninformation effectively due to the lack of instruction-like training data.\nExisting instruction-tuning techniques, which improve instruction-following in\ngeneral code generation, paradoxically degrade FIM performance, forcing a\ntrade-off between instruction-following and infilling capabilities. To address\nthis gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an\ninstruction-tuning method specifically designed to enhance FIM code completion\nmodels. IFIM extends the conventional FIM training objective by incorporating\nan explicit instruction section into the input, enabling the model to learn\nfrom (prefix, instruction, suffix) triplets. This approach allows the model to\neffectively leverage developer-provided directives while preserving its core\ncompletion abilities when no instructions are present. To facilitate this, we\nconstructed a large-scale dataset by using GPT-4o to generate concise,\nintent-focused instructions for code infilling examples. We evaluated IFIM by\napplying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on\nthe benchmarks derived from HumanEval-infilling and RepoMasterEval. The results\ndemonstrate that IFIM significantly improves instruction-following\ncapabilities, boosting the Pass@1 score from 84.6% to 93.6% on\nHumanEval-infilling. Moreover, this enhancement does not compromise the models'\noriginal performance on FIM code completion tasks with no instructions\nprovided.", "AI": {"tldr": "IFIM is an instruction-tuning method that enhances fill-in-the-middle code completion models by incorporating explicit instructions, improving instruction-following without compromising standard infilling performance.", "motivation": "LLMs struggle with code completion when developer intent is underspecified, and existing methods force a trade-off between instruction-following and infilling capabilities.", "method": "IFIM extends FIM training by adding an explicit instruction section, training on (prefix, instruction, suffix) triplets using a GPT-4o-generated dataset.", "result": "IFIM boosts Pass@1 score from 84.6% to 93.6% on HumanEval-infilling while maintaining original FIM performance without instructions.", "conclusion": "IFIM effectively enhances instruction-following in code completion models without sacrificing their core infilling capabilities."}}
{"id": "2509.24694", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24694", "abs": "https://arxiv.org/abs/2509.24694", "authors": ["Gangda Xiong", "Tao Chen"], "title": "CoTune: Co-evolutionary Configuration Tuning", "comment": "Accepted by ASE 2025", "summary": "To automatically tune configurations for the best possible system performance\n(e.g., runtime or throughput), much work has been focused on designing\nintelligent heuristics in a tuner. However, existing tuner designs have mostly\nignored the presence of complex performance requirements (e.g., the latency\nshall ideally be 2 seconds), but simply assume that better performance is\nalways more preferred. This would not only waste valuable information in a\nrequirement but might also consume extensive resources to tune for a goal with\nlittle gain. Yet, prior studies have shown that simply incorporating the\nrequirement as a tuning objective is problematic since the requirement might be\ntoo strict, harming convergence; or its highly diverse satisfactions might lead\nto premature convergence. In this paper, we propose CoTune, a tool that takes\nthe information of a given target performance requirement into account through\nco-evolution. CoTune is unique in the sense that it creates an auxiliary\nperformance requirement to be co-evolved with the configurations, which assists\nthe target performance requirement when it becomes ineffective or even\nmisleading, hence allowing the tuning to be guided by the requirement while\nbeing robust to its harm. Experiment results on 162 cases (nine systems and 18\nrequirements) reveal that CoTune considerably outperforms existing tuners,\nranking as the best for 90% cases (against the 0%--35% for other tuners) with\nup to 2.9x overall improvements, while doing so under a much better efficiency.", "AI": {"tldr": "CoTune is a configuration tuning tool that incorporates performance requirements through co-evolution, creating an auxiliary requirement to guide tuning while being robust to strict or misleading requirements.", "motivation": "Existing tuners ignore complex performance requirements and assume better performance is always preferred, wasting valuable requirement information and consuming resources for minimal gains. Simply incorporating requirements as objectives can cause convergence issues.", "method": "CoTune uses co-evolution by creating an auxiliary performance requirement that evolves alongside configurations, assisting the target requirement when it becomes ineffective or misleading.", "result": "CoTune outperforms existing tuners in 90% of 162 test cases across nine systems and 18 requirements, achieving up to 2.9x overall improvements with better efficiency.", "conclusion": "CoTune successfully addresses the challenge of incorporating performance requirements in configuration tuning through co-evolution, providing robust guidance while avoiding the pitfalls of strict requirements."}}
{"id": "2509.24782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24782", "abs": "https://arxiv.org/abs/2509.24782", "authors": ["Muhammad Laiq"], "title": "Large language models for behavioral modeling: A literature survey", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs.", "AI": {"tldr": "This paper provides a systematic overview of using large language models (LLMs) for behavioral modeling, specifically for generating use case and sequence diagrams, identifying current research gaps and future directions.", "motivation": "No comprehensive overview exists on LLM applications for behavioral modeling, making it difficult to identify research directions and assess LLM effectiveness for practitioners and educators in this domain.", "method": "Conducted a term-based search to identify relevant primary studies, filtering down to 14 studies for systematic analysis of LLM applications in behavioral modeling.", "result": "LLMs show promising results in automatically generating use case and sequence diagrams, but current literature lacks expert-based evaluations and primarily uses GPT-based models.", "conclusion": "Future research should evaluate a broader range of LLMs for behavioral modeling and involve domain experts to properly assess the quality of LLM-generated outputs."}}
{"id": "2509.24828", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24828", "abs": "https://arxiv.org/abs/2509.24828", "authors": ["Joshua Heisler", "Johannes Reisinger", "Andreas Fischer"], "title": "Evaluating SAP Joule for Code Generation", "comment": null, "summary": "SAP has released its own proprietary generative model SAP Joule, intended for\nvarious generative tasks, including serving as a code assistant for software\nengineers. While Joule is yet not focused on SAP-specific ABAP code generation,\nit can be used for other common languages, including Javascript. This paper\ncompares SAP Joules Javascript coding capabilities against a total of 29 other\nmodels using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict\naccuracy of 80.49% as the fifth best model in our evaluation. To the best of\nour knowledge, this is the first comparative evaluation of SAP Joule code\ngeneration capabilities.", "AI": {"tldr": "SAP Joule achieves 80.49% accuracy on HumanEval-X JavaScript benchmark, ranking 5th among 30 models evaluated.", "motivation": "To evaluate SAP Joule's JavaScript coding capabilities compared to other models, as this is the first comparative evaluation of SAP Joule's code generation capabilities.", "method": "Used HumanEval-X JavaScript benchmark to compare SAP Joule against 29 other models for code generation performance.", "result": "SAP Joule achieved 80.49% strict accuracy, ranking as the fifth best model in the evaluation.", "conclusion": "SAP Joule demonstrates strong JavaScript code generation capabilities, performing competitively among 30 evaluated models."}}
{"id": "2509.24975", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24975", "abs": "https://arxiv.org/abs/2509.24975", "authors": ["Lekang Yang", "Yuetong Liu", "Yitong Zhang", "Jia Li"], "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "comment": null, "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .", "AI": {"tldr": "DiffTester is an acceleration framework for diffusion LLMs in unit test generation that identifies repetitive structural patterns in test cases to enable parallel token generation without quality loss.", "motivation": "Existing LLMs generate unit tests inefficiently one token at a time, while diffusion LLMs offer parallel generation but suffer from quality degradation when generating multiple tokens per step.", "method": "Uses abstract syntax tree analysis to dynamically identify common structural patterns in unit tests targeting the same method, enabling adaptive parallel token generation without compromising quality.", "result": "Extensive experiments on extended TestEval benchmark (Python, Java, C++) show significant acceleration while preserving test coverage, with good generalization across different dLLMs and programming languages.", "conclusion": "DiffTester provides a practical and scalable solution for efficient unit test generation in software development by overcoming the efficiency-quality trade-off in diffusion LLMs."}}
{"id": "2509.25043", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25043", "abs": "https://arxiv.org/abs/2509.25043", "authors": ["Cristian Augusto", "Antonia Bertolino", "Guglielmo De Angelis", "Francesca Lonetti", "Jes\u00fas Mor\u00e1n"], "title": "Large Language Models for Software Testing: A Research Roadmap", "comment": "40 pages & 10 figures Submitted on 29th September 2025", "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.", "AI": {"tldr": "This paper provides a comprehensive roadmap and structured vision of LLM-based software testing research, identifying current progress, trends, and future directions through a semi-systematic literature review.", "motivation": "The rapid growth of LLM applications in software testing has created hundreds of new contributions monthly, making it difficult for researchers to stay current. No prior work has provided a structured overview of this emerging field.", "method": "Conducted a semi-systematic literature review to collect articles, map them into prominent categories, analyze current status and ongoing work, and identify open challenges in LLM-based software testing.", "result": "Created a roadmap illustrating the current state of LLM-based testing, grouped contributions into categories, identified most promising research directions, and outlined open challenges in the field.", "conclusion": "LLMs represent a significant disruption in software testing, and the paper provides a structured vision of progress while outlining expected long-term impacts of LLMs on the entire software testing field."}}
{"id": "2509.25117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25117", "abs": "https://arxiv.org/abs/2509.25117", "authors": ["Sogol Masoumzadeh", "Keheliya Gallaba", "Dayi Lin", "Ahmed E. Hassan"], "title": "Towards Reliable Generation of Executable Workflows by Foundation Models", "comment": null, "summary": "Recent advancements in Foundation Models (FMs) have demonstrated significant\nprogress in comprehending complex natural language to perform intricate tasks.\nSuccessfully executing these tasks often requires orchestrating calls to FMs\nalongside other software components. However, manually decomposing a task into\na coherent sequence of smaller, logically aggregated steps, commonly referred\nto as workflows, demands considerable effort and specialized domain knowledge.\nWhile FMs can assist in generating such workflows specified in domain-specific\nlanguages (DSLs), achieving accuracy and reliability in this process remains a\nchallenge.\n  This work introduces a framework that leverages static analysis feedback to\nenable FMs to detect and repair defects in the DSL-based workflows they\ngenerate. We begin by presenting the first-ever taxonomy of incidences of\ndefects in FM-generated DSL workflows, categorizing them into 18 distinct\ntypes. Furthermore, we observe a high prevalence of defects across FM-generated\nDSL workflows, with 87.27% of the studied instances containing at least one\ndefect. This, in turn, emphasizes the magnitude of the problem in practice and\nunderscores the necessity for implementing mitigation strategies. Following\nthis, we demonstrate that nine types of these defects can be effectively\nidentified through static analysis of the workflows. For this purpose, we\ndevelop Timon, the first-of-its-kind static analyzer specifically designed for\nFM-generated DSL workflows. Finally, we show that by incorporating feedback\nfrom Timon, we can guide Pumbaa, an FM-based tool, to repair the detected\ndefect incidences. By systematically detecting and repairing defects, our work\nprovides a crucial step towards the reliable and automated generation of\nexecutable workflows from natural language requirements.", "AI": {"tldr": "A framework using static analysis feedback to help Foundation Models detect and repair defects in DSL-based workflows they generate, improving reliability of automated workflow generation from natural language.", "motivation": "Manual decomposition of tasks into workflows requires significant effort and domain knowledge. While FMs can generate DSL workflows, achieving accuracy and reliability remains challenging due to high defect prevalence (87.27% of instances contain defects).", "method": "Developed Timon, a static analyzer for FM-generated DSL workflows, and Pumbaa, an FM-based repair tool. Uses static analysis feedback to guide FMs in detecting and repairing 9 types of workflow defects identified through a taxonomy of 18 defect types.", "result": "The framework successfully identifies and repairs workflow defects through static analysis feedback, addressing the high prevalence of defects in FM-generated DSL workflows.", "conclusion": "The work provides a crucial step towards reliable and automated generation of executable workflows from natural language requirements by systematically detecting and repairing defects in FM-generated DSL workflows."}}
