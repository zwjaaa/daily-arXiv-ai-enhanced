<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: Code World Model (CWM) is a 32B-parameter LLM trained on code execution trajectories and multi-task reasoning to improve code generation through world modeling capabilities.


<details>
  <summary>Details</summary>
Motivation: To advance code generation research by improving code understanding beyond static code training through world modeling with observation-action trajectories from computational environments.

Method: Mid-training on Python interpreter and Docker environment trajectories, followed by multi-task reasoning RL in verifiable coding, math, and software engineering environments. Uses dense decoder-only architecture with 131k token context.

Result: Achieves strong performance: 65.8% on SWE-bench Verified, 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. Enables Python code execution simulation and benefits agentic coding.

Conclusion: CWM provides a strong foundation for exploring world modeling in code generation, demonstrating benefits for reasoning and planning in computational environments, with released checkpoints for further research.

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [2] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: T2L-Agent is an end-to-end framework that enables precise line-level vulnerability localization in software by progressively narrowing scope from modules to exact vulnerable lines, using multi-round feedback and runtime evidence fusion.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based vulnerability detection methods inspect code in isolation, struggle with long contexts, and only provide coarse function- or file-level detections, offering limited actionable guidance for real-world software development.

Method: T2L-Agent uses an Agentic Trace Analyzer (ATA) that fuses runtime evidence (crash points, stack traces, coverage deltas) with AST-based code chunking, enabling iterative refinement through multi-round feedback to translate symptoms into line-level diagnoses.

Result: On the T2L-ARVO benchmark (50 expert-verified cases across 5 crash families), T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baseline methods.

Conclusion: The framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust precision diagnostics that reduce noise and accelerate patching in open-source software workflows.

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [3] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: AP2O-Coder is a preference optimization method that progressively guides LLMs to correct code errors by type, using an error notebook and adaptive replay of error types during training.


<details>
  <summary>Details</summary>
Motivation: Existing offline preference optimization methods for code generation focus only on pass/fail signals and overlook the specific error types in failed codes, limiting their effectiveness in reducing compilation and runtime errors.

Method: Construct an error notebook from failed codes and progressively optimize the LLM to correct errors type by type. Adaptively replay error types throughout training to address the LLM's changing weaknesses.

Result: AP2O-Coder improves code generation performance by up to 3% in pass@k while using less preference data, tested on LLMs from 0.5B to 34B parameters (Llama, Qwen, DeepSeek series).

Conclusion: The proposed AP2O-Coder method effectively enhances LLM code generation by systematically addressing specific error types through progressive and adaptive optimization.

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [4] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: This paper analyzes resource configuration challenges in serverless/FaaS platforms and proposes a taxonomy for function configuration factors, reviewing existing literature and identifying research gaps.


<details>
  <summary>Details</summary>
Motivation: Serverless platforms lack transparency in resource management, forcing developers to rely on ad-hoc decisions for function configuration, which affects both cost and performance. The complexity increases with open-source frameworks allowing independent resource configuration.

Method: The authors identify different aspects of resource configuration techniques in FaaS settings, propose a taxonomy of influencing factors, and conduct a comprehensive literature review of existing studies on function configuration.

Result: The paper presents a taxonomy of factors that influence function design, configuration, runtime cost, and performance guarantees in serverless environments.

Conclusion: The research identifies existing gaps in function configuration and suggests future directions to enhance serverless computing capabilities and drive broader adoption.

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [5] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: Study examines how Generative AI is transforming Product Managers' work at Microsoft, revealing adoption patterns, task delegation frameworks, and role evolution.


<details>
  <summary>Details</summary>
Motivation: While much research focuses on developers using GenAI, there's limited understanding of how Product Managers' work is evolving with GenAI adoption in software development.

Method: Mixed-methods study at Microsoft: surveyed 885 PMs, analyzed telemetry data for 731 PMs, and conducted 15 interviews.

Result: Identified PMs' GenAI adoption rates, use cases, benefits/barriers; developed framework for task delegation to GenAI; documented adaptation practices and role evolution perceptions.

Conclusion: GenAI is significantly transforming PM roles, with implications for broader GenAI workflow adoption and software development team structures.

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [6] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: ZeroFalse integrates static analysis with LLMs to reduce false positives in SAST tools while maintaining coverage, achieving F1-scores up to 0.955.


<details>
  <summary>Details</summary>
Motivation: SAST tools suffer from excessive false positives that undermine developer trust and require costly manual triage.

Method: Treats static analyzer outputs as structured contracts, enriches them with flow-sensitive traces, contextual evidence, and CWE-specific knowledge before LLM adjudication.

Result: Achieves F1-scores of 0.912 on OWASP Java Benchmark and 0.955 on OpenVuln dataset, with recall and precision above 90%. CWE-specialized prompting and reasoning-oriented LLMs perform best.

Conclusion: ZeroFalse provides a practical and scalable approach to enhance SAST reliability for real-world CI/CD integration.

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [7] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: The paper identifies practical auto-scaling considerations for microservices by applying state-of-the-art methods to benchmarks, classifying issues across Architecture, Implementation, and Deployment phases, and showing that addressing lifecycle concerns improves scaling performance.


<details>
  <summary>Details</summary>
Motivation: Achieving effective auto-scaling in microservices is challenging and depends on sound design, implementation, and deployment practices, which are often overlooked in existing benchmarks, making it difficult to evaluate autoscaling methods under realistic conditions.

Method: Applied several state-of-the-art autoscaling methods to widely used microservice benchmarks, classified issues based on software lifecycle phases (Architecture, Implementation, Deployment), and validated using the Sock-Shop benchmark with diverse scaling strategies.

Result: Findings show that overlooking key lifecycle concerns degrades autoscaler performance, while addressing them leads to more stable and efficient scaling.

Conclusion: Lifecycle-aware engineering is crucial for unlocking the full potential of auto-scaling in microservice-based systems.

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [8] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: RedCodeAgent is an automated red-teaming agent that systematically uncovers vulnerabilities in code agents by leveraging adaptive memory, dynamic tool selection, and simulated sandbox environments for reliable evaluation.


<details>
  <summary>Details</summary>
Motivation: Current static safety benchmarks and red-teaming tools are inadequate for identifying emerging real-world risky scenarios in code agents, particularly boundary conditions like combined effects of different jailbreak tools.

Method: Uses an adaptive memory module to leverage existing jailbreak knowledge, dynamically selects effective red-teaming tools and combinations from a tailored toolbox, and employs simulated sandbox environments to evaluate execution results.

Result: Outperforms existing red-teaming methods across multiple code agents, achieving higher attack success rates and lower rejection rates with high efficiency. Successfully exposed previously unidentified security risks in real-world code assistants like Cursor and Codeium.

Conclusion: RedCodeAgent enables scalable, adaptive, and effective safety assessments of code agents by automating and optimizing red-teaming processes.

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [9] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: A novel agent-driven framework that integrates BIM data extraction with automated building code verification using RAG and MCP agent pipelines, enabling scalable and reliable automated code review.


<details>
  <summary>Details</summary>
Motivation: Manual building code reviews are labor-intensive, error-prone, and costly, especially for resource-constrained jurisdictions, while BIM adoption and LLMs present opportunities for automated solutions.

Method: Uses LLM-enabled agents to extract geometry, schedules, and system attributes from BIM files, then processes them through two mechanisms: direct API calls to COMcheck engine for deterministic outputs, and RAG-based reasoning for flexible interpretation of ambiguous rules.

Result: GPT-4o achieved the best balance of efficiency and stability; MCP agent pipelines outperformed RAG reasoning pipelines in rigor and reliability; successfully demonstrated automated extraction of geometric attributes, operational schedules, and lighting allowances validation.

Conclusion: The framework advances automated code review research by providing a scalable, interoperable, and production-ready approach that bridges BIM with authoritative code review tools.

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [10] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: DM# accelerates DNN mutation testing using Fourier analysis to cluster mutants by behavior, enabling representative testing that reduces computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: DNN mutation analysis is computationally expensive due to the large number of mutants and datasets, requiring an efficient acceleration technique.

Method: Uses Fourier analysis to quantify mutant behavior with few data points, clusters mutants by similarity, and tests only representatives from each cluster.

Result: Accelerates mutation testing by 28.38% on average with only 0.72% error in mutation score, outperforming baseline techniques by significant margins.

Conclusion: DM# effectively reduces computational costs of DNN mutation testing while maintaining high accuracy through intelligent mutant clustering and representative testing.

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [11] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: AuthFix is an automated program repair system using LLMs and counterexample-guided synthesis to fix OpenID Connect security bugs, achieving 74% success rate on real-world vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: OpenID Connect has critical security bugs causing financial losses and breaches, but automated repair faces challenges with domain complexity and precise fault localization.

Method: AuthFix uses LLMs for counterexample-guided repair with three components: fault localization, patch synthesis, and patch verification using a Petri-net-based model checker.

Result: AuthFix successfully generated correct patches for 17 out of 23 OpenID bugs (74%), with many patches semantically equivalent to developer-written fixes.

Conclusion: AuthFix effectively automates OpenID bug fixing, demonstrating high success rates and semantic equivalence to manual fixes, addressing critical security vulnerabilities.

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [12] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: C2|Q>: is a quantum software development framework that translates classical code into quantum programs, reducing implementation effort by 40X compared to manual quantum SDKs.


<details>
  <summary>Details</summary>
Motivation: To make quantum computing accessible to classical software engineers by bridging the gap between classical specifications and quantum execution, eliminating the need for low-level quantum details.

Method: Uses a modular three-component workflow: encoder (problem classification, QCF generation, circuit construction), deployment module (circuit generation, hardware recommendation), and decoder (quantum output interpretation).

Result: Encoder achieved 93.8% completion rate, hardware recommendation worked for up to 56 qubits, full workflow processed 434 Python snippets (93.8% completion) and 100 JSON inputs (100% completion), with 40X reduction in implementation effort.

Conclusion: C2|Q>: successfully demonstrates that quantum software development can be made accessible to classical developers through automated translation frameworks, significantly reducing implementation complexity.

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [13] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: The paper shows that code representations that are easier to parse (using simpler grammar classes like LL(1)) lead to better model performance in code generation. They introduce GramTrans to automatically transform context-free languages into LL(1) representations.


<details>
  <summary>Details</summary>
Motivation: Existing studies use various code representations but lack understanding of how parsing difficulty affects model performance. The paper aims to establish that simpler parsing leads to better model results.

Method: Propose the conjecture that easier-to-parse representations improve model performance. Introduce GramTrans with hierarchical conflict elimination to transform context-free languages into LL(1) grammar class representations.

Result: Parsing difficulty strongly correlates with model performance. GramTrans consistently improves performance across Python and Java using three code generation models (StarCoder, DeepSeek-Coder, Qwen2.5) on multiple benchmarks.

Conclusion: The conjecture is supported - simpler parsing representations lead to better model performance. GramTrans provides an effective approach to achieve this transformation while balancing syntactic simplicity and token efficiency.

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [14] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: The paper analyzes LLM code correctness mechanisms using sparse autoencoders, identifying directions that predict incorrect code and enable selective steering interventions.


<details>
  <summary>Details</summary>
Motivation: Understanding LLM internal correctness mechanisms is critical for safe deployment as AI-suggested code enters production systems.

Method: Apply sparse autoencoders to decompose LLM representations, identify code correctness directions using t-statistics and separation scores, then analyze through steering, attention analysis, and weight orthogonalization.

Result: Code correctness directions reliably predict incorrect code, while correction involves tradeoffs between fixing errors and preserving correct code. Successful code generation depends on attending to test cases rather than problem descriptions.

Conclusion: Code correctness mechanisms learned during pre-training are repurposed during fine-tuning. Practical applications include prioritizing test examples in prompts, using predictor directions as error alarms, and selective steering to prevent code corruption.

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [15] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: AUTOPROBE is a model-agnostic approach that dynamically selects the most informative internal representations from LLMs for code correctness assessment, outperforming existing methods across compilability, functionality, and security dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing LLM-generated code correctness rely on fixed layer and token positions, limiting generalizability across different model architectures and tasks.

Method: AUTOPROBE uses an attention-based mechanism to learn importance scores for hidden states, aggregates weighted representations, and passes them to a probing classifier to predict code correctness across multiple dimensions.

Result: AUTOPROBE consistently outperforms baselines, achieving 18% improvement in security assessment and up to 19% and 111% improvements in compilability and functionality assessment respectively, showing highest robustness to code complexity.

Conclusion: Dynamically selecting important internal signals enables AUTOPROBE to serve as a robust and generalizable solution for assessing code correctness across various LLMs.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [16] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: This paper introduces three observability design patterns for cloud-native applications: Distributed Tracing, Application Metrics, and Infrastructure Metrics to improve system reliability and maintainability.


<details>
  <summary>Details</summary>
Motivation: As software architectures become increasingly distributed and subject to change, diagnosing system issues becomes more challenging due to fragmented observability and difficult root cause analysis.

Method: The paper builds on previous work and introduces three design patterns derived from industry practices and observability frameworks: Distributed Tracing for request flow visibility, Application Metrics for structured performance monitoring, and Infrastructure Metrics for environment monitoring.

Result: The patterns provide guidance for improving visibility into request flows across services, enabling real-time monitoring and anomaly detection, and helping teams assess resource utilization, scalability, and operational health.

Conclusion: These three observability design patterns aim to offer practical guidance for software practitioners to address key challenges in monitoring cloud-native applications and improve system reliability.

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [17] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: A pattern language for teaching agile software development practices to university students, focusing on team and project setup with five initial patterns.


<details>
  <summary>Details</summary>
Motivation: Existing literature on teaching ASD lacks actionable advice, often focusing on frameworks or moving beyond software development into agile teaching methods.

Method: Developed a pattern language based on educators' experiences in higher education, presenting five initial patterns for team and project setup phase.

Result: Created five patterns: Capping Team Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling Teams, and Team Chooses Topic as a starting point.

Conclusion: This early work provides a foundation for developing a comprehensive pattern language to effectively teach agile software development practices in university settings.

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [18] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: This study evaluates code quality (specifically code smells) in LLM-generated Java programs compared to professionally written reference solutions, finding LLM code has significantly more smells (63.34% average increase) with performance varying by task complexity and topic.


<details>
  <summary>Details</summary>
Motivation: While much research focuses on functional correctness of LLM-generated code, there is limited investigation into code quality aspects like code smells, which are important indicators of maintainability and software quality.

Method: Scenario-based evaluation measuring code smells in Java programs generated by four LLMs (Gemini Pro, ChatGPT, Codex, Falcon), comparing against baseline reference solutions, with analysis across different coding topics and task complexities.

Result: LLM-generated code has 63.34% more code smells on average than reference solutions, with Codex performing worst (84.97% increase) and Falcon best (42.28% increase). More complex tasks and object-oriented topics showed greater smell increases.

Conclusion: LLM performance on code quality correlates with human code quality patterns across scenarios, but LLM-generated code quality is noticeably poorer than human-written code, especially for complex tasks and advanced programming concepts.

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [19] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: A catalogue of seven refactorings for transitioning from monoliths to microservices, focusing on code-level dependencies and providing a systematic guide for developers.


<details>
  <summary>Details</summary>
Motivation: Current migration from monoliths to microservices is manual and labor-intensive, with existing approaches focusing on architecture-level guidance while overlooking code-level challenges and dependencies.

Method: Introduces a catalogue of seven refactorings specifically designed to handle dependencies during microservices migration, consolidating refactorings from literature and providing a structured, step-by-step approach.

Result: Provides developers with a systematic guide that addresses the critical gap in systematizing the migration process at the code level, simplifying the migration process.

Conclusion: The catalogue lays the groundwork for potential automation of microservices migration, empowering developers to implement changes efficiently and effectively.

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [20] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: The paper introduces state field coverage, a novel metric for assessing oracle quality by measuring the proportion of an object's state fields that an oracle accesses during test execution.


<details>
  <summary>Details</summary>
Motivation: Existing oracle quality metrics are either not comprehensive enough to guide oracle improvement or too specific to certain oracle types, limiting their general applicability.

Method: The authors implement a static analysis mechanism to compute state field coverage, which identifies which class fields an oracle may access during test execution without requiring dynamic execution.

Result: Evaluation with 273 representation invariants and 249,027 test assertions shows state field coverage strongly correlates with mutation score, indicating it effectively measures oracle fault-detection ability.

Conclusion: State field coverage is an effective metric for assessing oracle quality that provides direct guidance for improving test oracles by identifying unexamined state fields.

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [21] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: LLMs rely on naming patterns rather than structural semantics for code understanding, as shown through obfuscation experiments that reveal performance drops on both intent-level and execution tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs derive program meaning and determine whether they rely on structural semantics or human-interpretable naming patterns.

Method: Introduced semantics-preserving obfuscations to remove naming cues while preserving code behavior, and created ClassEval-Obf benchmark to systematically test LLMs.

Result: Removing naming channels degrades performance on intent-level tasks and surprisingly also reduces performance on execution tasks, revealing that current benchmarks reward memorization of naming patterns rather than genuine semantic reasoning.

Conclusion: ClassEval-Obf provides a more reliable basis for assessing LLMs' code understanding by reducing inflated performance gaps and weakening memorization shortcuts.

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [22] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: Introduces two LLM-based policies (bug abstention and patch validation) to reduce noise in agentic automated program repair by filtering out unlikely fixes before human review.


<details>
  <summary>Details</summary>
Motivation: Agentic APR systems generate patches for complex bugs but show many unlikely patches to developers, wasting time and eroding trust in automated code changes.

Method: Two complementary LLM-based policies: bug abstention (excludes bugs the system is unlikely to fix) and patch validation (rejects patches unlikely to be good fixes).

Result: On 174 human-reported bugs, policies raised success rates by up to 13pp (bug abstention), 15pp (patch validation), and 39pp combined. Also improved success rates for null pointer exceptions and sanitizer-reported bugs.

Conclusion: The two-policy approach provides a practical path for reliable, industrial-scale deployment of agentic APR systems by reducing noise and improving patch quality.

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>
