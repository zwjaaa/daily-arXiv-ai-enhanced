{"id": "2509.16268", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16268", "abs": "https://arxiv.org/abs/2509.16268", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Wenxuan Wang", "Pingchuan Ma", "Shuai Wang", "Lei Ma"], "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling", "comment": null, "summary": "Function calling (FC) has emerged as a powerful technique for facilitating\nlarge language models (LLMs) to interact with external systems and perform\nstructured tasks. However, the mechanisms through which it influences model\nbehavior remain largely under-explored. Besides, we discover that in addition\nto the regular usage of FC, this technique can substantially enhance the\ncompliance of LLMs with user instructions. These observations motivate us to\nleverage causality, a canonical analysis method, to investigate how FC works\nwithin LLMs. In particular, we conduct layer-level and token-level causal\ninterventions to dissect FC's impact on the model's internal computational\nlogic when responding to user queries. Our analysis confirms the substantial\ninfluence of FC and reveals several in-depth insights into its mechanisms. To\nfurther validate our findings, we conduct extensive experiments comparing the\neffectiveness of FC-based instructions against conventional prompting methods.\nWe focus on enhancing LLM safety robustness, a critical LLM application\nscenario, and evaluate four mainstream LLMs across two benchmark datasets. The\nresults are striking: FC shows an average performance improvement of around\n135% over conventional prompting methods in detecting malicious inputs,\ndemonstrating its promising potential to enhance LLM reliability and capability\nin practical applications.", "AI": {"tldr": "Function calling (FC) significantly enhances LLM compliance with user instructions and improves safety robustness by 135% over conventional prompting methods.", "motivation": "The mechanisms through which function calling influences LLM behavior remain largely unexplored, and FC shows potential to substantially enhance LLM compliance with user instructions.", "method": "Used layer-level and token-level causal interventions to dissect FC's impact on model's internal computational logic, and conducted extensive experiments comparing FC-based instructions against conventional prompting methods on LLM safety robustness.", "result": "FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs across four mainstream LLMs and two benchmark datasets.", "conclusion": "FC demonstrates promising potential to enhance LLM reliability and capability in practical applications, particularly for safety-critical scenarios."}}
{"id": "2509.16478", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16478", "abs": "https://arxiv.org/abs/2509.16478", "authors": ["Hossein Yousefizadeh", "Shenghui Gu", "Lionel C. Briand", "Ali Nasr"], "title": "Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach", "comment": null, "summary": "Autonomous systems, such as autonomous driving systems, evolve rapidly\nthrough frequent updates, risking unintended behavioral degradations. Effective\nsystem-level testing is challenging due to the vast scenario space, the absence\nof reliable test oracles, and the need for practically applicable and\ninterpretable test cases. We present CoCoMagic, a novel automated test case\ngeneration method that combines metamorphic testing, differential testing, and\nadvanced search-based techniques to identify behavioral divergences between\nversions of autonomous systems. CoCoMagic formulates test generation as a\nconstrained cooperative co-evolutionary search, evolving both source scenarios\nand metamorphic perturbations to maximize differences in violations of\npredefined metamorphic relations across versions. Constraints and population\ninitialization strategies guide the search toward realistic, relevant\nscenarios. An integrated interpretability approach aids in diagnosing the root\ncauses of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,\nwithin the Carla virtual simulator. Results show significant improvements over\nbaseline search methods, identifying up to 287\\% more distinct high-severity\nbehavioral differences while maintaining scenario realism. The interpretability\napproach provides actionable insights for developers, supporting targeted\ndebugging and safety assessment. CoCoMagic offers an efficient, effective, and\ninterpretable way for the differential testing of evolving autonomous systems\nacross versions.", "AI": {"tldr": "CoCoMagic is an automated test case generation method that combines metamorphic testing, differential testing, and search-based techniques to identify behavioral divergences between versions of autonomous systems through constrained cooperative co-evolutionary search.", "motivation": "Autonomous systems evolve rapidly through frequent updates, risking unintended behavioral degradations. System-level testing is challenging due to vast scenario space, absence of reliable test oracles, and need for practically applicable and interpretable test cases.", "method": "CoCoMagic formulates test generation as constrained cooperative co-evolutionary search, evolving both source scenarios and metamorphic perturbations to maximize differences in violations of predefined metamorphic relations across versions. It includes constraints and population initialization strategies for realistic scenarios, plus integrated interpretability for root cause diagnosis.", "result": "Evaluation on InterFuser ADS in Carla simulator shows CoCoMagic identifies up to 287% more distinct high-severity behavioral differences than baseline methods while maintaining scenario realism. The interpretability approach provides actionable insights for developers.", "conclusion": "CoCoMagic offers an efficient, effective, and interpretable way for differential testing of evolving autonomous systems across versions, supporting targeted debugging and safety assessment."}}
{"id": "2509.16525", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16525", "abs": "https://arxiv.org/abs/2509.16525", "authors": ["Anna Mazhar", "Sainyam Galhotra"], "title": "Causal Fuzzing for Verifying Machine Unlearning", "comment": null, "summary": "As machine learning models become increasingly embedded in decision-making\nsystems, the ability to \"unlearn\" targeted data or features is crucial for\nenhancing model adaptability, fairness, and privacy in models which involves\nexpensive training. To effectively guide machine unlearning, a thorough testing\nis essential. Existing methods for verification of machine unlearning provide\nlimited insights, often failing in scenarios where the influence is indirect.\nIn this work, we propose CAF\\'E, a new causality based framework that unifies\ndatapoint- and feature-level unlearning for verification of black-box ML\nmodels. CAF\\'E evaluates both direct and indirect effects of unlearning targets\nthrough causal dependencies, providing actionable insights with fine-grained\nanalysis. Our evaluation across five datasets and three model architectures\ndemonstrates that CAF\\'E successfully detects residual influence missed by\nbaselines while maintaining computational efficiency.", "AI": {"tldr": "CAF\u00c9 is a causality-based framework for verifying machine unlearning in black-box ML models, addressing limitations of existing methods by evaluating both direct and indirect effects through causal dependencies.", "motivation": "Machine learning models need \"unlearning\" capabilities for adaptability, fairness, and privacy, but existing verification methods provide limited insights and fail in scenarios with indirect influence.", "method": "CAF\u00c9 uses causal dependencies to evaluate both direct and indirect effects of unlearning targets (datapoints and features) in black-box ML models, providing fine-grained analysis.", "result": "Evaluation across five datasets and three model architectures shows CAF\u00c9 successfully detects residual influence missed by baselines while maintaining computational efficiency.", "conclusion": "CAF\u00c9 provides a unified causality-based framework for comprehensive verification of machine unlearning, offering actionable insights that overcome limitations of existing methods."}}
{"id": "2509.16595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16595", "abs": "https://arxiv.org/abs/2509.16595", "authors": ["Jiaming Ye", "Xiongfei Wu", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing", "comment": "This paper will be appeared in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025), NIER\n  track, Seoul, South Korea, November 16 -20, 2025", "summary": "As quantum computing continues to emerge, ensuring the quality of quantum\nprograms has become increasingly critical. Quantum program testing has emerged\nas a prominent research area within the scope of quantum software engineering.\nWhile numerous approaches have been proposed to address quantum program quality\nassurance, our analysis reveals that most existing methods rely on\nmeasurement-based validation in practice. However, due to the inherently\nprobabilistic nature of quantum programs, measurement-based validation methods\nface significant limitations.\n  To investigate these limitations, we conducted an empirical study of recent\nresearch on quantum program testing, analyzing measurement-based validation\nmethods in the literature. Our analysis categorizes existing measurement-based\nvalidation methods into two groups: distribution-level validation and\noutput-value-level validation. We then compare measurement-based validation\nwith statevector-based validation methods to evaluate their pros and cons. Our\nfindings demonstrate that measurement-based validation is suitable for\nstraightforward assessments, such as verifying the existence of specific output\nvalues, while statevector-based validation proves more effective for\ncomplicated tasks such as assessing the program behaviors.", "AI": {"tldr": "This paper analyzes limitations of measurement-based validation methods in quantum program testing and compares them with statevector-based validation approaches.", "motivation": "Quantum program quality assurance is critical as quantum computing emerges, but existing measurement-based validation methods face significant limitations due to the probabilistic nature of quantum programs.", "method": "Conducted an empirical study of recent quantum program testing research, categorizing measurement-based validation into distribution-level and output-value-level validation, and comparing them with statevector-based validation.", "result": "Measurement-based validation is suitable for straightforward assessments like verifying specific output values, while statevector-based validation is more effective for complex tasks like assessing program behaviors.", "conclusion": "The study reveals the limitations of measurement-based validation and demonstrates the advantages of statevector-based validation for comprehensive quantum program testing."}}
{"id": "2509.16655", "categories": ["cs.SE", "cs.CR", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16655", "abs": "https://arxiv.org/abs/2509.16655", "authors": ["Serena Wang", "Martino Banchio", "Krzysztof Kotowicz", "Katrina Ligett", "R. Preston McAfee", "Eduardo' Vela'' Nava"], "title": "Incentives and Outcomes in Bug Bounties", "comment": null, "summary": "Bug bounty programs have contributed significantly to security in technology\nfirms in the last decade, but little is known about the role of reward\nincentives in producing useful outcomes. We analyze incentives and outcomes in\nGoogle's Vulnerability Rewards Program (VRP), one of the world's largest bug\nbounty programs. We analyze the responsiveness of the quality and quantity of\nbugs received to changes in payments, focusing on a change in Google's reward\namounts posted in July, 2024, in which reward amounts increased by up to 200%\nfor the highest impact tier. Our empirical results show an increase in the\nvolume of high-value bugs received after the reward increase, for which we also\ncompute elasticities. We further break down the sources of this increase\nbetween veteran researchers and new researchers, showing that the reward\nincrease both redirected the attention of veteran researchers and attracted new\ntop security researchers into the program.", "AI": {"tldr": "Analysis of Google's bug bounty program shows that increasing rewards by up to 200% for high-impact bugs led to more high-value submissions, with elasticities measured, and attracted both veteran and new researchers.", "motivation": "To understand how reward incentives affect the quality and quantity of bug submissions in bug bounty programs, using Google's VRP as a case study.", "method": "Empirical analysis of Google's Vulnerability Rewards Program data, focusing on changes in reward amounts posted in July 2024, with increased rewards up to 200% for high-impact bugs.", "result": "Increased volume of high-value bugs received after reward increase; elasticities computed; both veteran researchers redirected attention and new top researchers attracted to the program.", "conclusion": "Higher financial incentives effectively boost both the quantity and quality of bug submissions in bug bounty programs, engaging both existing and new security researchers."}}
{"id": "2509.16681", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16681", "abs": "https://arxiv.org/abs/2509.16681", "authors": ["Peterson Jean"], "title": "Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver", "comment": "62 pages. Master's dissertation submitted to Swansea University,\n  Department of Computer Science, September 2022. Supervisor Dr Jens Blanck", "summary": "The increase in safety and critical systems improved Healthcare. Due to their\nrisk of harm, such systems are subject to stringent guidelines and compliances.\nThese safety measures ensure a seamless experience and mitigate the risk to\nend-users. Institutions like the Food and Drug Administration and the NHS,\nrespectively, established international standards and competency frameworks to\nensure industry compliance with these safety concerns. Medical device\nmanufacturing is mainly concerned with standards. Consequently, these standards\nnow advocate for better human factors considered in user interaction for\nmedical devices. This forces manufacturers to rely on heavy testing and review\nto cover many of these factors during development. Sadly, many human factor\nrisks will not be caught until proper testing in real life, which might be\ncatastrophic in the case of an ambulatory device like the T34 syringe pump.\nTherefore, effort in formal methods research may propose new solutions in\nanticipating these errors in the early stages of development or even reducing\ntheir occurrence based on the use of standard generic model. These generically\ndeveloped models will provide a common framework for safety integration in\nindustry and may potentially be proven using formal verification mathematical\nproofs. This research uses SPARK Ada's formal verification tool against a\nbehavioural model of the T34 syringe driver. A Generic Infusion Pump model\nrefinement is explored and implemented in SPARK Ada. As a subset of the Ada\nlanguage, the verification level of the end prototype is evaluated using SPARK.\nExploring potential limitations defines the proposed model's implementation\nliability when considering abstraction and components of User Interface design\nin SPARK Ada.", "AI": {"tldr": "This research explores using formal verification methods (specifically SPARK Ada) to anticipate human factor risks in medical devices like the T34 syringe pump during early development stages, rather than relying solely on real-life testing which can be catastrophic.", "motivation": "Current medical device safety standards advocate for better human factors in user interaction, but many risks aren't caught until real-life testing, which can be dangerous for ambulatory devices. Formal methods can help anticipate errors earlier in development.", "method": "The research uses SPARK Ada's formal verification tool against a behavioral model of the T34 syringe driver, exploring and implementing a Generic Infusion Pump model refinement in SPARK Ada.", "result": "The verification level of the end prototype is evaluated using SPARK, with exploration of potential limitations regarding abstraction and UI design components in SPARK Ada.", "conclusion": "Formal verification methods like SPARK Ada can provide a common framework for safety integration in medical device industry, potentially allowing mathematical proofs to reduce human factor risks early in development."}}
{"id": "2509.16701", "categories": ["cs.SE", "D.2.5; I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16701", "abs": "https://arxiv.org/abs/2509.16701", "authors": ["Shunyu Liu", "Guangdong Bai", "Mark Utting", "Guowei Yang"], "title": "RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code", "comment": "11 pages, 5 figures, under review at TSE", "summary": "Automated Program Repair (APR) has emerged as a promising paradigm for\nreducing debugging time and improving the overall efficiency of software\ndevelopment. Recent advances in Large Language Models (LLMs) have demonstrated\ntheir potential for automated bug fixing and other software engineering tasks.\nNevertheless, the general-purpose nature of LLM pre-training means these models\noften lack the capacity to perform project-specific repairs, which require\nunderstanding of domain-specific identifiers, code structures, and contextual\nrelationships within a particular codebase. As a result, LLMs may struggle to\ngenerate correct patches when the repair depends on project-specific\ninformation.\n  To address this limitation, we introduce RelRepair, a novel approach that\nretrieves relevant project-specific code to enhance automated program repair.\nRelRepair first identifies relevant function signatures by analyzing function\nnames and code comments within the project. It then conducts deeper code\nanalysis to retrieve code snippets relevant to the repair context. The\nretrieved relevant information is then incorporated into the LLM's input\nprompt, guiding the model to generate more accurate and informed patches. We\nevaluate RelRepair on two widely studied datasets, Defects4J V1.2 and\nManySStuBs4J, and compare its performance against several state-of-the-art\nLLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J\nV1.2. Furthermore, RelRepair achieves a 17.1\\% improvement in the ManySStuBs4J\ndataset, increasing the overall fix rate to 48.3\\%. These results highlight the\nimportance of providing relevant project-specific information to LLMs, shedding\nlight on effective strategies for leveraging LLMs in APR tasks.", "AI": {"tldr": "RelRepair is a novel approach that enhances automated program repair by retrieving project-specific code to guide LLMs in generating more accurate patches.", "motivation": "LLMs struggle with project-specific repairs due to their general-purpose pre-training, lacking understanding of domain-specific identifiers and code structures within specific codebases.", "method": "RelRepair identifies relevant function signatures by analyzing function names and comments, then retrieves relevant code snippets through deeper code analysis, incorporating this information into LLM prompts.", "result": "RelRepair repairs 101 bugs in Defects4J V1.2 and achieves a 17.1% improvement on ManySStuBs4J, increasing the overall fix rate to 48.3%.", "conclusion": "Providing relevant project-specific information to LLMs is crucial for effective automated program repair, demonstrating the importance of context-aware approaches in LLM-based APR."}}
{"id": "2509.16795", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16795", "abs": "https://arxiv.org/abs/2509.16795", "authors": ["Saikat Mondal", "Chanchal K. Roy", "Hong Wang", "Juan Arguello", "Samantha Mathan"], "title": "Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction", "comment": "Accepted in the 35th IEEE International Conference on Collaborative\n  Advances in Software Computing", "summary": "API misuse introduces security vulnerabilities, system failures, and\nincreases maintenance costs, all of which remain critical challenges in\nsoftware development. Existing detection approaches rely on static analysis or\nmachine learning-based tools that operate post-development, which delays defect\nresolution. Delayed defect resolution can significantly increase the cost and\ncomplexity of maintenance and negatively impact software reliability and user\ntrust. AI-powered code assistants, such as GitHub Copilot, offer the potential\nfor real-time API misuse detection within development environments. This study\nevaluates GitHub Copilot's effectiveness in identifying and correcting API\nmisuse using MUBench, which provides a curated benchmark of misuse cases. We\nconstruct 740 misuse examples, manually and via AI-assisted variants, using\ncorrect usage patterns and misuse specifications. These examples and 147\ncorrect usage cases are analyzed using Copilot integrated in Visual Studio\nCode. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and\nrecall of 92.4%. It performed strongly on common misuse types (e.g.,\nmissing-call, null-check) but struggled with compound or context-sensitive\ncases. Notably, Copilot successfully fixed over 95% of the misuses it\nidentified. These findings highlight both the strengths and limitations of\nAI-driven coding assistants, positioning Copilot as a promising tool for\nreal-time pair programming and detecting and fixing API misuses during software\ndevelopment.", "AI": {"tldr": "GitHub Copilot achieves 86.2% accuracy in detecting API misuse with 91.2% precision and 92.4% recall, successfully fixing over 95% of identified misuses, though it struggles with complex cases.", "motivation": "API misuse causes security vulnerabilities and system failures, but existing detection tools operate post-development, delaying resolution and increasing costs. AI code assistants like Copilot offer potential for real-time detection.", "method": "Evaluated Copilot using MUBench with 740 misuse examples (manual and AI-assisted) and 147 correct usage cases in Visual Studio Code, analyzing detection accuracy, precision, recall, and fix rates.", "result": "Copilot achieved 86.2% detection accuracy, 91.2% precision, 92.4% recall, and fixed over 95% of identified misuses. Strong performance on common misuse types but limitations with compound/context-sensitive cases.", "conclusion": "Copilot shows promise for real-time API misuse detection and correction during development, though improvements are needed for complex misuse scenarios."}}
{"id": "2509.16844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16844", "abs": "https://arxiv.org/abs/2509.16844", "authors": ["Rim Zrelli", "Henrique Amaral Misson", "Sorelle Kamkuimo", "Maroua Ben Attia", "Abdo Shabah", "Felipe Gohring de Magalhaes", "Gabriela Nicolescu"], "title": "Implementation of the Collision Avoidance System for DO-178C Compliance", "comment": null, "summary": "This technical report presents the detailed implementation of a Collision\nAvoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case\nstudy to demonstrate a rigorous methodology for achieving DO-178C compliance in\nsafety-critical software. The CAS is based on functional requirements inspired\nby NASA's Access 5 project and is designed to autonomously detect, evaluate,\nand avoid potential collision threats in real-time, supporting the safe\nintegration of UAVs into civil airspace.\n  The implementation environment combines formal methods, model-based\ndevelopment, and automated verification tools, including Alloy, SPIN, Simulink\nEmbedded Coder, and the LDRA tool suite. The report documents each phase of the\nsoftware lifecycle: requirements specification and validation, architectural\nand detailed design, coding, verification, and traceability, with a strong\nfocus on compliance with DO-178C Design Assurance Level B objectives.\n  Results demonstrate that formal modelling and automated toolchains enabled\nearly detection and correction of specification defects, robust traceability,\nand strong evidence of verification and validation across all development\nstages. Static and dynamic analyses confirmed code quality and coverage, while\nformal verification methods provided mathematical assurance of correctness for\ncritical components. Although the integration phase was not fully implemented,\nthe approach proved effective in addressing certification challenges for UAV\nsafety-critical systems.\n  \\keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),\nDO-178C compliance, Safety-critical software, Formal methods, Model-based\ndevelopment, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool\nsuite, Software verification and validation, Traceability, Certification.", "AI": {"tldr": "Implementation of a DO-178C compliant Collision Avoidance System for UAVs using formal methods and automated verification tools, demonstrating effective certification approach for safety-critical software.", "motivation": "To demonstrate a rigorous methodology for achieving DO-178C compliance in safety-critical software for UAVs, supporting safe integration into civil airspace.", "method": "Combined formal methods, model-based development, and automated verification tools (Alloy, SPIN, Simulink Embedded Coder, LDRA tool suite) throughout the software lifecycle from requirements to verification.", "result": "Early detection and correction of specification defects, robust traceability, strong verification evidence, confirmed code quality and coverage, mathematical assurance of correctness for critical components.", "conclusion": "The approach proved effective in addressing certification challenges for UAV safety-critical systems, although full integration phase was not implemented."}}
{"id": "2509.16864", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16864", "abs": "https://arxiv.org/abs/2509.16864", "authors": ["Wei Liu", "Yi Wen Heng", "Feng Lin", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions", "comment": "ASE 2025 Industry Showcase", "summary": "Mobile operating systems (OS) are frequently updated, but such updates can\nunintentionally degrade user experience by introducing performance regressions.\nExisting detection techniques often rely on system-level metrics (e.g., CPU or\nmemory usage) or focus on specific OS components, which may miss regressions\nactually perceived by users -- such as slower responses or UI stutters. To\naddress this gap, we present MobileUPReg, a black-box framework for detecting\nuser-perceived performance regressions across OS versions. MobileUPReg runs the\nsame apps under different OS versions and compares user-perceived performance\nmetrics -- response time, finish time, launch time, and dropped frames -- to\nidentify regressions that are truly perceptible to users. In a large-scale\nstudy, MobileUPReg achieves high accuracy in extracting user-perceived metrics\nand detects user-perceived regressions with 0.96 precision, 0.91 recall, and\n0.93 F1-score -- significantly outperforming a statistical baseline using the\nWilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an\nindustrial CI pipeline, where it analyzes thousands of screencasts across\nhundreds of apps daily and has uncovered regressions missed by traditional\ntools. These results demonstrate that MobileUPReg enables accurate, scalable,\nand perceptually aligned regression detection for mobile OS validation.", "AI": {"tldr": "MobileUPReg is a black-box framework that detects user-perceived performance regressions in mobile OS updates by comparing metrics like response time and dropped frames across OS versions, achieving high accuracy and deployment in industrial CI pipelines.", "motivation": "Existing detection techniques rely on system-level metrics or focus on specific OS components, missing regressions actually perceived by users such as slower responses or UI stutters.", "method": "MobileUPReg runs the same apps under different OS versions and compares user-perceived performance metrics (response time, finish time, launch time, and dropped frames) to identify perceptible regressions.", "result": "MobileUPReg achieves 0.96 precision, 0.91 recall, and 0.93 F1-score, significantly outperforming statistical baselines. It has been deployed in industrial CI pipelines, analyzing thousands of screencasts daily and uncovering regressions missed by traditional tools.", "conclusion": "MobileUPReg enables accurate, scalable, and perceptually aligned regression detection for mobile OS validation."}}
{"id": "2509.16870", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16870", "abs": "https://arxiv.org/abs/2509.16870", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems", "comment": "Under Review", "summary": "Intelligent software systems powered by Large Language Models (LLMs) are\nincreasingly deployed in critical sectors, raising concerns about their safety\nduring runtime. Through an industry-academic collaboration when deploying an\nLLM-powered virtual customer assistant, a critical software engineering\nchallenge emerged: how to enhance a safer deployment of LLM-powered software\nsystems at runtime? While LlamaGuard, the current state-of-the-art runtime\nguardrail, offers protection against unsafe inputs, our study reveals a Defense\nSuccess Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak\nattacks. In this paper, we propose DecipherGuard, a novel framework that\nintegrates a deciphering layer to counter obfuscation-based prompts and a\nlow-rank adaptation mechanism to enhance guardrail effectiveness against\ntemplate-based attacks. Empirical evaluation on over 22,000 prompts\ndemonstrates that DecipherGuard improves DSR by 36% to 65% and Overall\nGuardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other\nruntime guardrails. These results highlight the effectiveness of DecipherGuard\nin defending LLM-powered software systems against jailbreak attacks during\nruntime.", "AI": {"tldr": "DecipherGuard is a novel framework that improves runtime safety of LLM-powered systems by integrating a deciphering layer and low-rank adaptation to defend against obfuscation- and template-based jailbreak attacks.", "motivation": "LLM-powered systems are increasingly deployed in critical sectors, but current guardrails like LlamaGuard show significant vulnerability (24% DSR drop) under sophisticated jailbreak attacks, creating safety concerns during runtime deployment.", "method": "Proposes DecipherGuard framework with two key components: a deciphering layer to counter obfuscation-based prompts and a low-rank adaptation mechanism to enhance effectiveness against template-based attacks.", "result": "Empirical evaluation on 22,000+ prompts shows DecipherGuard improves Defense Success Rate (DSR) by 36-65% and Overall Guardrail Performance (OGP) by 20-50% compared to LlamaGuard and other runtime guardrails.", "conclusion": "DecipherGuard effectively defends LLM-powered software systems against jailbreak attacks during runtime, significantly outperforming existing state-of-the-art guardrail solutions."}}
{"id": "2509.16939", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16939", "abs": "https://arxiv.org/abs/2509.16939", "authors": ["Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling", "comment": "Submitted on April 26, 2025. Under review", "summary": "Software Reliability Growth Models (SRGMs) are widely used to predict\nsoftware reliability based on defect discovery data collected during testing or\noperational phases. However, their predictive accuracy often degrades in\ndata-scarce environments, such as early-stage testing or safety-critical\nsystems. Although cross-project transfer learning has been explored to mitigate\nthis issue by leveraging data from past projects, its applicability remains\nlimited due to the scarcity and confidentiality of real-world datasets. To\novercome these limitations, we propose Deep Synthetic Cross-project SRGM\n(DSC-SRGM), a novel approach that integrates synthetic data generation with\ncross-project transfer learning. Synthetic datasets are generated using\ntraditional SRGMs to preserve the statistical characteristics of real-world\ndefect discovery trends. A cross-correlation-based clustering method is applied\nto identify synthetic datasets with patterns similar to the target project.\nThese datasets are then used to train a deep learning model for reliability\nprediction. The proposed method is evaluated on 60 real-world datasets, and its\nperformance is compared with both traditional SRGMs and cross-project deep\nlearning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%\nimprovement in predictive accuracy over traditional SRGMs and 32.2% over\ncross-project deep learning models trained on real-world datasets. However,\nexcessive use of synthetic data or a naive combination of synthetic and\nreal-world data may degrade prediction performance, highlighting the importance\nof maintaining an appropriate data balance. These findings indicate that\nDSC-SRGM is a promising approach for software reliability prediction in\ndata-scarce environments.", "AI": {"tldr": "DSC-SRGM integrates synthetic data generation with cross-project transfer learning to improve software reliability prediction in data-scarce environments, achieving up to 23.3% improvement over traditional methods.", "motivation": "Software Reliability Growth Models (SRGMs) suffer from degraded predictive accuracy in data-scarce environments, and cross-project transfer learning is limited by dataset scarcity and confidentiality.", "method": "Generate synthetic datasets using traditional SRGMs, apply cross-correlation-based clustering to identify similar patterns to target projects, then train deep learning models on these synthetic datasets for reliability prediction.", "result": "DSC-SRGM achieves up to 23.3% improvement over traditional SRGMs and 32.2% over cross-project deep learning models trained on real-world datasets, evaluated on 60 real-world datasets.", "conclusion": "DSC-SRGM is a promising approach for software reliability prediction in data-scarce environments, though maintaining appropriate data balance between synthetic and real-world data is crucial to avoid performance degradation."}}
{"id": "2509.16941", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16941", "abs": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "comment": null, "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.", "AI": {"tldr": "SWE-Bench Pro is a challenging benchmark for software engineering AI agents, featuring 1,865 complex enterprise-level problems from 41 repositories, with current models achieving below 25% success rate.", "motivation": "To create a more realistic and challenging benchmark that captures complex, enterprise-level software engineering problems beyond the scope of existing benchmarks like SWE-BENCH.", "method": "Built upon SWE-BENCH best practices, SWE-Bench Pro contains 1,865 problems from 41 repositories across business applications, B2B services, and developer tools. The benchmark is partitioned into public, held-out, and commercial sets with human-verified tasks requiring multi-file patches and substantial code modifications.", "result": "Current coding models perform poorly on SWE-Bench Pro, with the highest score (GPT-5) achieving only 23.3% Pass@1. The benchmark reveals significant limitations in current AI models' ability to handle complex software engineering tasks.", "conclusion": "SWE-Bench Pro provides a contamination-resistant testbed that better captures real-world software development complexity, advancing the development of truly autonomous software engineering agents at professional levels."}}
{"id": "2509.16985", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16985", "abs": "https://arxiv.org/abs/2509.16985", "authors": ["James J. Cusick"], "title": "Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results", "comment": "A total of 8 pages, 7 figures, 4 tables, and 31 references", "summary": "Software vulnerabilities remain a significant risk factor in achieving\nsecurity objectives within software development organizations. This is\nespecially true where either proprietary or open-source software (OSS) is\nincluded in the technological environment. In this paper an end-to-end process\nwith supporting methods and tools is presented. This industry proven generic\nprocess allows for the custom instantiation, configuration, and execution of\nroutinized code scanning for software vulnerabilities and their prioritized\nremediation. A select set of tools are described for this key DevSecOps\nfunction and placed into an iterative process. Examples of both industrial\nproprietary applications and open-source applications are provided including\nspecific vulnerability instances and a discussion of their treatment. The\nbenefits of each selected tool are considered, and alternative tools are also\nintroduced. Application of this method in a comprehensive SDLC model is also\nreviewed along with prospective enhancements from automation and the\napplication of advanced technologies including AI. Adoption of this method can\nbe achieved with minimal adjustments and with maximum flexibility for results\nin reducing source code vulnerabilities, reducing supply chain risk, and\nimproving the security profile of new or legacy solutions.", "AI": {"tldr": "An end-to-end process for routinized code scanning and prioritized remediation of software vulnerabilities in both proprietary and open-source software, with tool selection and integration into DevSecOps workflows.", "motivation": "Software vulnerabilities pose significant security risks in software development, especially when using both proprietary and open-source software components.", "method": "Presents an industry-proven generic process for custom instantiation, configuration, and execution of code scanning, with selected tools integrated into an iterative DevSecOps process.", "result": "Provides examples of vulnerability instances and their treatment in both industrial proprietary and open-source applications, demonstrating the process effectiveness.", "conclusion": "The method can be adopted with minimal adjustments to reduce source code vulnerabilities, decrease supply chain risk, and improve security profiles of both new and legacy solutions, with potential enhancements from automation and AI technologies."}}
{"id": "2509.17096", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17096", "abs": "https://arxiv.org/abs/2509.17096", "authors": ["Ziyou Li", "Agnia Sergeyuk", "Maliheh Izadi"], "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large Language Models are transforming software engineering, yet prompt\nmanagement in practice remains ad hoc, hindering reliability, reuse, and\nintegration into industrial workflows. We present Prompt-with-Me, a practical\nsolution for structured prompt management embedded directly in the development\nenvironment. The system automatically classifies prompts using a\nfour-dimensional taxonomy encompassing intent, author role, software\ndevelopment lifecycle stage, and prompt type. To enhance prompt reuse and\nquality, Prompt-with-Me suggests language refinements, masks sensitive\ninformation, and extracts reusable templates from a developer's prompt library.\nOur taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can\naccurately classify software engineering prompts. Furthermore, our user study\nwith 11 participants shows strong developer acceptance, with high usability\n(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in\nprompt quality and efficiency through reduced repetitive effort. Lastly, we\noffer actionable insights for building the next generation of prompt management\nand maintenance tools for software engineering workflows.", "AI": {"tldr": "Prompt-with-Me is a structured prompt management system for software engineering that automatically classifies prompts using a four-dimensional taxonomy and provides features like language refinement, sensitive information masking, and template extraction to improve prompt quality and reuse.", "motivation": "Current prompt management in software engineering is ad hoc, hindering reliability, reuse, and integration into industrial workflows, despite LLMs transforming the field.", "method": "The system uses a four-dimensional taxonomy (intent, author role, software development lifecycle stage, prompt type) for automatic classification, and includes features for language refinement, sensitive information masking, and template extraction. Validated through taxonomy study of 1108 real-world prompts and user study with 11 participants.", "result": "LLMs can accurately classify software engineering prompts. User study showed strong acceptance with high usability (Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in prompt quality and efficiency through reduced repetitive effort.", "conclusion": "The paper offers actionable insights for building next-generation prompt management tools for software engineering workflows, demonstrating practical viability of structured prompt management."}}
{"id": "2509.17314", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17314", "abs": "https://arxiv.org/abs/2509.17314", "authors": ["Juyeon Yoon", "Somin Kim", "Robert Feldt", "Shin Yoo"], "title": "Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs", "comment": null, "summary": "Software increasingly relies on the emergent capabilities of Large Language\nModels (LLMs), from natural language understanding to program analysis and\ngeneration. Yet testing them on specific tasks remains difficult and costly:\nmany prompts lack ground truth, forcing reliance on human judgment, while\nexisting uncertainty and adequacy measures typically require full inference. A\nkey challenge is to assess input adequacy in a way that reflects the demands of\nthe task, ideally before even generating any output. We introduce CLOTHO, a\ntask-specific, pre-generation adequacy measure that estimates input difficulty\ndirectly from hidden LLM states. Given a large pool of unlabelled inputs for a\nspecific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample\nthe most informative cases for human labelling. Based on this reference set the\nGMM can then rank unseen inputs by their likelihood of failure. In our\nempirical evaluation across eight benchmark tasks and three open-weight LLMs,\nCLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference\nsets that are on average only 5.4% of inputs. It does so without generating any\noutputs, thereby reducing costs compared to existing uncertainty measures.\nComparison of CLOTHO and post-generation uncertainty measures shows that the\ntwo approaches complement each other. Crucially, we show that adequacy scores\nlearnt from open-weight LLMs transfer effectively to proprietary models,\nextending the applicability of the approach. When prioritising test inputs for\nproprietary models, CLOTHO increases the average number of failing inputs from\n18.7 to 42.5 out of 100, compared to random prioritisation.", "AI": {"tldr": "CLOTHO is a task-specific adequacy measure that estimates input difficulty from hidden LLM states before generation, using Gaussian Mixture Models to predict failures with high accuracy while reducing labeling costs.", "motivation": "Testing LLMs on specific tasks is difficult and costly due to lack of ground truth and reliance on human judgment. Existing measures require full inference, creating a need for pre-generation adequacy assessment.", "method": "Uses Gaussian Mixture Model (GMM) to adaptively sample informative cases from unlabeled inputs for human labeling, then ranks unseen inputs by failure likelihood based on hidden LLM states without generating outputs.", "result": "Achieves ROC-AUC of 0.716 across 8 benchmark tasks and 3 LLMs, with reference sets only 5.4% of inputs. Increases failing inputs from 18.7 to 42.5 out of 100 for proprietary models compared to random prioritization.", "conclusion": "CLOTHO effectively predicts LLM failures pre-generation, reduces costs, and transfers well from open-weight to proprietary models, complementing existing post-generation uncertainty measures."}}
{"id": "2509.17335", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17335", "abs": "https://arxiv.org/abs/2509.17335", "authors": ["Mingxuan Xiao", "Yan Xiao", "Shunhui Ji", "Jiahe Tu", "Pengcheng Zhang"], "title": "BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing", "comment": null, "summary": "Fuzzing has shown great success in evaluating the robustness of intelligent\nnatural language processing (NLP) software. As large language model (LLM)-based\nNLP software is widely deployed in critical industries, existing methods still\nface two main challenges: 1 testing methods are insufficiently coupled with the\nbehavioral patterns of LLM-based NLP software; 2 fuzzing capability for the\ntesting scenario of natural language generation (NLG) generally degrades. To\naddress these issues, we propose BASFuzz, an efficient Fuzz testing method\ntailored for LLM-based NLP software. BASFuzz targets complete test inputs\ncomposed of prompts and examples, and uses a text consistency metric to guide\nmutations of the fuzzing loop, aligning with the behavioral patterns of\nLLM-based NLP software. A Beam-Annealing Search algorithm, which integrates\nbeam search and simulated annealing, is employed to design an efficient fuzzing\nloop. In addition, information entropy-based adaptive adjustment and an elitism\nstrategy further enhance fuzzing capability. We evaluate BASFuzz on six\ndatasets in representative scenarios of NLG and natural language understanding\n(NLU). Experimental results demonstrate that BASFuzz achieves a testing\neffectiveness of 90.335% while reducing the average time overhead by 2,163.852\nseconds compared to the current best baseline, enabling more effective\nrobustness evaluation prior to software deployment.", "AI": {"tldr": "BASFuzz is an efficient fuzz testing method for LLM-based NLP software that addresses challenges in testing behavioral patterns and natural language generation scenarios through text consistency metrics and Beam-Annealing Search algorithm.", "motivation": "Existing fuzzing methods for LLM-based NLP software face challenges: insufficient coupling with behavioral patterns and degraded capability for natural language generation testing, especially as LLMs are deployed in critical industries.", "method": "BASFuzz targets complete test inputs (prompts + examples), uses text consistency metrics to guide mutations, employs Beam-Annealing Search (combining beam search and simulated annealing), and incorporates information entropy-based adaptive adjustment with elitism strategy.", "result": "BASFuzz achieves 90.335% testing effectiveness while reducing average time overhead by 2,163.852 seconds compared to best baseline, evaluated on six datasets across NLG and NLU scenarios.", "conclusion": "BASFuzz enables more effective robustness evaluation for LLM-based NLP software prior to deployment by addressing key testing challenges through tailored fuzzing approach."}}
{"id": "2509.17338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17338", "abs": "https://arxiv.org/abs/2509.17338", "authors": ["Pengfei He", "Shaowei Wang", "Tse-Hsun Chen"], "title": "SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding", "comment": "3 tables, 6 Figures, 12 pages", "summary": "Static program slicing is a fundamental technique in software engineering.\nTraditional static slicing tools rely on parsing complete source code, which\nlimits their applicability to real-world scenarios where code snippets are\nincomplete or unparsable. While recent research developed learning-based\napproaches to predict slices, they face critical challenges: (1) Inaccurate\ndependency identification, where models fail to precisely capture data and\ncontrol dependencies between code elements; and (2) Unconstrained generation,\nwhere models produce slices with extraneous or hallucinated tokens not present\nin the input, violating the structural integrity of slices. To address these\nchallenges, we propose \\ourtool, a novel slicing framework that reformulates\nstatic program slicing as a sequence-to-sequence task using lightweight\nlanguage models (e.g., CodeT5+). Our approach incorporates two key innovations.\nFirst, we introduce a copy mechanism that enables the model to more accurately\ncapture inter-element dependencies and directly copy relevant tokens from the\ninput, improving both dependency reasoning and generation constraint. Second,\nwe design a constrained decoding process with (a) lexical constraint,\nrestricting outputs to input tokens only, and (b) syntactic constraint,\nleveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect\nstructurally invalid outputs and discard them. We evaluate \\ourtool on CodeNet\nand LeetCode datasets and show it consistently outperforms state-of-the-art\nbaselines, improving ExactMatch scores by up to 27\\%. Furthermore, \\ourtool\ndemonstrates strong performance on incomplete code, highlighting its robustness\nand practical utility in real-world development environments.", "AI": {"tldr": "\\ourtool is a novel static program slicing framework that reformulates slicing as a sequence-to-sequence task using lightweight language models with copy mechanism and constrained decoding to address dependency identification and generation constraint challenges.", "motivation": "Traditional static slicing tools require complete parsable source code, limiting real-world applicability. Learning-based approaches face inaccurate dependency identification and unconstrained generation problems.", "method": "Proposes \\ourtool with copy mechanism for accurate dependency capture and constrained decoding (lexical constraint restricting outputs to input tokens, and syntactic constraint using TSED monotonicity to detect invalid outputs).", "result": "Outperforms state-of-the-art baselines on CodeNet and LeetCode datasets, improving ExactMatch scores by up to 27%, and demonstrates strong performance on incomplete code.", "conclusion": "\\ourtool provides robust and practical static program slicing for real-world development environments, effectively addressing key challenges in learning-based slicing approaches."}}
{"id": "2509.17548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17548", "abs": "https://arxiv.org/abs/2509.17548", "authors": ["Hugo Villamizar", "Jannik Fischbach", "Alexander Korn", "Andreas Vogelsang", "Daniel Mendez"], "title": "Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings", "comment": "This paper has been accepted for presentation at the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Developers now routinely interact with large language models (LLMs) to\nsupport a range of software engineering (SE) tasks. This prominent role\npositions prompts as potential SE artifacts that, like other artifacts, may\nrequire systematic development, documentation, and maintenance. However, little\nis known about how prompts are actually used and managed in LLM-integrated\nworkflows, what challenges practitioners face, and whether the benefits of\nsystematic prompt management outweigh the associated effort. To address this\ngap, we propose a research programme that (a) characterizes current prompt\npractices, challenges, and influencing factors in SE; (b) analyzes prompts as\nsoftware artifacts, examining their evolution, traceability, reuse, and the\ntrade-offs of systematic management; and (c) develops and empirically evaluates\nevidence-based guidelines for managing prompts in LLM-integrated workflows. As\na first step, we conducted an exploratory survey with 74 software professionals\nfrom six countries to investigate current prompt practices and challenges. The\nfindings reveal that prompt usage in SE is largely ad-hoc: prompts are often\nrefined through trial-and-error, rarely reused, and shaped more by individual\nheuristics than standardized practices. These insights not only highlight the\nneed for more systematic approaches to prompt management but also provide the\nempirical foundation for the subsequent stages of our research programme.", "AI": {"tldr": "This paper proposes a research program to study prompt management in LLM-integrated software engineering workflows, highlighting the current ad-hoc nature of prompt usage and the need for systematic approaches.", "motivation": "Prompts are becoming key software engineering artifacts but little is known about how they're actually used, managed, and whether systematic management is worthwhile. Current practices are largely ad-hoc with trial-and-error refinement.", "method": "The research program involves: (a) characterizing current prompt practices and challenges, (b) analyzing prompts as software artifacts, and (c) developing evidence-based guidelines. An exploratory survey with 74 professionals from 6 countries was conducted as a first step.", "result": "Survey findings reveal prompt usage in SE is largely ad-hoc: prompts are refined through trial-and-error, rarely reused, and shaped by individual heuristics rather than standardized practices.", "conclusion": "The findings highlight the need for more systematic approaches to prompt management and provide empirical foundation for the research program's subsequent stages focused on developing evidence-based guidelines."}}
{"id": "2509.17629", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17629", "abs": "https://arxiv.org/abs/2509.17629", "authors": ["Antonio Bucchiarone", "Juri Di Rocco", "Damiano Di Vincenzo", "Alfonso Pierantonio"], "title": "From OCL to JSX: declarative constraint modeling in modern SaaS tools", "comment": "10 pages, 2 Figures, Joint Proceedings of the STAF 2025 Workshops:\n  OCL, OOPSLE, LLM4SE, ICMM, AgileMDE, AI4DPS, and TTC. Koblenz, Germany, June\n  10-13, 2025", "summary": "The rise of Node.js in 2010, followed by frameworks like Angular, React, and\nVue.js, has accelerated the growth of low code development platforms. These\nplatforms harness modern UIX paradigms, component-based architectures, and the\nSaaS model to enable non-experts to build software. The widespread adoption of\nsingle-page applications (SPAs), driven by these frameworks, has shaped\nlow-code tools to deliver responsive, client side experiences. In parallel,\nmany modeling platforms have moved to the cloud, adopting either server-centric\narchitectures (e.g., GSLP) or client-side intelligence via SPA frameworks,\nanchoring core components in JavaScript or TypeScript. Within this context,\nOCL.js, a JavaScript-based implementation of the Object Constraint Language,\noffers a web aligned approach to model validation, yet faces challenges such as\npartial standard coverage, limited adoption, and weak integration with modern\nfront-end toolchains. In this paper, we explore JSX, a declarative, functional\nsubset of JavaScript/TypeScript used in the React ecosystem, as an alternative\nto constraint expression in SaaS-based modeling environments. Its\ncomponent-oriented structure supports inductive definitions for syntax, code\ngeneration, and querying. Through empirical evaluation, we compare JSX-based\nconstraints with OCL.js across representative modeling scenarios. Results show\nJSX provides broader expressiveness and better fits front-end-first\narchitectures, indicating a promising path for constraint specification in\nmodern modeling tools.", "AI": {"tldr": "JSX-based constraints outperform OCL.js in web-based modeling tools, offering better expressiveness and front-end integration.", "motivation": "OCL.js faces challenges like partial standard coverage and weak integration with modern front-end toolchains, despite being web-aligned for model validation.", "method": "Explore JSX (from React ecosystem) as an alternative to OCL.js for constraint expression in SaaS modeling environments, using empirical evaluation across representative modeling scenarios.", "result": "JSX provides broader expressiveness and better fits front-end-first architectures compared to OCL.js.", "conclusion": "JSX shows promise as a constraint specification language for modern modeling tools, particularly in front-end-focused architectures."}}
{"id": "2509.17776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17776", "abs": "https://arxiv.org/abs/2509.17776", "authors": ["Cristina Stratan", "Claudio Mandrioli", "Domenico Bianculli"], "title": "Diagnosing Violations of State-based Specifications in iCFTL", "comment": null, "summary": "As modern software systems grow in complexity and operate in dynamic\nenvironments, the need for runtime analysis techniques becomes a more critical\npart of the verification and validation process. Runtime verification monitors\nthe runtime system behaviour by checking whether an execution trace - a\nsequence of recorded events - satisfies a given specification, yielding a\nBoolean or quantitative verdict. However, when a specification is violated,\nsuch a verdict is often insufficient to understand why the violation happened.\nTo fill this gap, diagnostics approaches aim to produce more informative\nverdicts. In this paper, we address the problem of generating informative\nverdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)\nspecifications that express constraints over program variable values. We\npropose a diagnostic approach based on backward data-flow analysis to\nstatically determine the relevant statements contributing to the specification\nviolation. Using this analysis, we instrument the program to produce enriched\nexecution traces. Using the enriched execution traces, we perform the runtime\nanalysis and identify the statements whose execution led to the specification\nviolation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,\nand evaluated it on 112 specifications across 10 software projects. Our tool\nachieves 90% precision in identifying relevant statements for 100 of the 112\nspecifications. It reduces the number of lines that have to be inspected for\ndiagnosing a violation by at least 90%. In terms of computational cost,\niCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than\n25 MB of memory. The instrumentation required to support diagnostics incurs an\nexecution time overhead of less than 30% and a memory overhead below 20%.", "AI": {"tldr": "This paper presents iCFTL-Diagnostics, a tool that generates informative diagnostics for runtime verification violations by using backward data-flow analysis to identify relevant statements that caused specification violations.", "motivation": "Runtime verification monitors system behavior but often provides insufficient information when specifications are violated. Current Boolean or quantitative verdicts don't explain why violations occurred, making debugging difficult.", "method": "The approach uses backward data-flow analysis to statically determine relevant statements contributing to specification violations, instruments programs to produce enriched execution traces, and performs runtime analysis to identify violation-causing statements.", "result": "The tool achieves 90% precision in identifying relevant statements for 100 out of 112 specifications, reduces inspection lines by at least 90%, generates diagnoses within 7 minutes using \u226425MB memory, with less than 30% execution time overhead and below 20% memory overhead from instrumentation.", "conclusion": "iCFTL-Diagnostics effectively addresses the diagnostic gap in runtime verification by providing detailed information about specification violations while maintaining reasonable computational costs, making it practical for real-world software verification."}}
