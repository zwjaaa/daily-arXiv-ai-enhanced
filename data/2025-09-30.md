<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 35]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: Largest benchmark for vericoding (formal code generation from specifications) with 12,504 specs across Dafny, Verus/Rust, and Lean. Success rates: 82% in Dafny, 44% in Verus/Rust, 27% in Lean using off-the-shelf LLMs.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive benchmark for evaluating LLM performance in generating formally verified code from formal specifications, distinguishing it from natural language-based coding.

Method: Created benchmark with 12,504 formal specifications (3,029 Dafny, 2,334 Verus/Rust, 7,141 Lean) including 6,174 new problems. Tested off-the-shelf LLMs on vericoding tasks and measured success rates.

Result: Vericoding success rates: 82% in Dafny, 44% in Verus/Rust, 27% in Lean. Natural language descriptions didn't significantly improve performance. Pure Dafny verification improved from 68% to 96% over past year.

Conclusion: Vericoding is viable with current LLMs, with significant performance differences across verification languages. The benchmark enables systematic evaluation of formal code generation capabilities.

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [2] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: This paper proposes using large language models (LLMs) like ChatGPT-4 as post hoc explainers for ML-based code clone detectors, achieving high accuracy in explaining predictions without requiring white-box model access.


<details>
  <summary>Details</summary>
Motivation: ML-based code clone detectors are effective but operate as black boxes, lacking interpretability. Current post hoc explanation techniques require white-box access or are computationally expensive, creating a need for better explainability methods.

Method: Leveraged ChatGPT-4's in-context learning capabilities to explain predictions made by GraphCodeBERT code clone detector. Explored the impact of temperature settings on explanation accuracy.

Result: The approach achieved correct explanations up to 98% of the time and good explanations 95% of the time. Lowering temperature to zero increased explanation accuracy. However, the explanations and code line examples were only useful in some cases.

Conclusion: LLMs show promise as post hoc explainers for code clone detection and other software engineering tasks. Future work should focus on improving the usefulness of explanations and code examples provided by LLMs.

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [3] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: AI-assisted programming creates a Matthew effect where LLM-generated code succeeds more in popular languages/frameworks, potentially reinforcing existing hierarchies and reducing ecosystem diversity.


<details>
  <summary>Details</summary>
Motivation: To investigate how AI-assisted programming impacts software development dynamics and ecosystem evolution, beyond just code generation quality.

Method: Large-scale experiments on thousands of algorithmic programming tasks and hundreds of framework selection tasks to analyze LLM performance across different programming languages and frameworks.

Result: Found a strong Matthew effect - LLM-generated code has higher success rates in more popular programming languages and frameworks, suggesting AI systems reinforce existing popularity hierarchies.

Conclusion: AI-assisted programming may accelerate convergence around dominant tools while hindering diversity and innovation in programming ecosystems.

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [4] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: The paper presents a configurable software visualization system with flexible grouping, multi-level metrics, and interactive rendering to enhance source code comprehension.


<details>
  <summary>Details</summary>
Motivation: To improve software comprehension, analysis, maintenance, and evolution by providing better visualization tools that can reveal structural patterns and complexity hotspots that are hard to see directly from source code.

Method: Developed a configurable grouping mechanism for code elements, combined fine-grained and coarse-grained software metrics, and created an interactive visualization engine with dynamic rendering adjustment capabilities.

Result: Created a more adaptable and insightful approach to source code visualization that provides macroscopic overviews while enabling focused inspection of specific program elements.

Conclusion: The three contributions (configurable grouping, multi-level metrics, and interactive visualization) collectively provide a more effective framework for understanding large-scale software systems through visualization.

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [5] [Methods for evaluating software accessibility](https://arxiv.org/abs/2509.23469)
*Mykola Kuz,Ivan Yaremiy,Hanna Yaremii,Mykola Pikuliak,Ihor Lazarovych,Mykola Kozlenko,Denys Vekeryk*

Main category: cs.SE

TL;DR: The paper presents a new detailed methodology for assessing software accessibility, specifically focusing on visual impairments, and applies it to analyze a university website.


<details>
  <summary>Details</summary>
Motivation: Existing accessibility evaluation methods are too generalized and fail to address specific user needs and interaction patterns, necessitating more detailed assessment approaches.

Method: Developed a classification and mathematical model for accessibility assessment, creating a method to evaluate the "Accessibility" subcharacteristic within the "Usability" quality characteristic.

Result: Applied the methodology to analyze Vasyl Stefanyk Precarpathian National University's website, identifying accessibility issues and providing specific improvement recommendations.

Conclusion: The proposed methodology offers a more detailed and practical approach to accessibility assessment compared to standardized methods, contributing to creating more inclusive digital environments.

Abstract: The development and enhancement of methods for evaluating software
accessibility is a relevant challenge in modern software engineering, as
ensuring equal access to digital services is a key factor in improving their
efficiency and inclusivity. The increasing digitalization of society
necessitates the creation of software that complies with international
accessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these
standards helps eliminate barriers to software use for individuals with diverse
physical, sensory, and cognitive needs. Despite advancements in regulatory
frameworks, existing accessibility evaluation methodologies are often
generalized and fail to account for the specific needs of different user
categories or the unique ways they interact with digital systems. This
highlights the need for the development of new, more detailed methods for
defining metrics that influence the quality of user interaction with software
products. Building a classification and mathematical model and developing
accessibility assessment methods for software based on it. A method for
assessing the quality subcharacteristic "Accessibility", which is part of the
"Usability" quality characteristic, has been developed. This enabled the
analysis of a website's inclusivity for individuals with visual impairments,
and the formulation of specific recommendations for further improvements, which
is a crucial step toward creating an inclusive digital environment. Comparing
to standardized approaches, a more detailed and practically oriented
accessibility assessment methodology has been proposed. Using this methodology,
an analysis of the accessibility of the main pages of Vasyl Stefanyk
Precarpathian National University's website was conducted, and improvements
were suggested to enhance its inclusivity.

</details>


### [6] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: AgentDiet is a trajectory reduction approach that reduces computational costs in LLM-based multi-turn agent systems by removing useless, redundant, and expired information from agent trajectories while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Multi-turn LLM agent systems face high computational costs due to growing trajectories, but efficiency is largely neglected in existing studies and products.

Method: Design AgentDiet - a simple yet effective trajectory reduction approach that automatically identifies and removes waste information (useless, redundant, expired) from agent trajectories.

Result: AgentDiet reduces input tokens by 39.9%-59.7% and final computational cost by 21.1%-35.9% while maintaining the same agent performance on two LLMs and two benchmarks.

Conclusion: Trajectory reduction is a promising direction for improving efficiency in agent systems without compromising performance.

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [7] [Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks](https://arxiv.org/abs/2509.23645)
*A S M Shahadat Hossain,Colin Brown,David Koop,Tanu Malik*

Main category: cs.SE

TL;DR: The paper introduces SRI, a metric for assessing reproducibility in Jupyter Notebooks using similarity metrics for different Python objects to compare rerun outputs against originals.


<details>
  <summary>Details</summary>
Motivation: Jupyter Notebooks facilitate sharing computational experiments but rerunning them may not produce identical results due to randomness, library changes, or environmental variations.

Method: Developed Similarity-based Reproducibility Index (SRI) using novel similarity metrics specific to different Python object types to compare rerun outputs with original outputs.

Result: SRI provides quantitative scores (0-1) and qualitative insights for each output-generating cell in rerun notebooks, demonstrated through a case study on multiple Jupyter Notebooks.

Conclusion: The proposed SRI metric effectively quantifies computational reproducibility in Jupyter Notebooks by leveraging various similarity metrics to compare outputs.

Abstract: Computational reproducibility refers to obtaining consistent results when
rerunning an experiment. Jupyter Notebook, a web-based computational notebook
application, facilitates running, publishing, and sharing computational
experiments along with their results. However, rerunning a Jupyter Notebook may
not always generate identical results due to various factors, such as
randomness, changes in library versions, or variations in the computational
environment. This paper introduces the Similarity-based Reproducibility Index
(SRI) -- a metric for assessing the reproducibility of results in Jupyter
Notebooks. SRI employs novel methods developed based on similarity metrics
specific to different types of Python objects to compare rerun outputs against
original outputs. For every cell generating an output in a rerun notebook, SRI
reports a quantitative score in the range [0, 1] as well as some qualitative
insights to assess reproducibility. The paper also includes a case study in
which the proposed metric is applied to a set of Jupyter Notebooks,
demonstrating how various similarity metrics can be leveraged to quantify
computational reproducibility.

</details>


### [8] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: PAT-Agent is an end-to-end framework that combines LLMs with formal verification to automate formal model construction and repair, achieving high verification success with superior efficiency.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs to formal verification is challenging due to specification language complexity, hallucinated output risks, and the semantic gap between natural language and formal logic.

Method: Uses a Planning LLM to extract modeling elements and generate detailed plans, then a Code Generation LLM to synthesize formal models, verified by PAT model checker with iterative repair using counterexamples.

Result: Outperforms baselines on 40 systems, achieves high verification success with superior efficiency. Ablation studies confirm importance of planning and repair components.

Conclusion: The framework enables effective formal modeling for non-experts through user-friendly interface, demonstrating accessibility and practical utility.

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [9] [Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse](https://arxiv.org/abs/2509.23679)
*Zeqin Liao,Yuhong Nan,Zixu Gao,Henglong Liang,Sicheng Hao,Jiajing Wu,Zibin Zheng*

Main category: cs.SE

TL;DR: Satellite is a bytecode-level static analysis framework that detects subcontract misuse vulnerabilities in smart contracts by recovering inherited methods through transfer learning, performing method-level comparison, and using SMV indicators.


<details>
  <summary>Details</summary>
Motivation: Smart contract developers frequently reuse subcontracts for efficiency, but this can introduce vulnerabilities. Bytecode compilation obscures class-level information and semantics, making automatic detection challenging.

Method: Uses transfer learning to recover inherited methods, extracts fine-grained method-level features, performs method-level comparison to identify subcontract reuse, and employs SMV indicators for vulnerability detection.

Result: Achieved 84.68% precision and 92.11% recall on test dataset. Identified 14 new/unknown SMVs in 10,011 real-world contracts affecting $201,358 worth of digital assets.

Conclusion: Satellite effectively detects subcontract misuse vulnerabilities in smart contracts with high accuracy and has practical value in identifying real-world security threats.

Abstract: Developers of smart contracts pervasively reuse subcontracts to improve
development efficiency. Like any program language, such subcontract reuse may
unexpectedly include, or introduce vulnerabilities to the end-point smart
contract. Unfortunately, automatically detecting such issues poses several
unique challenges. Particularly, in most cases, smart contracts are compiled as
bytecode, whose class-level information (e.g., inheritance, virtual function
table), and even semantics (e.g., control flow and data flow) are fully
obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis
framework for subcontract misuse vulnerability (SMV) detection in smart
contracts. Satellite incorporates a series of novel designs to enhance its
overall effectiveness.. Particularly, Satellite utilizes a transfer learning
method to recover the inherited methods, which are critical for identifying
subcontract reuse in smart contracts. Further, Satellite extracts a set of
fine-grained method-level features and performs a method-level comparison, for
identifying the reuse part of subcontract in smart contracts. Finally,
Satellite summarizes a set of SMV indicators according to their types, and
hence effectively identifies SMVs. To evaluate Satellite, we construct a
dataset consisting of 58 SMVs derived from real-world attacks and collect
additional 56 SMV patterns from SOTA studies. Experiment results indicate that
Satellite exhibits good performance in identifying SMV, with a precision rate
of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully
identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting
a total amount of digital assets worth 201,358 USD.

</details>


### [10] [Influence-Guided Concolic Testing of Transformer Robustness](https://arxiv.org/abs/2509.23806)
*Chih-Duo Hong,Yu Wang,Yao-Chen Chang,Fang Yu*

Main category: cs.SE

TL;DR: Influence-guided concolic testing for Transformer classifiers uses SHAP-based influence ranking to efficiently find label-flip inputs, with solver-compatible attention semantics and scheduling heuristics enabling practical testing on modern architectures.


<details>
  <summary>Details</summary>
Motivation: To make concolic testing feasible for contemporary Transformer models by addressing the challenges of constraint solving on modern architectures and improving search efficiency for finding adversarial inputs.

Method: Developed an influence-guided concolic tester that ranks path predicates by SHAP-based influence estimates, created solver-compatible pure-Python semantics for multi-head self-attention, and introduced practical scheduling heuristics to manage constraint growth.

Result: Influence guidance finds label-flip inputs more efficiently than FIFO baseline, maintains steady progress on deeper networks, and reveals recurring compact decision logic shared across attacks through SHAP-based critical decision path analysis.

Conclusion: Influence signals provide useful search bias for symbolic exploration, and solver-friendly attention semantics with lightweight scheduling make concolic testing feasible for Transformer models, offering utility for debugging and model auditing.

Abstract: Concolic testing for deep neural networks alternates concrete execution with
constraint solving to search for inputs that flip decisions. We present an
{influence-guided} concolic tester for Transformer classifiers that ranks path
predicates by SHAP-based estimates of their impact on the model output. To
enable SMT solving on modern architectures, we prototype a solver-compatible,
pure-Python semantics for multi-head self-attention and introduce practical
scheduling heuristics that temper constraint growth on deeper models. In a
white-box study on compact Transformers under small $L_0$ budgets, influence
guidance finds label-flip inputs more efficiently than a FIFO baseline and
maintains steady progress on deeper networks. Aggregating successful attack
instances with a SHAP-based critical decision path analysis reveals recurring,
compact decision logic shared across attacks. These observations suggest that
(i) influence signals provide a useful search bias for symbolic exploration,
and (ii) solver-friendly attention semantics paired with lightweight scheduling
make concolic testing feasible for contemporary Transformer models, offering
potential utility for debugging and model auditing.

</details>


### [11] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: JUnitGenie is a path-sensitive framework that combines code knowledge with LLMs to generate high-coverage unit tests, outperforming existing methods by 29-31% in coverage and finding real bugs.


<details>
  <summary>Details</summary>
Motivation: Existing unit test generation approaches are path-insensitive and struggle with deep control-flow structures, leading to inadequate coverage for complex execution paths.

Method: Extracts code knowledge from Java projects and distills it into structured prompts to guide LLMs in context-aware unit test generation.

Result: Achieved 29.60% and 31.00% improvement in branch and line coverage respectively over baselines, and generated tests that uncovered real-world bugs confirmed by developers.

Conclusion: The path-sensitive approach combining code knowledge with LLMs significantly improves test coverage and bug detection capabilities in unit test generation.

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [12] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: SolContractEval is the first contract-level benchmark for Solidity code generation, evaluating LLMs on real-world smart contracts across 9 domains. Claude-3.7-Sonnet performs best but models struggle with complex logic, inter-contract dependencies, and Solidity-specific features.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluations for Solidity code generation are inadequate as they focus on isolated functions and synthetic inputs, failing to assess real-world contract development capabilities. Solidity constitutes only a small portion of LLM training data and has version-sensitive syntax with limited flexibility.

Method: Created SolContractEval benchmark with 124 tasks from real on-chain contracts across 9 domains, with complete context dependencies and structured contract frameworks. Developed dynamic evaluation framework using historical transaction replay for automated functional correctness assessment. Evaluated 6 mainstream LLMs systematically.

Result: Claude-3.7-Sonnet achieved the highest overall performance, but all models underperformed compared to their capabilities on general-purpose programming languages. Models performed better on standard patterns but struggled with complex logic and inter-contract dependencies. Limited understanding of Solidity-specific features and contextual dependencies was observed.

Conclusion: Current LLMs have significant limitations in Solidity code generation for real-world smart contract development, particularly with complex logic, dependencies, and domain-specific features. The SolContractEval benchmark provides a more realistic evaluation framework for future model improvements.

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [13] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: HFUZZER is a phrase-based fuzzing framework that tests LLMs for package hallucinations in code generation, identifying non-existent packages that could be exploited in supply chain attacks.


<details>
  <summary>Details</summary>
Motivation: LLMs face security risks from package hallucinations where they recommend non-existent packages, creating vulnerabilities for software supply chain attacks. Current research lacks testing frameworks specifically for package hallucinations.

Method: HFUZZER uses phrase-based fuzzing technology to guide LLMs in generating diverse coding tasks. It extracts phrases from package information or coding tasks to ensure relevance between phrases and generated code.

Result: HFUZZER triggers package hallucinations across all tested LLMs, identifying 2.60x more unique hallucinated packages than mutational fuzzing. For GPT-4o alone, it found 46 unique hallucinated packages, revealing hallucinations occur in both code generation and environment configuration.

Conclusion: Package hallucinations are a widespread security issue in LLMs for code generation, and HFUZZER effectively identifies these vulnerabilities, demonstrating the need for robust testing frameworks to defend against potential supply chain attacks.

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [14] [Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization](https://arxiv.org/abs/2509.23961)
*Sheikh Md Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: Proposes a Learning-Based Testing method that integrates hypothesis and mutation testing to efficiently prioritize adversarial test cases for DNNs, outperforming existing approaches in fault detection speed and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing test prioritization methods (coverage-based or confidence-based) often fail to efficiently identify the most fault-revealing inputs in DNNs, limiting their practical effectiveness for critical applications requiring resilience against adversarial inputs.

Method: Integrates Learning-Based Testing with hypothesis and mutation testing to select a subset of adversarial inputs with high likelihood of exposing model faults, without relying on architecture-specific characteristics or formal verification.

Result: The LBT method consistently surpasses baseline approaches in prioritizing fault-revealing inputs and accelerating fault detection, uncovering all potential faults significantly faster across various datasets, model architectures, and adversarial attack techniques.

Conclusion: Beyond improving fault detection, the method preserves input diversity and provides effective guidance for model retraining, establishing it as a powerful and practical solution for adversarial test prioritization in real-world DNN applications.

Abstract: Context: Deep Neural Networks (DNNs) are increasingly deployed in critical
applications, where resilience against adversarial inputs is paramount.
However, whether coverage-based or confidence-based, existing test
prioritization methods often fail to efficiently identify the most
fault-revealing inputs, limiting their practical effectiveness. Aims: This
project aims to enhance fault detection and model robustness in DNNs by
integrating Learning-Based Testing (LBT) with hypothesis and mutation testing
to efficiently prioritize adversarial test cases. Methods: Our method selects a
subset of adversarial inputs with a high likelihood of exposing model faults,
without relying on architecture-specific characteristics or formal
verification, making it adaptable across diverse DNNs. Results: Our results
demonstrate that the proposed LBT method consistently surpasses baseline
approaches in prioritizing fault-revealing inputs and accelerating fault
detection. By efficiently organizing test permutations, it uncovers all
potential faults significantly faster across various datasets, model
architectures, and adversarial attack techniques. Conclusion: Beyond improving
fault detection, our method preserves input diversity and provides effective
guidance for model retraining, further enhancing robustness. These advantages
establish our approach as a powerful and practical solution for adversarial
test prioritization in real-world DNN applications.

</details>


### [15] [SandCell: Sandboxing Rust Beyond Unsafe Code](https://arxiv.org/abs/2509.24032)
*Jialun Zhang,Merve Gülmez,Thomas Nyman,Gang Tan*

Main category: cs.SE

TL;DR: SandCell is a flexible isolation system for Rust that allows fine-grained sandboxing of both safe and unsafe code with minimal annotations, preventing vulnerabilities while maintaining reasonable performance.


<details>
  <summary>Details</summary>
Motivation: Rust's unsafe keyword bypasses memory safety guarantees, creating risks. Existing isolation approaches provide only fixed boundaries and cannot sandbox both safe and unsafe code with expressive policies.

Method: Leverages existing syntactic boundaries in Rust, allows programmers to specify sandboxed components with minimal annotations, and introduces techniques to minimize data transfer overhead between sandboxes.

Result: Effectively prevents vulnerabilities across various Rust applications while maintaining reasonable performance overheads.

Conclusion: SandCell provides flexible, lightweight isolation for Rust that accommodates expressive policies and fine-grained control over sandboxing both safe and unsafe code.

Abstract: Rust is a modern systems programming language that ensures memory safety by
enforcing ownership and borrowing rules at compile time. While the unsafe
keyword allows programmers to bypass these restrictions, it introduces
significant risks. Various approaches for isolating unsafe code to protect safe
Rust from vulnerabilities have been proposed, yet these methods provide only
fixed isolation boundaries and do not accommodate expressive policies that
require sandboxing both safe and unsafe code. This paper presents SandCell for
flexible and lightweight isolation in Rust by leveraging existing syntactic
boundaries. SandCell allows programmers to specify which components to sandbox
with minimal annotation effort, enabling fine-grained control over isolation.
The system also introduces novel techniques to minimize overhead when
transferring data between sandboxes. Our evaluation demonstrates SandCell's
effectiveness in preventing vulnerabilities across various Rust applications
while maintaining reasonable performance overheads.

</details>


### [16] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: PerfBench is a new benchmark with 81 real-world performance bug-fixing tasks from .NET repositories, featuring a novel evaluation harness that allows agents to generate their own performance benchmarks and validate fixes by comparing execution metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on functional correctness but fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs, which waste computational resources without causing functional failures.

Method: Developed PerfBench with tasks derived from actual developer fixes linked to performance-related issues, verified by human experts. Created a novel evaluation harness that allows agents to generate performance benchmarks and validates fixes by comparing execution metrics between developer fix and agent fix.

Result: Current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only ~3% success rate. OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions, achieved ~20% success rate.

Conclusion: Proper instructions for benchmarking changes and tooling for benchmark output processing significantly improve agent performance, but room for improvement remains. PerfBench provides a challenging test set for advancing agent capabilities in fixing performance issues.

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [17] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: TENET is an LLM agent that addresses challenges in test-driven development for vibe coding by selecting diverse test suites, efficiently retrieving code context, and using reflection-based refinement, achieving state-of-the-art performance on code generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: In the era of vibe coding where developers use LLMs for code generation, TDD becomes crucial as tests serve as executable specifications. However, challenges exist in test suite selection, context retrieval, and systematic test feedback usage.

Method: TENET features three components: (1) test harness mechanism for concise diverse test suite selection, (2) tailored agent toolset for efficient code retrieval with interactive debugging, and (3) reflection-based refinement workflow that analyzes failures and applies iterative code refinement.

Result: TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points respectively.

Conclusion: This is the first study of test-driven code generation with repository-level context, demonstrating how test suite aspects affect LLM agent performance in TDD settings, with TENET providing an effective solution for test-driven vibe coding.

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [18] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: MTAM is a metamorphic testing framework for audio content moderation software that generates adversarial audio test cases to evaluate the robustness of moderation systems against evasion attacks.


<details>
  <summary>Details</summary>
Motivation: Audio platforms are increasingly misused to spread harmful content, and existing moderation tools can be bypassed through subtle audio modifications, but their effectiveness against such adversarial inputs is not well studied.

Method: Proposed MTAM framework with 14 metamorphic relations across Audio Features-Based and Heuristic perturbation categories, applied to toxic audio content to generate test cases that remain harmful but evade detection.

Result: MTAM achieved error finding rates up to 38.6%, 18.3%, 35.1%, 16.7%, and 51.1% for commercial software from Gladia, Assembly AI, Baidu, Nextdata, and Tencent respectively, and up to 45.7% for academic models.

Conclusion: MTAM effectively identifies vulnerabilities in audio content moderation systems by generating adversarial test cases that bypass detection while maintaining harmful content, highlighting the need for more robust moderation tools.

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [19] [Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs](https://arxiv.org/abs/2509.24344)
*Theo Koraag,Niklas Wagner,Felix Dobslaw,Lucas Gren*

Main category: cs.SE

TL;DR: LLMs show strong potential for automating financial reporting tasks but face significant implementation challenges including prompt design, contextual dependency, and trade-offs between cloud-based models (better fluency) vs local models (better data control).


<details>
  <summary>Details</summary>
Motivation: Limited research exists on domain-specific LLM applications in Finance, despite LLMs' ability to automate complex natural language processing tasks across domains.

Method: Design Science Research methodology with exploratory case study, iteratively designing and evaluating two LLM-based systems: one with local open-source models in multi-agent workflow, another using commercial GPT-4o, assessed through expert evaluation of real-world financial reporting use cases.

Result: LLMs demonstrated strong potential for financial reporting automation but integration presented significant challenges. Cloud-based models offered superior fluency and usability but raised data privacy concerns, while local models provided better data control but required more engineering effort.

Conclusion: LLMs show strong potential for financial reporting automation, but successful integration requires careful attention to architecture, prompt design, and system reliability, with tailored validation mechanisms and engineering strategies balancing accuracy, control, and compliance.

Abstract: Context: Large Language Models (LLMs) enable automation of complex natural
language processing across domains, but research on domain-specific
applications like Finance remains limited. Objectives: This study explored
open-source and commercial LLMs for financial report analysis and commentary
generation, focusing on software engineering challenges in implementation.
Methods: Using Design Science Research methodology, an exploratory case study
iteratively designed and evaluated two LLM-based systems: one with local
open-source models in a multi-agent workflow, another using commercial GPT-4o.
Both were assessed through expert evaluation of real-world financial reporting
use cases. Results: LLMs demonstrated strong potential for automating financial
reporting tasks, but integration presented significant challenges. Iterative
development revealed issues including prompt design, contextual dependency, and
implementation trade-offs. Cloud-based models offered superior fluency and
usability but raised data privacy and external dependency concerns. Local
open-source models provided better data control and compliance but required
substantially more engineering effort for reliability and usability.
Conclusion: LLMs show strong potential for financial reporting automation, but
successful integration requires careful attention to architecture, prompt
design, and system reliability. Implementation success depends on addressing
domain-specific challenges through tailored validation mechanisms and
engineering strategies that balance accuracy, control, and compliance.

</details>


### [20] [Efficient Decomposition Identification of Deterministic Finite Automata from Examples](https://arxiv.org/abs/2509.24347)
*Junjie Meng,Jie An,Yong Li,Andrea Turrini,Fanjiang Xu,Naijun Zhan,Miaomiao Zhang*

Main category: cs.SE

TL;DR: This paper proposes a novel framework for DFA decomposition identification problems (DFA-DIPs) that replaces Augmented Prefix Tree Acceptors (APTAs) with 3-valued DFAs (3DFAs) to eliminate redundancies and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional DFA learning methods produce monolithic DFAs that lack simplicity and interoperability. While recent DFA-DIP approaches offer modularity through decomposition, they suffer from scalability limitations due to redundant SAT encodings derived from APTAs.

Method: The authors introduce a framework that bridges DFA decomposition with modern automata representation by replacing APTA with 3DFA derived directly from labeled examples. This compact representation reduces variables in SAT encoding and enables efficient solutions for both Pareto-optimal and states-optimal DIP variants.

Result: Experimental results show that the 3DFA-based approach achieves significant efficiency gains for Pareto-optimal DIP and enables scalable solutions for the states-optimal DIP variant.

Conclusion: The proposed 3DFA-based framework overcomes scalability limitations of existing DFA-DIP approaches by eliminating redundancies in automata representation, making DFA decomposition more practical for complex system modeling.

Abstract: The identification of deterministic finite automata (DFAs) from labeled
examples is a cornerstone of automata learning, yet traditional methods focus
on learning monolithic DFAs, which often yield a large DFA lacking simplicity
and interoperability. Recent work addresses these limitations by exploring DFA
decomposition identification problems (DFA-DIPs), which model system behavior
as intersections of multiple DFAs, offering modularity for complex tasks.
However, existing DFA-DIP approaches depend on SAT encodings derived from
Augmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due
to their inherent redundancy.
  In this work, we advance DFA-DIP research through studying two variants: the
traditional Pareto-optimal DIP and the novel states-optimal DIP, which
prioritizes a minimal number of states. We propose a novel framework that
bridges DFA decomposition with recent advancements in automata representation.
One of our key innovations replaces APTA with 3-valued DFA (3DFA) derived
directly from labeled examples. This compact representation eliminates
redundancies of APTA, thus drastically reducing variables in the improved SAT
encoding. Experimental results demonstrate that our 3DFA-based approach
achieves significant efficiency gains for the Pareto-optimal DIP while enabling
a scalable solution for the states-optimal DIP.

</details>


### [21] [Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?](https://arxiv.org/abs/2509.24352)
*Minghua He,Tong Jia,Chiming Duan,Pei Xiao,Lingzhe Zhang,Kangjin Wang,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: This paper proposes FaithLog, a faithful log-based anomaly detection system that addresses the black-box nature of existing deep learning methods by introducing diagnostic faithfulness as a trustworthiness metric.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based log analysis methods are black boxes that prevent service providers from understanding how anomalies are detected, hindering trust and deployment in production environments.

Method: FaithLog achieves faithfulness through a causality-guided attention mechanism and adversarial consistency learning, with evaluation tasks including attention-based root cause localization and event perturbation.

Result: Empirical studies show existing methods perform poorly in diagnostic faithfulness, while FaithLog achieves state-of-the-art performance in diagnostic faithfulness on two public datasets and one industrial dataset.

Conclusion: FaithLog successfully addresses the trustworthiness issue in log-based anomaly detection by providing diagnostic faithfulness, enabling better deployment in real production environments.

Abstract: Log-based software reliability maintenance systems are crucial for sustaining
stable customer experience. However, existing deep learning-based methods
represent a black box for service providers, making it impossible for providers
to understand how these methods detect anomalies, thereby hindering trust and
deployment in real production environments. To address this issue, this paper
defines a trustworthiness metric, diagnostic faithfulness, for models to gain
service providers' trust, based on surveys of SREs at a major cloud provider.
We design two evaluation tasks: attention-based root cause localization and
event perturbation. Empirical studies demonstrate that existing methods perform
poorly in diagnostic faithfulness. Consequently, we propose FaithLog, a
faithful log-based anomaly detection system, which achieves faithfulness
through a carefully designed causality-guided attention mechanism and
adversarial consistency learning. Evaluation results on two public datasets and
one industrial dataset demonstrate that the proposed method achieves
state-of-the-art performance in diagnostic faithfulness.

</details>


### [22] [United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning](https://arxiv.org/abs/2509.24364)
*Minghua He,Chiming Duan,Pei Xiao,Tong Jia,Siyu Yu,Lingzhe Zhang,Weijie Hong,Jin Han,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: Chimera is an end-to-end log-based fault diagnosis method that bridges anomaly detection and root cause localization through bidirectional interaction and knowledge transfer, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fault diagnosis methods are task-independent, creating gaps between anomaly detection and root cause localization that lead to diagnostic bias, expensive monitoring requirements, and overlooked task collaboration.

Method: Based on interactive multi-task learning with carefully designed interaction strategies at data, feature, and diagnostic result levels between anomaly detection and root cause localization within a unified end-to-end framework.

Result: Evaluation on two public and one industrial dataset shows Chimera outperforms existing methods with improvements of 2.92%-5.00% in anomaly detection and 19.01%-37.09% in root cause localization.

Conclusion: Chimera successfully achieves end-to-end fault diagnosis through bidirectional interaction between diagnostic tasks and has been deployed in production for an industrial cloud platform.

Abstract: Log-based fault diagnosis is essential for maintaining software system
availability. However, existing fault diagnosis methods are built using a
task-independent manner, which fails to bridge the gap between anomaly
detection and root cause localization in terms of data form and diagnostic
objectives, resulting in three major issues: 1) Diagnostic bias accumulates in
the system; 2) System deployment relies on expensive monitoring data; 3) The
collaborative relationship between diagnostic tasks is overlooked. Facing this
problems, we propose a novel end-to-end log-based fault diagnosis method,
Chimera, whose key idea is to achieve end-to-end fault diagnosis through
bidirectional interaction and knowledge transfer between anomaly detection and
root cause localization. Chimera is based on interactive multi-task learning,
carefully designing interaction strategies between anomaly detection and root
cause localization at the data, feature, and diagnostic result levels, thereby
achieving both sub-tasks interactively within a unified end-to-end framework.
Evaluation on two public datasets and one industrial dataset shows that Chimera
outperforms existing methods in both anomaly detection and root cause
localization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,
respectively. It has been successfully deployed in production, serving an
industrial cloud platform.

</details>


### [23] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: Agentic Service Computing (ASC) is a new paradigm that transforms services into intelligent, autonomous entities through a lifecycle framework of Design, Deployment, Operation, and Evolution, analyzed across four research dimensions.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-powered agents is transforming services computing from static request-response functions to dynamic, goal-oriented multi-agent ecosystems, requiring a new paradigm for intelligent services.

Method: The paper presents a lifecycle-driven framework for ASC with four phases and systematically analyzes it through four research dimensions: Perception/Context/Environment Modeling, Autonomous Decision-Making, Multi-Agent Collaboration, and Evaluation/Trustworthiness.

Result: The synthesis reveals that agentic services are orchestrated rather than assembled, with contextual awareness enabling robust deployment, autonomous reasoning supporting real-time operation, collaborative structures emerging through interaction, and trustworthiness as a lifelong imperative.

Conclusion: ASC integrates classical services computing principles with LLM-based multi-agent systems to establish a holistic foundation for developing adaptive, accountable, and human-centered intelligent services, providing a unified reference for researchers and practitioners.

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [24] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: TESTUPDATER is an LLM-based approach for automated unit test maintenance that handles both test repair and enhancement in response to code changes, using context analysis, step-by-step prompting, and iterative refinement to achieve high correctness.


<details>
  <summary>Details</summary>
Motivation: Manual unit test maintenance is inefficient and error-prone, while existing automated methods only focus on test repair and struggle with complex code changes due to rule-based context collection and lack of verification mechanisms.

Method: Uses LLM to analyze code changes and extract relevant context, then guides the LLM through step-by-step prompts to handle various code changes and dependencies for both test repair and enhancement, with error-type-aware iterative refinement to fix failures.

Result: Achieves 94.4% compilation pass rate and 86.7% test pass rate, outperforming SYNTER by 15.9% and 20.0% respectively, with 12.9% higher branch coverage and 15.2% greater line coverage.

Conclusion: TESTUPDATER effectively addresses the limitations of existing test maintenance methods by enabling automated just-in-time test updates for both repair and enhancement scenarios, significantly improving correctness and coverage compared to state-of-the-art approaches.

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [25] [Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development](https://arxiv.org/abs/2509.24485)
*Vlad Stirbu,Mateen Ahmed Abbasi,Teerath Das,Jesse Haimi,Niko Iljin,Pyry Kotilainen,Petrus Lipsanen,Niko Mäkitalo,Maiju Sipilä,Venla Veijalainen,Tommi Mikkonen*

Main category: cs.SE

TL;DR: Proposes a "shift-up" framework for GenAI native development to help software teams focus on high-value work while being supported by GenAI tools.


<details>
  <summary>Details</summary>
Motivation: Generative AI is transforming software engineering by replacing human developers with specialized AI agents, creating a need for frameworks that help teams adapt to this shift.

Method: Proposes a shift-up framework for GenAI native development and conducts a preliminary study testing these ideas with current GenAI tools.

Result: The paper presents initial findings from testing the shift-up framework with existing GenAI tools.

Conclusion: Proposes future research goals to study the shift-up framework in more detail and further explore GenAI's role in software engineering.

Abstract: Generative AI (GenAI) has significantly influenced software engineering.
Associated tools have created a shift in software engineering, where
specialized agents, based on user-provided prompts, are replacing human
developers. In this paper, we propose a framework for GenAI native development
that we call \textit{shift-up}, which helps software teams focus on high-value
work while being supported by GenAI. Furthermore, we also present a preliminary
study testing these ideas with current GenAI tools. Towards the end of the
paper, we propose future research goals to study shift-up in more detail.

</details>


### [26] [JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat](https://arxiv.org/abs/2509.24498)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Zelin Su,Qun Xia,Haochuan Lu,Ting Xiong,Man Ho Lam,Shuzheng Gao,Yuchong Xie,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.SE

TL;DR: JSProtect is a parallelized JavaScript obfuscation framework that addresses scalability issues in existing tools, processing 20MB codebases in minutes with minimal performance impact and only 20% code size increase.


<details>
  <summary>Details</summary>
Motivation: Current JavaScript obfuscation tools are inadequate for large-scale applications like WeChat mini-games, suffering from slow processing, severe performance degradation, and unsustainable code bloat when protecting against intellectual property theft.

Method: Uses Parallel-Aware Scope Analysis (PASA) algorithm for independent code partitioning across multiple cores and aggressive identifier reuse to minimize code size inflation.

Result: Processes 20MB codebases in minutes with 100% semantic equivalence, controls code size inflation to 20% (vs 1000%+ in baselines), preserves near-native performance, and provides superior security against static analysis and LLMs.

Conclusion: JSProtect establishes a new paradigm for industrial-scale JavaScript protection that effectively balances robust security with high performance and scalability.

Abstract: The WeChat mini-game ecosystem faces rampant intellectual property theft to
other platforms via secondary development, yet existing JavaScript obfuscation
tools are ill-equipped for large-scale applications, suffering from prohibitive
processing times, severe runtime performance degradation, and unsustainable
code size inflation. This paper introduces JSProtect, a high-throughput
parallelized obfuscation framework designed to overcome these fundamental
limitations. At the core of our framework is the Parallel-Aware Scope Analysis
(PASA) algorithm, which enables two key optimizations: independent code
partitioning for multi-core processing and independent namespace management
that aggressively reuses short identifiers to combat code bloat. Our evaluation
demonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining
100\% semantic equivalence while controlling code size inflation to as low as
20\% compared to over 1,000\% with baseline tools. Furthermore, it preserves
near-native runtime performance and provides superior security effectiveness
against both static analysis tools and large language models. This work
presents a new paradigm for industrial-scale JavaScript protection that
effectively balances robust security with high performance and scalability.

</details>


### [27] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: SemGuard is a framework that performs real-time semantic supervision during code generation to prevent semantic errors in LLM-generated code, outperforming existing methods across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs generate code with high rates of semantic errors (programs that compile but behave incorrectly), and existing repair approaches detect faults only after execution, causing latency and relying on incomplete test suites.

Method: SemGuard embeds a semantic evaluator trained on SemDiff dataset into the LLM's decoder to flag deviations in partial code, roll back to faulty lines, and guide regeneration without program execution or test cases.

Result: Across four benchmarks, SemGuard lowers semantic error rate by 19.86% on SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on LiveCodeBench with CodeLlama-7B, demonstrating model- and language-agnostic effectiveness.

Conclusion: SemGuard provides an effective solution for real-time semantic supervision during code generation, significantly reducing semantic errors without requiring program execution or test cases.

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [28] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: MSG is an automated specification generation tool for Move smart contracts that demonstrates LLMs can effectively handle non-mainstream languages, achieving 84% success rate and identifying overlooked clauses.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based specification tools focus on mainstream languages like C and Java, leaving verification-oriented languages like Move underexplored.

Method: MSG uses an agentic, modular design that explicitly leverages specification language features and incorporates feedback from the verification toolchain.

Result: MSG generates verifiable specifications for 84% of tested Move functions, identifies clauses overlooked by experts, and produces 57% more verifiable clauses than conventional designs. Verification toolchain feedback increases verifiable specifications by 30%.

Conclusion: LLMs exhibit robust capabilities for non-mainstream languages, and agentic modular designs with verification feedback significantly improve specification generation quality.

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [29] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: IFIM is an instruction-tuning method that enhances fill-in-the-middle code completion models by incorporating explicit instructions, improving instruction-following without compromising standard infilling performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with code completion when developer intent is underspecified, and existing methods force a trade-off between instruction-following and infilling capabilities.

Method: IFIM extends FIM training by adding an explicit instruction section, training on (prefix, instruction, suffix) triplets using a GPT-4o-generated dataset.

Result: IFIM boosts Pass@1 score from 84.6% to 93.6% on HumanEval-infilling while maintaining original FIM performance without instructions.

Conclusion: IFIM effectively enhances instruction-following in code completion models without sacrificing their core infilling capabilities.

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [30] [CoTune: Co-evolutionary Configuration Tuning](https://arxiv.org/abs/2509.24694)
*Gangda Xiong,Tao Chen*

Main category: cs.SE

TL;DR: CoTune is a configuration tuning tool that incorporates performance requirements through co-evolution, creating an auxiliary requirement to guide tuning while being robust to strict or misleading requirements.


<details>
  <summary>Details</summary>
Motivation: Existing tuners ignore complex performance requirements and assume better performance is always preferred, wasting valuable requirement information and consuming resources for minimal gains. Simply incorporating requirements as objectives can cause convergence issues.

Method: CoTune uses co-evolution by creating an auxiliary performance requirement that evolves alongside configurations, assisting the target requirement when it becomes ineffective or misleading.

Result: CoTune outperforms existing tuners in 90% of 162 test cases across nine systems and 18 requirements, achieving up to 2.9x overall improvements with better efficiency.

Conclusion: CoTune successfully addresses the challenge of incorporating performance requirements in configuration tuning through co-evolution, providing robust guidance while avoiding the pitfalls of strict requirements.

Abstract: To automatically tune configurations for the best possible system performance
(e.g., runtime or throughput), much work has been focused on designing
intelligent heuristics in a tuner. However, existing tuner designs have mostly
ignored the presence of complex performance requirements (e.g., the latency
shall ideally be 2 seconds), but simply assume that better performance is
always more preferred. This would not only waste valuable information in a
requirement but might also consume extensive resources to tune for a goal with
little gain. Yet, prior studies have shown that simply incorporating the
requirement as a tuning objective is problematic since the requirement might be
too strict, harming convergence; or its highly diverse satisfactions might lead
to premature convergence. In this paper, we propose CoTune, a tool that takes
the information of a given target performance requirement into account through
co-evolution. CoTune is unique in the sense that it creates an auxiliary
performance requirement to be co-evolved with the configurations, which assists
the target performance requirement when it becomes ineffective or even
misleading, hence allowing the tuning to be guided by the requirement while
being robust to its harm. Experiment results on 162 cases (nine systems and 18
requirements) reveal that CoTune considerably outperforms existing tuners,
ranking as the best for 90% cases (against the 0%--35% for other tuners) with
up to 2.9x overall improvements, while doing so under a much better efficiency.

</details>


### [31] [Large language models for behavioral modeling: A literature survey](https://arxiv.org/abs/2509.24782)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: This paper provides a systematic overview of using large language models (LLMs) for behavioral modeling, specifically for generating use case and sequence diagrams, identifying current research gaps and future directions.


<details>
  <summary>Details</summary>
Motivation: No comprehensive overview exists on LLM applications for behavioral modeling, making it difficult to identify research directions and assess LLM effectiveness for practitioners and educators in this domain.

Method: Conducted a term-based search to identify relevant primary studies, filtering down to 14 studies for systematic analysis of LLM applications in behavioral modeling.

Result: LLMs show promising results in automatically generating use case and sequence diagrams, but current literature lacks expert-based evaluations and primarily uses GPT-based models.

Conclusion: Future research should evaluate a broader range of LLMs for behavioral modeling and involve domain experts to properly assess the quality of LLM-generated outputs.

Abstract: In recent years, large language models (LLMs) have been extensively utilized
for behavioral modeling, for example, to automatically generate sequence
diagrams. However, no overview of this work has been published yet. Such an
overview will help identify future research directions and inform practitioners
and educators about the effectiveness of LLMs in assisting behavioral modeling.
This study aims to provide an overview of the existing research on the use of
LLMs for behavioral modeling, particularly focusing on use case and sequence
diagrams. Through a term-based search, we filtered and identified 14 relevant
primary studies. Our analysis of the selected primary studies reveals that LLMs
have demonstrated promising results in automatically generating use case and
sequence diagrams. In addition, we found that most of the current literature
lacks expert-based evaluations and has mainly used GPT-based models. Therefore,
future work should evaluate a broader range of LLMs for behavioral modeling and
involve domain experts to evaluate the output of LLMs.

</details>


### [32] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: SAP Joule achieves 80.49% accuracy on HumanEval-X JavaScript benchmark, ranking 5th among 30 models evaluated.


<details>
  <summary>Details</summary>
Motivation: To evaluate SAP Joule's JavaScript coding capabilities compared to other models, as this is the first comparative evaluation of SAP Joule's code generation capabilities.

Method: Used HumanEval-X JavaScript benchmark to compare SAP Joule against 29 other models for code generation performance.

Result: SAP Joule achieved 80.49% strict accuracy, ranking as the fifth best model in the evaluation.

Conclusion: SAP Joule demonstrates strong JavaScript code generation capabilities, performing competitively among 30 evaluated models.

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [33] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: DiffTester is an acceleration framework for diffusion LLMs in unit test generation that identifies repetitive structural patterns in test cases to enable parallel token generation without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs generate unit tests inefficiently one token at a time, while diffusion LLMs offer parallel generation but suffer from quality degradation when generating multiple tokens per step.

Method: Uses abstract syntax tree analysis to dynamically identify common structural patterns in unit tests targeting the same method, enabling adaptive parallel token generation without compromising quality.

Result: Extensive experiments on extended TestEval benchmark (Python, Java, C++) show significant acceleration while preserving test coverage, with good generalization across different dLLMs and programming languages.

Conclusion: DiffTester provides a practical and scalable solution for efficient unit test generation in software development by overcoming the efficiency-quality trade-off in diffusion LLMs.

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


### [34] [Large Language Models for Software Testing: A Research Roadmap](https://arxiv.org/abs/2509.25043)
*Cristian Augusto,Antonia Bertolino,Guglielmo De Angelis,Francesca Lonetti,Jesús Morán*

Main category: cs.SE

TL;DR: This paper provides a comprehensive roadmap and structured vision of LLM-based software testing research, identifying current progress, trends, and future directions through a semi-systematic literature review.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLM applications in software testing has created hundreds of new contributions monthly, making it difficult for researchers to stay current. No prior work has provided a structured overview of this emerging field.

Method: Conducted a semi-systematic literature review to collect articles, map them into prominent categories, analyze current status and ongoing work, and identify open challenges in LLM-based software testing.

Result: Created a roadmap illustrating the current state of LLM-based testing, grouped contributions into categories, identified most promising research directions, and outlined open challenges in the field.

Conclusion: LLMs represent a significant disruption in software testing, and the paper provides a structured vision of progress while outlining expected long-term impacts of LLMs on the entire software testing field.

Abstract: Large Language Models (LLMs) are starting to be profiled as one of the most
significant disruptions in the Software Testing field.
  Specifically, they have been successfully applied in software testing tasks
such as generating test code, or summarizing documentation.
  This potential has attracted hundreds of researchers, resulting in dozens of
new contributions every month, hardening researchers to
  stay at the forefront of the wave. Still, to the best of our knowledge, no
prior work has provided a structured vision of the progress
  and most relevant research trends in LLM-based testing. In this article, we
aim to provide a roadmap that illustrates its current state,
  grouping the contributions into different categories, and also sketching the
most promising and active research directions for the field.
  To achieve this objective, we have conducted a semi-systematic literature
review, collecting articles and mapping them into the most
  prominent categories, reviewing the current and ongoing status, and analyzing
the open challenges of LLM-based software testing.
  Lastly, we have outlined several expected long-term impacts of LLMs over the
whole software testing field.

</details>


### [35] [Towards Reliable Generation of Executable Workflows by Foundation Models](https://arxiv.org/abs/2509.25117)
*Sogol Masoumzadeh,Keheliya Gallaba,Dayi Lin,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: A framework using static analysis feedback to help Foundation Models detect and repair defects in DSL-based workflows they generate, improving reliability of automated workflow generation from natural language.


<details>
  <summary>Details</summary>
Motivation: Manual decomposition of tasks into workflows requires significant effort and domain knowledge. While FMs can generate DSL workflows, achieving accuracy and reliability remains challenging due to high defect prevalence (87.27% of instances contain defects).

Method: Developed Timon, a static analyzer for FM-generated DSL workflows, and Pumbaa, an FM-based repair tool. Uses static analysis feedback to guide FMs in detecting and repairing 9 types of workflow defects identified through a taxonomy of 18 defect types.

Result: The framework successfully identifies and repairs workflow defects through static analysis feedback, addressing the high prevalence of defects in FM-generated DSL workflows.

Conclusion: The work provides a crucial step towards reliable and automated generation of executable workflows from natural language requirements by systematically detecting and repairing defects in FM-generated DSL workflows.

Abstract: Recent advancements in Foundation Models (FMs) have demonstrated significant
progress in comprehending complex natural language to perform intricate tasks.
Successfully executing these tasks often requires orchestrating calls to FMs
alongside other software components. However, manually decomposing a task into
a coherent sequence of smaller, logically aggregated steps, commonly referred
to as workflows, demands considerable effort and specialized domain knowledge.
While FMs can assist in generating such workflows specified in domain-specific
languages (DSLs), achieving accuracy and reliability in this process remains a
challenge.
  This work introduces a framework that leverages static analysis feedback to
enable FMs to detect and repair defects in the DSL-based workflows they
generate. We begin by presenting the first-ever taxonomy of incidences of
defects in FM-generated DSL workflows, categorizing them into 18 distinct
types. Furthermore, we observe a high prevalence of defects across FM-generated
DSL workflows, with 87.27% of the studied instances containing at least one
defect. This, in turn, emphasizes the magnitude of the problem in practice and
underscores the necessity for implementing mitigation strategies. Following
this, we demonstrate that nine types of these defects can be effectively
identified through static analysis of the workflows. For this purpose, we
develop Timon, the first-of-its-kind static analyzer specifically designed for
FM-generated DSL workflows. Finally, we show that by incorporating feedback
from Timon, we can guide Pumbaa, an FM-based tool, to repair the detected
defect incidences. By systematically detecting and repairing defects, our work
provides a crucial step towards the reliable and automated generation of
executable workflows from natural language requirements.

</details>
