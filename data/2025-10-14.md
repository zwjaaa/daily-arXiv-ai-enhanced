<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 30]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: This survey provides the first holistic analysis of LLM-empowered software engineering, bridging the gap between evaluation benchmarks and solution approaches through a comprehensive taxonomy of 150+ papers.


<details>
  <summary>Details</summary>
Motivation: The field lacks comprehensive understanding of how benchmarks and solutions interconnect in LLM-empowered software engineering, hindering systematic progress and evaluation despite the paradigm shift from traditional rule-based systems to autonomous agentic systems.

Method: Analyzed 150+ recent papers and organized them into a taxonomy spanning two dimensions: Solutions (prompt-based, fine-tuning-based, agent-based paradigms) and Benchmarks (code generation, translation, repair, etc.). Presented a unified pipeline illustrating workflow from task specification to deliverables.

Result: Revealed evolution from simple prompt engineering to complex agentic systems with planning, reasoning, memory mechanisms, and tool augmentation. Provided full-spectrum coverage connecting 50+ benchmarks with solution strategies, enabling optimal approach identification for specific evaluation criteria.

Conclusion: Identified critical research gaps and proposed future directions including multi-agent collaboration, self-evolving code generation, and formal verification integration. Serves as foundational resource for understanding and advancing LLM-empowered software engineering systems.

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [2] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: InteractScience is a new benchmark for evaluating LLMs' ability to generate interactive scientific demonstrations by combining domain knowledge with front-end coding, using programmatic testing and visual verification.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks don't evaluate the combined capability of accurate scientific knowledge and interactive front-end code generation needed for educational demonstrations.

Method: Hybrid framework combining programmatic functional testing for interaction logic and visually-grounded qualitative testing against reference snapshots, applied across five scientific domains.

Result: Evaluation of 30 leading LLMs revealed ongoing weaknesses in integrating domain knowledge with interactive front-end coding.

Conclusion: InteractScience is the first benchmark to automatically measure this combined capability with realistic interactive operations, advancing reliable scientific demonstration code generation.

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [3] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: An LLM-based agent that autonomously analyzes Python code, infers properties, synthesizes property-based tests, identifies bugs, and generates actionable bug reports with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate software testing by combining LLMs with property-based testing to find bugs in real-world Python packages without manual test writing.

Method: The agent analyzes Python modules, infers function properties from code and docs, synthesizes PBTs, executes tests, reflects on outputs to confirm bugs, and generates bug reports.

Result: Tested on 100 popular Python packages: 56% of generated bug reports were valid, 32% were reportable to maintainers. Top 21 bugs had 86% validity and 81% reportability. 4 patches submitted with 3 successfully merged.

Conclusion: LLMs combined with property-based testing provide a rigorous and scalable method for autonomous software testing, capable of finding diverse bugs in real-world codebases.

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [4] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: OFP-Repair is a novel method for fixing floating-point program errors that can distinguish between errors repairable with original precision vs those requiring high precision, achieving significant accuracy improvements and successfully fixing real-world bugs.


<details>
  <summary>Details</summary>
Motivation: Floating-point errors in critical systems have severe consequences, but developers struggle to distinguish between errors fixable with original precision vs those requiring high precision. Existing tools either rely on time-consuming high-precision implementations or can only fix limited error types.

Method: The proposed OFP-Repair method addresses the challenge by providing a novel approach that doesn't require high-precision programs and can effectively distinguish between different types of floating-point errors.

Result: On ACESO's dataset, OFP-Repair achieved improvements of 3-8 orders of magnitude across four accuracy metrics. It successfully detected all five original-precision-repairable errors and fixed three, outperforming ACESO which only repaired one. Additionally, it repaired 5 out of 15 bugs in GNU Scientific Library.

Conclusion: OFP-Repair demonstrates strong practical applicability, with developers expressing interest in integrating the tool into their workflow. The method shows promise for effectively addressing floating-point errors without the overhead of high-precision implementations.

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [5] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: Analysis of 8,000 user reviews shows that 7 out of 9 MLOps practices significantly improve user satisfaction in AI development platforms, with benefits being universally perceived across organizations regardless of size.


<details>
  <summary>Details</summary>
Motivation: Limited empirical evidence exists on whether MLOps practices actually support users in developing and operationalizing AI applications, despite the emergence of MLOps as a solution to challenges in AI implementation.

Method: Analyzed over 8,000 user reviews from G2.com using zero-shot classification to measure sentiment toward nine established MLOps practices including CI/CD, workflow orchestration, reproducibility, versioning, collaboration, and monitoring.

Result: Seven of the nine MLOps practices showed significant positive relationship with user satisfaction. Small firms discuss certain MLOps practices less frequently, but firm size does not moderate the MLOps-satisfaction link.

Conclusion: Effective MLOps implementation provides tangible value to AI development, and once applied, MLOps practices are perceived as universally beneficial across different organizational settings.

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [6] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN is a deterministic framework that coordinates multiple LLM providers through text-based prompt orchestration to filter harmful AI-generated code suggestions before deployment, using a three-phase protocol of independent analysis, cross-critique, and arbitration.


<details>
  <summary>Details</summary>
Motivation: Addresses the problem of AI-assisted debugging increasingly producing modifications that introduce unnecessary complexity, break existing functionality, or address non-existent problems, requiring a reliable way to filter harmful AI-generated code suggestions.

Method: Three-phase protocol: independent analysis by multiple LLMs, cross-critique between providers, and arbitration to resolve disagreements. Uses simple .txt templates for prompt orchestration without requiring specialized infrastructure or deep technical knowledge.

Result: Evaluated on 15 software bugs with 69 AI-generated fix propositions. SLEAN accepted 31.9% of fixes while rejecting 47 harmful ones. Reduced code change surface by 83-90% compared to raw AI outputs. Minimal inputs were 20% more efficient than detailed inputs.

Conclusion: SLEAN provides a file-driven, provider-agnostic architecture that enables reliable multi-provider synthesis with end-to-end auditability, applicable to security auditing, code review, and document verification without requiring specialized coding expertise.

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [7] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: OBsmith is a framework that uses LLMs to systematically test JavaScript obfuscators for correctness bugs, uncovering 11 previously unknown issues that state-of-the-art fuzzers missed.


<details>
  <summary>Details</summary>
Motivation: JavaScript obfuscators are widely used for intellectual property protection, but their correctness has been overlooked compared to performance and resilience. Incorrect transformations can silently alter functionality, compromising reliability and security.

Method: OBsmith uses LLMs to generate program sketches (abstract templates) that capture diverse language constructs and corner cases, then instantiates them into executable programs for obfuscation testing. It also extracts sketches from real programs for focused testing.

Result: OBsmith uncovered 11 previously unknown correctness bugs in JavaScript obfuscators. Five state-of-the-art JavaScript fuzzers failed to detect these issues under equal program budgets, demonstrating OBsmith's complementary focus on obfuscation-induced misbehavior.

Conclusion: OBsmith represents an important step towards automated testing and quality assurance of obfuscators and other semantic-preserving toolchains, highlighting the need for obfuscator-specific metamorphic relations and balanced obfuscation presets.

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [8] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: MGDE is a novel method that uses mathematical guidance (Newton-Raphson method) to efficiently detect error-inducing inputs in floating-point programs, significantly outperforming state-of-the-art FPCC in bug detection rate and computational speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting floating-point program errors suffer from high computational costs due to reliance on high-precision programs and limited long-range convergence capability, making the search process inefficient like finding a needle in a haystack.

Method: Proposed MGDE method uses mathematical guidance through the Newton-Raphson method, which exhibits quadratic convergence properties, to achieve highly effective and efficient detection of error-inducing inputs.

Result: MGDE detected 89 bugs across 44 programs vs FPCC's 48 bugs across 29 programs, with 6.4x faster computation. For multi-input programs, MGDE found 9 bugs with 0.6443s average detection time, while FPCC found no bugs and took 100s per program.

Conclusion: MGDE significantly outperforms state-of-the-art methods in both bug detection capability and computational efficiency, demonstrating the effectiveness of mathematical guidance for floating-point error detection.

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [9] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: IntrinTrans is an LLM-based multi-agent system that automatically translates SIMD intrinsic code between architectures (e.g., Arm Neon to RISC-V Vector) using compile-and-test feedback and optimizes performance through register-usage analysis.


<details>
  <summary>Details</summary>
Motivation: Manual translation of vectorized intrinsic code across architectures is time-consuming and error-prone, while existing rule-based methods have limited success rates and poor performance due to incomplete rule coverage and inadequate use of target architecture features.

Method: Uses LLM-based multi-agent approach with compile-and-test feedback for automatic translation, combined with register-usage optimization from liveness analysis to better utilize RISC-V Vector specific features.

Result: Successfully translated 34 vectorized algorithms from Arm Neon to RISC-V Vector intrinsics, achieving semantic correctness in most cases within limited iterations, with some implementations reaching up to 5.93x performance improvement over community implementations.

Conclusion: IntrinTrans demonstrates that LLM-based approaches with feedback mechanisms can effectively automate cross-architecture intrinsic translation while achieving significant performance improvements over manual implementations.

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [10] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: LLMs can automatically generate working Proof-of-Concept exploits for web vulnerabilities using public CVE information, with success rates of 8%-34% using basic data and up to 68%-72% with adaptive reasoning strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can effectively leverage publicly disclosed CVE information to automatically generate valid PoC exploits for web application vulnerabilities, addressing a gap in previous research focused on zero-day exploitation.

Method: Empirical evaluation of GPT-4o and DeepSeek-R1 on 100 real-world CVEs across three disclosure stages: newly disclosed (descriptions only), 1-day (with patches), and N-day (with full code context), using different levels of code context and adaptive reasoning strategies.

Result: LLMs generated working PoCs in 8%-34% of cases using public data alone, with DeepSeek-R1 outperforming GPT-4o. Code context improved success by 17%-20%, and adaptive reasoning strategies boosted success rates to 68%-72%. 23 generated PoCs were accepted by NVD and Exploit DB.

Conclusion: LLMs can effectively generate valid PoC exploits using public vulnerability information, potentially reshaping vulnerability exploitation dynamics and demonstrating practical feasibility for automated exploit generation.

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [11] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: MOJOFuzzer is the first adaptive LLM-based fuzzing framework for emerging programming languages like MOJO, addressing model hallucination through multi-phase filtering and dynamic prompt adaptation, significantly improving test validity and bug detection.


<details>
  <summary>Details</summary>
Motivation: MOJO language lacks comprehensive testing frameworks and sufficient corpus for LLM-based testing, leading to model hallucination where LLMs generate syntactically valid but semantically incorrect code, reducing fuzz testing effectiveness.

Method: MOJOFuzzer integrates a multi-phase framework that eliminates low-quality generated inputs before execution and dynamically adapts LLM prompts based on runtime feedback for test case mutation, enabling iterative learning.

Result: MOJOFuzzer significantly enhances test validity, API coverage, and bug detection performance, outperforming traditional fuzz testing and state-of-the-art LLM-based approaches. It uncovered 13 previously unknown bugs in MOJO.

Conclusion: The study advances LLM-driven software testing and establishes a foundational methodology for leveraging LLMs in testing emerging programming languages, demonstrating MOJOFuzzer's effectiveness in zero-shot learning environments.

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [12] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: A production system for automated code review that combines static analysis with AST-guided context extraction and efficient LLM serving to provide grounded, concise explanations and remediation guidance for compliance-heavy C/C++ code.


<details>
  <summary>Details</summary>
Motivation: Automated code review adoption lags in compliance-heavy settings due to high-volume static analyzer outputs with low rationale and risks of LLM hallucination and cost overhead.

Method: Pairs static-analysis findings with AST-guided context extraction and uses a single-GPU on-demand serving stack with quantized open-weight model and multi-tier caching for efficient, grounded code review.

Result: Achieves sub-minute median first-feedback (59.8s offline p50 build+LLM) while maintaining competitive violation reduction and lower violation rates versus larger proprietary models.

Conclusion: The decoupled architecture allows independent adoption of grounding/prompting or serving layers, with survey results showing reduced triage effort and fewer human review iterations, emphasizing reproducibility and auditability.

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [13] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: Drift-based retraining of capacity forecasting models achieves comparable accuracy to periodic retraining in most cases, offering a cost-effective alternative while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Continuous retraining of ML forecasting models for capacity management is expensive and difficult to scale, creating a need for more efficient retraining strategies that balance accuracy and computational costs.

Method: Investigating the effects of retraining capacity forecasting models based on detected data changes (drift-based) compared to periodic retraining for time series data.

Result: Drift-based retraining achieves comparable forecasting accuracy to periodic retraining in most cases, making it cost-effective. However, for rapidly changing data, periodic retraining still provides better accuracy.

Conclusion: Drift-based retraining offers a viable cost-effective strategy for capacity forecasting systems, reducing retraining overhead while maintaining robust performance, though periodic retraining remains preferable for rapidly evolving data scenarios.

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [14] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: A hybrid framework combining heterogeneous program graphs with lightweight local LLMs achieves 93.57% accuracy in Java vulnerability detection, outperforming Graph Attention Networks and pretrained LLMs while providing interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Current static and dynamic analyses often miss structural dependencies that lead to insecure behaviors, and existing approaches face cost and privacy concerns with large cloud models.

Method: View programs as heterogeneous graphs capturing control- and data-flow relations, then combine these graph representations with lightweight (<4B) local LLMs to unite topological features with semantic reasoning.

Result: Achieves 93.57% accuracy in Java vulnerability detection (binary classification), representing an 8.36% improvement over Graph Attention Network-based embeddings and 17.81% over pretrained LLM baselines like Qwen2.5 Coder 3B.

Conclusion: The approach enables scalable, explainable, and locally deployable vulnerability analysis tools that shift from purely syntactic checks to deeper structural and semantic insights, facilitating broader adoption in secure software development.

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [15] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: This paper presents the first comprehensive study on the robustness of multi-agent systems (MASs) for code generation, revealing significant vulnerabilities through fuzzing-based testing and proposing effective mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Despite the promising performance of MASs in automated code generation, their robustness remains largely unexplored, raising critical concerns for real-world deployment and reliability.

Method: The study uses a fuzzing-based testing approach with semantic-preserving mutation operators and a novel fitness function to assess mainstream MASs across multiple datasets and LLMs.

Result: The research reveals substantial robustness flaws in popular MASs, showing they fail to solve 7.9%-83.3% of problems they initially resolved successfully after semantic-preserving mutations. The main cause is miscommunication between planning and coding agents.

Conclusion: The paper uncovers critical robustness flaws in MASs for code generation and provides effective mitigation strategies through a repairing method that enhances robustness by solving 40.0%-88.9% of identified failures, contributing essential insights for developing more reliable MASs.

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [16] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: This study examines how novice developers use generative AI for unit testing, identifying four interaction strategies and prompting styles, with benefits like time-saving but drawbacks including trust issues and quality concerns.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI tools like ChatGPT affect novice developers' workflows in unit testing, particularly regarding control, output quality, and learning outcomes.

Method: Observational study with 12 undergraduate students working on unit testing tasks using generative AI, analyzing interaction strategies and prompting styles.

Result: Identified four interaction strategies based on test idea/implementation origin, and two prompting styles (one-shot vs iterative). Benefits included time-saving and reduced cognitive load, while drawbacks involved trust issues and quality concerns. Strategy and prompting styles didn't significantly affect test effectiveness or code quality.

Conclusion: Generative AI can assist novice developers in unit testing but raises concerns about trust, quality, and ownership that need addressing in AI-assisted workflows.

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [17] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: This paper examines how generative AI and large language models are transforming software engineering practices, including new development styles like chat-oriented programming and agentic programming, while addressing challenges around trust, reliability, and skills.


<details>
  <summary>Details</summary>
Motivation: To understand how AI-assisted techniques are changing software engineering practice and address related issues of trust, accountability, and shifting skill requirements in the era of generative AI.

Method: The paper surveys various AI-assisted development approaches including iterative chat-based development, multi-agent systems, dynamic prompt orchestration, and integration via Model Context Protocol (MCP), using case studies and industry data.

Result: The research outlines opportunities such as faster development cycles and democratized coding, while identifying challenges including model reliability and cost considerations in applying generative AI to coding.

Conclusion: The paper describes new roles, skills, and best practices needed for using AI in software engineering in a responsible and effective way, emphasizing the evolving nature of development practices with generative AI.

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [18] [Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration](https://arxiv.org/abs/2510.10824)
*Mohanakrishnan Hariharan,Satish Arvapalli,Seshu Barma,Evangeline Sheela*

Main category: cs.SE

TL;DR: Automated software testing using Agentic RAG systems that combine AI agents with hybrid vector-graph knowledge to generate test plans, cases, and metrics, achieving 94.8% accuracy and significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional software testing by automating quality engineering artifact creation and improving testing efficiency through AI-powered systems.

Method: Combines autonomous AI agents with hybrid vector-graph knowledge systems, leveraging LLMs (Gemini and Mistral), multi-agent orchestration, and enhanced contextualization for test automation.

Result: Achieved 94.8% accuracy (from 65%), 85% reduction in testing timeline, 85% improvement in test suite efficiency, 35% cost savings, and 2-month acceleration of go-live in enterprise projects.

Conclusion: Agentic RAG systems successfully automate software testing with dramatic improvements in accuracy, efficiency, and cost-effectiveness while maintaining comprehensive document traceability.

Abstract: We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.

</details>


### [19] [Software Defect Prediction using Autoencoder Transformer Model](https://arxiv.org/abs/2510.10840)
*Seshu Barma,Mohanakrishnan Hariharan,Satish Arvapalli*

Main category: cs.SE

TL;DR: The paper proposes ADE-QVAET, an AI-ML model combining Adaptive Differential Evolution with Quantum Variational Autoencoder-Transformer for enhanced software defect prediction, achieving over 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing ML models struggle with noisy data, imbalances, pattern recognition, feature extraction, and generalization in software quality assessment.

Method: Developed ADE-QVAET model that combines Adaptive Differential Evolution optimization with Quantum Variational Autoencoder-Transformer to extract high-dimensional latent features while maintaining sequential dependencies.

Result: Achieved 98.08% accuracy, 92.45% precision, 94.67% recall, and 98.12% F1-score with 90% training data, outperforming Differential Evolution ML model.

Conclusion: ADE-QVAET represents an effective AI-ML-driven technology for quality engineering, providing scalable and accurate software defect prediction through optimized hyperparameter tuning.

Abstract: An AI-ML-powered quality engineering approach uses AI-ML to enhance software
quality assessments by predicting defects. Existing ML models struggle with
noisy data types, imbalances, pattern recognition, feature extraction, and
generalization. To address these challenges, we develop a new model, Adaptive
Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer
(QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional
latent features and maintain sequential dependencies, resulting in enhanced
defect prediction accuracy. ADE optimization enhances model convergence and
predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning
hyperparameters for scalable and accurate software defect prediction,
representing an AI-ML-driven technology for quality engineering. During
training with a 90% training percentage, ADE-QVAET achieves high accuracy,
precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%,
respectively, when compared to the Differential Evolution (DE) ML model.

</details>


### [20] [Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature](https://arxiv.org/abs/2510.10887)
*Lakshana Iruni Assalaarachchi,Zainab Masood,Rashina Hoda,John Grundy*

Main category: cs.SE

TL;DR: Software project managers view GenAI as an assistant/copilot rather than replacement, with benefits in task automation, analytics, communication, and agile practices, but emphasize responsible usage due to concerns like hallucinations and ethics.


<details>
  <summary>Details</summary>
Motivation: To understand how software practitioners are discussing and implementing GenAI transformations in software project management through publicly available practitioner sources.

Method: Grey literature review using 47 publicly available practitioner sources including blogs, articles, and industry reports.

Result: Software project managers primarily perceive GenAI as an assistant/copilot/friend rather than replacement, with support in automating routine tasks, predictive analytics, communication/collaboration, and agile practices. Concerns include hallucinations, ethics/privacy, and lack of emotional intelligence.

Conclusion: Upskilling requirements for software project managers in GenAI era are presented, mapped to PMI's talent triangle, with key recommendations for practitioners and researchers.

Abstract: Software practitioners are discussing GenAI transformations in software
project management openly and widely. To understand the state of affairs, we
performed a grey literature review using 47 publicly available practitioner
sources including blogs, articles, and industry reports. We found that software
project managers primarily perceive GenAI as an "assistant", "copilot", or
"friend" rather than as a "PM replacement", with support of GenAI in automating
routine tasks, predictive analytics, communication and collaboration, and in
agile practices leading to project success. Practitioners emphasize responsible
GenAI usage given concerns such as hallucinations, ethics and privacy, and lack
of emotional intelligence and human judgment. We present upskilling
requirements for software project managers in the GenAI era mapped to the
Project Management Institute's talent triangle. We share key recommendations
for both practitioners and researchers.

</details>


### [21] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: A novel C-Rust Pointer Knowledge Graph approach that enables project-level C-to-Rust translation by providing LLMs with global pointer semantics, significantly reducing unsafe code and improving functional correctness.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based C-to-Rust translation methods struggle with project-level translation due to lack of global perspective on pointer usage, leading to unsafe Rust code generation.

Method: Proposes C-Rust Pointer Knowledge Graph that enriches code-dependency graph with pointer-usage information (points-to flows, struct mapping) and Rust-oriented annotations (ownership, mutability, nullability, lifetime), then synthesizes this KG with LLMs for translation.

Result: Reduces unsafe usages in translated Rust by 99.9% compared to rule-based and traditional LLM methods, and achieves 29.3% higher functional correctness than fuzzing-enhanced LLM methods.

Conclusion: The proposed KG-guided approach effectively addresses pointer translation challenges in project-level C-to-Rust conversion, enabling generation of safer and more correct Rust code.

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [22] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: RepoSummary is a feature-oriented code repository summarization approach that automatically generates documentation and establishes accurate traceability links from functional features to code elements, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing repository summarization techniques focus on directory tree structure, which is insufficient for tracing high-level features to the collaborative methods that implement them, limiting code comprehension and maintenance.

Method: Proposed RepoSummary, a feature-oriented approach that simultaneously generates repository documentation and establishes traceability links from functional features to corresponding code elements.

Result: RepoSummary increases feature coverage from 61.2% to 71.1%, improves file-level traceability recall from 29.9% to 53.0%, and generates more conceptually consistent, understandable, and better-formatted documentation than state-of-the-art baseline HGEN.

Conclusion: RepoSummary effectively addresses limitations of existing repository summarization by providing feature-oriented documentation with improved traceability links, enhancing code comprehension and maintenance capabilities.

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [23] [Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs](https://arxiv.org/abs/2510.11059)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Jiongchi Yu,Jiaolong Klong,Yi Li*

Main category: cs.SE

TL;DR: Defects4C is introduced as a comprehensive benchmark for C/C++ program repair, addressing the gap in research compared to Java-based APR. It includes 9M bug-relevant commits, 248 buggy functions, and 102 vulnerable functions with test cases.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap in C/C++ program repair research despite widespread use and vulnerability prevalence, primarily due to lack of high-quality benchmarks like Defects4J for Java.

Method: Constructed Defects4C from real-world C/C++ repositories with bug-relevant commits, buggy functions, and vulnerable functions paired with test cases. Used this benchmark to evaluate 24 state-of-the-art LLMs for C/C++ fault repair.

Result: The empirical study provides insights into the strengths and limitations of current LLM-based APR techniques for C/C++, highlighting both the effectiveness and areas needing improvement.

Conclusion: Defects4C fills a critical gap in C/C++ program repair research and enables rigorous evaluation of repair techniques, supporting future advancements in this domain.

Abstract: Automated Program Repair (APR) plays a critical role in enhancing the quality
and reliability of software systems. While substantial progress has been made
in Java-based APR, largely facilitated by benchmarks like Defects4J, there
remains a significant gap in research on C/C++ program repair, despite the
widespread use of C/C++ and the prevalence of associated vulnerabilities. This
gap is primarily due to the lack of high-quality, open-source benchmarks
tailored for C/C++.
  To address this issue, we introduce Defects4C, a comprehensive and executable
benchmark specifically designed for C/C++ program repair. Our dataset is
constructed from real-world C/C++ repositories and includes a large collection
of bug-relevant commits (9M in total), 248 high-quality buggy functions, and
102 vulnerable functions, all paired with test cases for reproduction. These
resources enable rigorous evaluation of repair techniques and support the
retraining of learning-based approaches for enhanced performance.
  Using Defects4C, we conduct a comprehensive empirical study evaluating the
effectiveness of 24 state-of-the-art large language models (LLMs) in repairing
C/C++ faults. Our findings offer valuable insights into the strengths and
limitations of current LLM-based APR techniques in this domain, highlighting
both the need for more robust methods and the critical role of Defects4C in
advancing future research

</details>


### [24] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: DebugTA is an LLM-based agent that uses specialized tools for code debugging and teaching, addressing challenges in programming education by decomposing complex tasks and leveraging standard code through retrieval, variable substitution, and real-time compilation.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with the complexity and heterogeneity of inputs in debugging and teaching tasks, and fail to fully utilize available standard code, limiting LLMs' effectiveness in helping students correct erroneous code.

Method: DebugTA uses an agent-based approach with specialized tools including standard code retrieval, variable substitution to align reference code, and an external compiler for real-time analysis. It decomposes tasks into sequential LLM interactions guided by pedagogical principles.

Result: Experimental results on three real-world code datasets show that DebugTA consistently improves teaching effectiveness while significantly reducing computational costs compared to existing approaches.

Conclusion: DebugTA effectively addresses the challenges in debugging and teaching tasks by simplifying reasoning complexity through tool-based decomposition and better utilization of standard code, leading to improved accuracy and efficiency in generating modification suggestions.

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [25] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: This paper presents the first large-scale analysis of FMware (Foundation Model-based software) development across cloud platforms and open-source repositories, examining application domains, developer challenges, and effort-intensive issues.


<details>
  <summary>Details</summary>
Motivation: Foundation Models are transforming software engineering by enabling FMware development, but this new paradigm presents significant challenges that differ from traditional software development across cloud and on-premise platforms.

Method: Empirical investigation using data from GitHub repositories and leading FMware platforms (HuggingFace, GPTStore, Ora, Poe) to analyze three focus areas: common application domains, key developer challenges, and types of issues requiring most effort.

Result: Findings show strong focus on education, content creation, and business strategy domains. Key technical challenges include memory management, dependency handling, and tokenizer configuration. Bug reports and core functionality issues are most frequent on GitHub, while code review, similarity search, and prompt template design are most time-consuming.

Conclusion: The study uncovers developer practices and pain points to improve FMware tools, workflows, and community support, providing actionable insights for the future of FMware development.

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [26] [Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop](https://arxiv.org/abs/2510.11179)
*David Georg Reichelt,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper presents a method to transform OpenTelemetry tracing data into the Kieker observability framework, enabling analysis of languages like C# and JavaScript that Kieker doesn't natively support.


<details>
  <summary>Details</summary>
Motivation: Kieker has limited language support (Java, C, Fortran, Python) while OpenTelemetry supports many more languages including C# and JavaScript. There's a need to bridge these frameworks to leverage Kieker's analysis capabilities with OpenTelemetry's broader language coverage.

Method: The authors developed a transformation approach to convert OpenTelemetry tracing data into Kieker's format, enabling the creation of call trees and other analysis features from OpenTelemetry instrumentations.

Result: The approach was successfully demonstrated using the Astronomy Shop OpenTelemetry demo application, showing that trace data can be visualized and analyzed through Kieker after transformation.

Conclusion: The integration allows Kieker to analyze applications instrumented with OpenTelemetry, extending Kieker's capabilities to languages it doesn't natively support while maintaining its powerful analysis features.

Abstract: The observability framework Kieker provides a range of analysis capabilities,
but it is currently only able to instrument a smaller selection of languages
and technologies, including Java, C, Fortran, and Python. The OpenTelemetry
standard aims for providing reference implementations for most programming
languages, including C# and JavaScript, that are currently not supported by
Kieker. In this work, we describe how to transform OpenTelemetry tracing data
into the Kieker framework. Thereby, it becomes possible to create for example
call trees from OpenTelemetry instrumentations. We demonstrate the usability of
our approach by visualizing trace data of the Astronomy Shop, which is an
OpenTelemetry demo application.

</details>


### [27] [Detection of Performance Changes in MooBench Results Using Nyrkiö on GitHub Actions](https://arxiv.org/abs/2510.11310)
*Shinhyung Yang,David Georg Reichelt,Henrik Ingo,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: The paper integrates Nyrkiö change detection into MooBench to identify performance changes in GitHub projects, successfully detecting a major regression caused by a Linux Kernel version update.


<details>
  <summary>Details</summary>
Motivation: Performance changes in GitHub's 518 million projects are critical for users, but detecting these changes is challenging despite GitHub CI/CD supporting performance measurement.

Method: Incorporated Nyrkiö change detection service into MooBench, which previously only measured tracing agent overhead without change detection. Added upload of measurements to Nyrkiö for change detection capability.

Result: Successfully identified one major performance regression. The regression was reproducible with GitHub actions and was caused by a Linux Kernel version change.

Conclusion: The integration of Nyrkiö with MooBench enables effective detection of performance changes in GitHub projects, demonstrating practical value in identifying performance regressions caused by system-level changes.

Abstract: In GitHub with its 518 million hosted projects, performance changes within
these projects are highly relevant to the project's users. Although performance
measurement is supported by GitHub CI/CD, performance change detection is a
challenging topic.
  In this paper, we demonstrate how we incorporated Nyrki\"o to MooBench. Prior
to this work, Moobench continuously ran on GitHub virtual machines, measuring
overhead of tracing agents, but without change detection. By adding the upload
of the measurements to the Nyrki\"o change detection service, we made it
possible to detect performance changes. We identified one major performance
regression and examined the performance change in depth. We report that (1) it
is reproducible with GitHub actions, and (2) the performance regression is
caused by a Linux Kernel version change.

</details>


### [28] [Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks](https://arxiv.org/abs/2510.11516)
*Jeena Javahar,Tanya Budhrani,Manaal Basha,Cleidson R. B. de Souza,Ivan Beschastnikh,Gema Rodriguez-Perez*

Main category: cs.SE

TL;DR: This study investigates how developers use Amazon's CodeWhisperer, identifying four key behavioral patterns in AI code-generation tool adoption.


<details>
  <summary>Details</summary>
Motivation: Understanding how software developers are adopting AI code-generation tools as their use becomes increasingly common.

Method: Conducted two user studies with 10 participants each, using mixed-methods analysis including custom telemetry plugin for low-level interaction data collection.

Result: Identified four behavioral patterns: incremental code refinement, explicit instruction using natural language comments, baseline structuring with model suggestions, and integrative use with external sources.

Conclusion: Provides comprehensive analysis of developer interaction patterns with CodeWhisperer, offering insights into AI code-generation tool adoption behaviors.

Abstract: The use of AI code-generation tools is becoming increasingly common, making
it important to understand how software developers are adopting these tools. In
this study, we investigate how developers engage with Amazon's CodeWhisperer,
an LLM-based code-generation tool. We conducted two user studies with two
groups of 10 participants each, interacting with CodeWhisperer - the first to
understand which interactions were critical to capture and the second to
collect low-level interaction data using a custom telemetry plugin. Our
mixed-methods analysis identified four behavioral patterns: 1) incremental code
refinement, 2) explicit instruction using natural language comments, 3)
baseline structuring with model suggestions, and 4) integrative use with
external sources. We provide a comprehensive analysis of these patterns .

</details>


### [29] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: CodeWatcher is a lightweight system that captures fine-grained developer interactions with code generation tools in VS Code without disrupting workflow.


<details>
  <summary>Details</summary>
Motivation: Understanding developer interactions with code generation tools requires real-time data that's difficult to collect without disrupting workflow.

Method: A client-server system with VS Code plugin, Python RESTful API, and MongoDB backend that logs semantically meaningful events like insertions, deletions, copy-paste actions, and focus shifts.

Result: Enables continuous monitoring of developer activity, post-hoc reconstruction of coding sessions, and rich behavioral analyses of CGT usage.

Conclusion: This infrastructure supports research on responsible AI, developer productivity, and human-centered evaluation of code generation tools.

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


### [30] [Automatically Generating Questions About Scratch Programs](https://arxiv.org/abs/2510.11658)
*Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: The paper presents an automated system for generating program comprehension questions for Scratch code, extending the LitterBox tool to create 30 different question types based on an established comprehension model.


<details>
  <summary>Details</summary>
Motivation: Current assessment methods in programming education focus on code completion but don't guarantee actual understanding of concepts. Manual question creation for individual student programs is tedious and challenging.

Method: Extended the LitterBox static analysis tool to automatically generate 30 different question types for Scratch programs, covering an established program comprehension model. Tested on 600,913 projects generating over 54 million questions.

Result: Successfully generated 54,118,694 questions automatically. Initial experiments with 34 ninth graders showed the approach generates meaningful questions, and students' ability to answer them correlates with their overall performance.

Conclusion: Automated question generation for Scratch programs is feasible and effective for assessing program comprehension, with student performance on these questions relating to their overall programming ability.

Abstract: When learning to program, students are usually assessed based on the code
they wrote. However, the mere completion of a programming task does not
guarantee actual comprehension of the underlying concepts. Asking learners
questions about the code they wrote has therefore been proposed as a means to
assess program comprehension. As creating targeted questions for individual
student programs can be tedious and challenging, prior work has proposed to
generate such questions automatically. In this paper we generalize this idea to
the block-based programming language Scratch. We propose a set of 30 different
questions for Scratch code covering an established program comprehension model,
and extend the LitterBox static analysis tool to automatically generate
corresponding questions for a given Scratch program. On a dataset of 600,913
projects we generated 54,118,694 questions automatically. Our initial
experiments with 34 ninth graders demonstrate that this approach can indeed
generate meaningful questions for Scratch programs, and we find that the
ability of students to answer these questions on their programs relates to
their overall performance.

</details>
