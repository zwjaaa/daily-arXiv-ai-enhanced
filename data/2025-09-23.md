<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Digging Into the Internal: Causality-Based Analysis of LLM Function Calling](https://arxiv.org/abs/2509.16268)
*Zhenlan Ji,Daoyuan Wu,Wenxuan Wang,Pingchuan Ma,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: Function calling (FC) significantly enhances LLM compliance with user instructions and improves safety robustness by 135% over conventional prompting methods.


<details>
  <summary>Details</summary>
Motivation: The mechanisms through which function calling influences LLM behavior remain largely unexplored, and FC shows potential to substantially enhance LLM compliance with user instructions.

Method: Used layer-level and token-level causal interventions to dissect FC's impact on model's internal computational logic, and conducted extensive experiments comparing FC-based instructions against conventional prompting methods on LLM safety robustness.

Result: FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs across four mainstream LLMs and two benchmark datasets.

Conclusion: FC demonstrates promising potential to enhance LLM reliability and capability in practical applications, particularly for safety-critical scenarios.

Abstract: Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC's impact on the model's internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.

</details>


### [2] [Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach](https://arxiv.org/abs/2509.16478)
*Hossein Yousefizadeh,Shenghui Gu,Lionel C. Briand,Ali Nasr*

Main category: cs.SE

TL;DR: CoCoMagic is an automated test case generation method that combines metamorphic testing, differential testing, and search-based techniques to identify behavioral divergences between versions of autonomous systems through constrained cooperative co-evolutionary search.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems evolve rapidly through frequent updates, risking unintended behavioral degradations. System-level testing is challenging due to vast scenario space, absence of reliable test oracles, and need for practically applicable and interpretable test cases.

Method: CoCoMagic formulates test generation as constrained cooperative co-evolutionary search, evolving both source scenarios and metamorphic perturbations to maximize differences in violations of predefined metamorphic relations across versions. It includes constraints and population initialization strategies for realistic scenarios, plus integrated interpretability for root cause diagnosis.

Result: Evaluation on InterFuser ADS in Carla simulator shows CoCoMagic identifies up to 287% more distinct high-severity behavioral differences than baseline methods while maintaining scenario realism. The interpretability approach provides actionable insights for developers.

Conclusion: CoCoMagic offers an efficient, effective, and interpretable way for differential testing of evolving autonomous systems across versions, supporting targeted debugging and safety assessment.

Abstract: Autonomous systems, such as autonomous driving systems, evolve rapidly
through frequent updates, risking unintended behavioral degradations. Effective
system-level testing is challenging due to the vast scenario space, the absence
of reliable test oracles, and the need for practically applicable and
interpretable test cases. We present CoCoMagic, a novel automated test case
generation method that combines metamorphic testing, differential testing, and
advanced search-based techniques to identify behavioral divergences between
versions of autonomous systems. CoCoMagic formulates test generation as a
constrained cooperative co-evolutionary search, evolving both source scenarios
and metamorphic perturbations to maximize differences in violations of
predefined metamorphic relations across versions. Constraints and population
initialization strategies guide the search toward realistic, relevant
scenarios. An integrated interpretability approach aids in diagnosing the root
causes of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,
within the Carla virtual simulator. Results show significant improvements over
baseline search methods, identifying up to 287\% more distinct high-severity
behavioral differences while maintaining scenario realism. The interpretability
approach provides actionable insights for developers, supporting targeted
debugging and safety assessment. CoCoMagic offers an efficient, effective, and
interpretable way for the differential testing of evolving autonomous systems
across versions.

</details>


### [3] [Causal Fuzzing for Verifying Machine Unlearning](https://arxiv.org/abs/2509.16525)
*Anna Mazhar,Sainyam Galhotra*

Main category: cs.SE

TL;DR: CAFÉ is a causality-based framework for verifying machine unlearning in black-box ML models, addressing limitations of existing methods by evaluating both direct and indirect effects through causal dependencies.


<details>
  <summary>Details</summary>
Motivation: Machine learning models need "unlearning" capabilities for adaptability, fairness, and privacy, but existing verification methods provide limited insights and fail in scenarios with indirect influence.

Method: CAFÉ uses causal dependencies to evaluate both direct and indirect effects of unlearning targets (datapoints and features) in black-box ML models, providing fine-grained analysis.

Result: Evaluation across five datasets and three model architectures shows CAFÉ successfully detects residual influence missed by baselines while maintaining computational efficiency.

Conclusion: CAFÉ provides a unified causality-based framework for comprehensive verification of machine unlearning, offering actionable insights that overcome limitations of existing methods.

Abstract: As machine learning models become increasingly embedded in decision-making
systems, the ability to "unlearn" targeted data or features is crucial for
enhancing model adaptability, fairness, and privacy in models which involves
expensive training. To effectively guide machine unlearning, a thorough testing
is essential. Existing methods for verification of machine unlearning provide
limited insights, often failing in scenarios where the influence is indirect.
In this work, we propose CAF\'E, a new causality based framework that unifies
datapoint- and feature-level unlearning for verification of black-box ML
models. CAF\'E evaluates both direct and indirect effects of unlearning targets
through causal dependencies, providing actionable insights with fine-grained
analysis. Our evaluation across five datasets and three model architectures
demonstrates that CAF\'E successfully detects residual influence missed by
baselines while maintaining computational efficiency.

</details>


### [4] [Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing](https://arxiv.org/abs/2509.16595)
*Jiaming Ye,Xiongfei Wu,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: This paper analyzes limitations of measurement-based validation methods in quantum program testing and compares them with statevector-based validation approaches.


<details>
  <summary>Details</summary>
Motivation: Quantum program quality assurance is critical as quantum computing emerges, but existing measurement-based validation methods face significant limitations due to the probabilistic nature of quantum programs.

Method: Conducted an empirical study of recent quantum program testing research, categorizing measurement-based validation into distribution-level and output-value-level validation, and comparing them with statevector-based validation.

Result: Measurement-based validation is suitable for straightforward assessments like verifying specific output values, while statevector-based validation is more effective for complex tasks like assessing program behaviors.

Conclusion: The study reveals the limitations of measurement-based validation and demonstrates the advantages of statevector-based validation for comprehensive quantum program testing.

Abstract: As quantum computing continues to emerge, ensuring the quality of quantum
programs has become increasingly critical. Quantum program testing has emerged
as a prominent research area within the scope of quantum software engineering.
While numerous approaches have been proposed to address quantum program quality
assurance, our analysis reveals that most existing methods rely on
measurement-based validation in practice. However, due to the inherently
probabilistic nature of quantum programs, measurement-based validation methods
face significant limitations.
  To investigate these limitations, we conducted an empirical study of recent
research on quantum program testing, analyzing measurement-based validation
methods in the literature. Our analysis categorizes existing measurement-based
validation methods into two groups: distribution-level validation and
output-value-level validation. We then compare measurement-based validation
with statevector-based validation methods to evaluate their pros and cons. Our
findings demonstrate that measurement-based validation is suitable for
straightforward assessments, such as verifying the existence of specific output
values, while statevector-based validation proves more effective for
complicated tasks such as assessing the program behaviors.

</details>


### [5] [Incentives and Outcomes in Bug Bounties](https://arxiv.org/abs/2509.16655)
*Serena Wang,Martino Banchio,Krzysztof Kotowicz,Katrina Ligett,R. Preston McAfee,Eduardo' Vela'' Nava*

Main category: cs.SE

TL;DR: Analysis of Google's bug bounty program shows that increasing rewards by up to 200% for high-impact bugs led to more high-value submissions, with elasticities measured, and attracted both veteran and new researchers.


<details>
  <summary>Details</summary>
Motivation: To understand how reward incentives affect the quality and quantity of bug submissions in bug bounty programs, using Google's VRP as a case study.

Method: Empirical analysis of Google's Vulnerability Rewards Program data, focusing on changes in reward amounts posted in July 2024, with increased rewards up to 200% for high-impact bugs.

Result: Increased volume of high-value bugs received after reward increase; elasticities computed; both veteran researchers redirected attention and new top researchers attracted to the program.

Conclusion: Higher financial incentives effectively boost both the quantity and quality of bug submissions in bug bounty programs, engaging both existing and new security researchers.

Abstract: Bug bounty programs have contributed significantly to security in technology
firms in the last decade, but little is known about the role of reward
incentives in producing useful outcomes. We analyze incentives and outcomes in
Google's Vulnerability Rewards Program (VRP), one of the world's largest bug
bounty programs. We analyze the responsiveness of the quality and quantity of
bugs received to changes in payments, focusing on a change in Google's reward
amounts posted in July, 2024, in which reward amounts increased by up to 200%
for the highest impact tier. Our empirical results show an increase in the
volume of high-value bugs received after the reward increase, for which we also
compute elasticities. We further break down the sources of this increase
between veteran researchers and new researchers, showing that the reward
increase both redirected the attention of veteran researchers and attracted new
top security researchers into the program.

</details>


### [6] [Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver](https://arxiv.org/abs/2509.16681)
*Peterson Jean*

Main category: cs.SE

TL;DR: This research explores using formal verification methods (specifically SPARK Ada) to anticipate human factor risks in medical devices like the T34 syringe pump during early development stages, rather than relying solely on real-life testing which can be catastrophic.


<details>
  <summary>Details</summary>
Motivation: Current medical device safety standards advocate for better human factors in user interaction, but many risks aren't caught until real-life testing, which can be dangerous for ambulatory devices. Formal methods can help anticipate errors earlier in development.

Method: The research uses SPARK Ada's formal verification tool against a behavioral model of the T34 syringe driver, exploring and implementing a Generic Infusion Pump model refinement in SPARK Ada.

Result: The verification level of the end prototype is evaluated using SPARK, with exploration of potential limitations regarding abstraction and UI design components in SPARK Ada.

Conclusion: Formal verification methods like SPARK Ada can provide a common framework for safety integration in medical device industry, potentially allowing mathematical proofs to reduce human factor risks early in development.

Abstract: The increase in safety and critical systems improved Healthcare. Due to their
risk of harm, such systems are subject to stringent guidelines and compliances.
These safety measures ensure a seamless experience and mitigate the risk to
end-users. Institutions like the Food and Drug Administration and the NHS,
respectively, established international standards and competency frameworks to
ensure industry compliance with these safety concerns. Medical device
manufacturing is mainly concerned with standards. Consequently, these standards
now advocate for better human factors considered in user interaction for
medical devices. This forces manufacturers to rely on heavy testing and review
to cover many of these factors during development. Sadly, many human factor
risks will not be caught until proper testing in real life, which might be
catastrophic in the case of an ambulatory device like the T34 syringe pump.
Therefore, effort in formal methods research may propose new solutions in
anticipating these errors in the early stages of development or even reducing
their occurrence based on the use of standard generic model. These generically
developed models will provide a common framework for safety integration in
industry and may potentially be proven using formal verification mathematical
proofs. This research uses SPARK Ada's formal verification tool against a
behavioural model of the T34 syringe driver. A Generic Infusion Pump model
refinement is explored and implemented in SPARK Ada. As a subset of the Ada
language, the verification level of the end prototype is evaluated using SPARK.
Exploring potential limitations defines the proposed model's implementation
liability when considering abstraction and components of User Interface design
in SPARK Ada.

</details>


### [7] [RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code](https://arxiv.org/abs/2509.16701)
*Shunyu Liu,Guangdong Bai,Mark Utting,Guowei Yang*

Main category: cs.SE

TL;DR: RelRepair is a novel approach that enhances automated program repair by retrieving project-specific code to guide LLMs in generating more accurate patches.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with project-specific repairs due to their general-purpose pre-training, lacking understanding of domain-specific identifiers and code structures within specific codebases.

Method: RelRepair identifies relevant function signatures by analyzing function names and comments, then retrieves relevant code snippets through deeper code analysis, incorporating this information into LLM prompts.

Result: RelRepair repairs 101 bugs in Defects4J V1.2 and achieves a 17.1% improvement on ManySStuBs4J, increasing the overall fix rate to 48.3%.

Conclusion: Providing relevant project-specific information to LLMs is crucial for effective automated program repair, demonstrating the importance of context-aware approaches in LLM-based APR.

Abstract: Automated Program Repair (APR) has emerged as a promising paradigm for
reducing debugging time and improving the overall efficiency of software
development. Recent advances in Large Language Models (LLMs) have demonstrated
their potential for automated bug fixing and other software engineering tasks.
Nevertheless, the general-purpose nature of LLM pre-training means these models
often lack the capacity to perform project-specific repairs, which require
understanding of domain-specific identifiers, code structures, and contextual
relationships within a particular codebase. As a result, LLMs may struggle to
generate correct patches when the repair depends on project-specific
information.
  To address this limitation, we introduce RelRepair, a novel approach that
retrieves relevant project-specific code to enhance automated program repair.
RelRepair first identifies relevant function signatures by analyzing function
names and code comments within the project. It then conducts deeper code
analysis to retrieve code snippets relevant to the repair context. The
retrieved relevant information is then incorporated into the LLM's input
prompt, guiding the model to generate more accurate and informed patches. We
evaluate RelRepair on two widely studied datasets, Defects4J V1.2 and
ManySStuBs4J, and compare its performance against several state-of-the-art
LLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J
V1.2. Furthermore, RelRepair achieves a 17.1\% improvement in the ManySStuBs4J
dataset, increasing the overall fix rate to 48.3\%. These results highlight the
importance of providing relevant project-specific information to LLMs, shedding
light on effective strategies for leveraging LLMs in APR tasks.

</details>


### [8] [Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction](https://arxiv.org/abs/2509.16795)
*Saikat Mondal,Chanchal K. Roy,Hong Wang,Juan Arguello,Samantha Mathan*

Main category: cs.SE

TL;DR: GitHub Copilot achieves 86.2% accuracy in detecting API misuse with 91.2% precision and 92.4% recall, successfully fixing over 95% of identified misuses, though it struggles with complex cases.


<details>
  <summary>Details</summary>
Motivation: API misuse causes security vulnerabilities and system failures, but existing detection tools operate post-development, delaying resolution and increasing costs. AI code assistants like Copilot offer potential for real-time detection.

Method: Evaluated Copilot using MUBench with 740 misuse examples (manual and AI-assisted) and 147 correct usage cases in Visual Studio Code, analyzing detection accuracy, precision, recall, and fix rates.

Result: Copilot achieved 86.2% detection accuracy, 91.2% precision, 92.4% recall, and fixed over 95% of identified misuses. Strong performance on common misuse types but limitations with compound/context-sensitive cases.

Conclusion: Copilot shows promise for real-time API misuse detection and correction during development, though improvements are needed for complex misuse scenarios.

Abstract: API misuse introduces security vulnerabilities, system failures, and
increases maintenance costs, all of which remain critical challenges in
software development. Existing detection approaches rely on static analysis or
machine learning-based tools that operate post-development, which delays defect
resolution. Delayed defect resolution can significantly increase the cost and
complexity of maintenance and negatively impact software reliability and user
trust. AI-powered code assistants, such as GitHub Copilot, offer the potential
for real-time API misuse detection within development environments. This study
evaluates GitHub Copilot's effectiveness in identifying and correcting API
misuse using MUBench, which provides a curated benchmark of misuse cases. We
construct 740 misuse examples, manually and via AI-assisted variants, using
correct usage patterns and misuse specifications. These examples and 147
correct usage cases are analyzed using Copilot integrated in Visual Studio
Code. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and
recall of 92.4%. It performed strongly on common misuse types (e.g.,
missing-call, null-check) but struggled with compound or context-sensitive
cases. Notably, Copilot successfully fixed over 95% of the misuses it
identified. These findings highlight both the strengths and limitations of
AI-driven coding assistants, positioning Copilot as a promising tool for
real-time pair programming and detecting and fixing API misuses during software
development.

</details>


### [9] [Implementation of the Collision Avoidance System for DO-178C Compliance](https://arxiv.org/abs/2509.16844)
*Rim Zrelli,Henrique Amaral Misson,Sorelle Kamkuimo,Maroua Ben Attia,Abdo Shabah,Felipe Gohring de Magalhaes,Gabriela Nicolescu*

Main category: cs.SE

TL;DR: Implementation of a DO-178C compliant Collision Avoidance System for UAVs using formal methods and automated verification tools, demonstrating effective certification approach for safety-critical software.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a rigorous methodology for achieving DO-178C compliance in safety-critical software for UAVs, supporting safe integration into civil airspace.

Method: Combined formal methods, model-based development, and automated verification tools (Alloy, SPIN, Simulink Embedded Coder, LDRA tool suite) throughout the software lifecycle from requirements to verification.

Result: Early detection and correction of specification defects, robust traceability, strong verification evidence, confirmed code quality and coverage, mathematical assurance of correctness for critical components.

Conclusion: The approach proved effective in addressing certification challenges for UAV safety-critical systems, although full integration phase was not implemented.

Abstract: This technical report presents the detailed implementation of a Collision
Avoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case
study to demonstrate a rigorous methodology for achieving DO-178C compliance in
safety-critical software. The CAS is based on functional requirements inspired
by NASA's Access 5 project and is designed to autonomously detect, evaluate,
and avoid potential collision threats in real-time, supporting the safe
integration of UAVs into civil airspace.
  The implementation environment combines formal methods, model-based
development, and automated verification tools, including Alloy, SPIN, Simulink
Embedded Coder, and the LDRA tool suite. The report documents each phase of the
software lifecycle: requirements specification and validation, architectural
and detailed design, coding, verification, and traceability, with a strong
focus on compliance with DO-178C Design Assurance Level B objectives.
  Results demonstrate that formal modelling and automated toolchains enabled
early detection and correction of specification defects, robust traceability,
and strong evidence of verification and validation across all development
stages. Static and dynamic analyses confirmed code quality and coverage, while
formal verification methods provided mathematical assurance of correctness for
critical components. Although the integration phase was not fully implemented,
the approach proved effective in addressing certification challenges for UAV
safety-critical systems.
  \keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),
DO-178C compliance, Safety-critical software, Formal methods, Model-based
development, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool
suite, Software verification and validation, Traceability, Certification.

</details>


### [10] [MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions](https://arxiv.org/abs/2509.16864)
*Wei Liu,Yi Wen Heng,Feng Lin,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: MobileUPReg is a black-box framework that detects user-perceived performance regressions in mobile OS updates by comparing metrics like response time and dropped frames across OS versions, achieving high accuracy and deployment in industrial CI pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing detection techniques rely on system-level metrics or focus on specific OS components, missing regressions actually perceived by users such as slower responses or UI stutters.

Method: MobileUPReg runs the same apps under different OS versions and compares user-perceived performance metrics (response time, finish time, launch time, and dropped frames) to identify perceptible regressions.

Result: MobileUPReg achieves 0.96 precision, 0.91 recall, and 0.93 F1-score, significantly outperforming statistical baselines. It has been deployed in industrial CI pipelines, analyzing thousands of screencasts daily and uncovering regressions missed by traditional tools.

Conclusion: MobileUPReg enables accurate, scalable, and perceptually aligned regression detection for mobile OS validation.

Abstract: Mobile operating systems (OS) are frequently updated, but such updates can
unintentionally degrade user experience by introducing performance regressions.
Existing detection techniques often rely on system-level metrics (e.g., CPU or
memory usage) or focus on specific OS components, which may miss regressions
actually perceived by users -- such as slower responses or UI stutters. To
address this gap, we present MobileUPReg, a black-box framework for detecting
user-perceived performance regressions across OS versions. MobileUPReg runs the
same apps under different OS versions and compares user-perceived performance
metrics -- response time, finish time, launch time, and dropped frames -- to
identify regressions that are truly perceptible to users. In a large-scale
study, MobileUPReg achieves high accuracy in extracting user-perceived metrics
and detects user-perceived regressions with 0.96 precision, 0.91 recall, and
0.93 F1-score -- significantly outperforming a statistical baseline using the
Wilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an
industrial CI pipeline, where it analyzes thousands of screencasts across
hundreds of apps daily and has uncovered regressions missed by traditional
tools. These results demonstrate that MobileUPReg enables accurate, scalable,
and perceptually aligned regression detection for mobile OS validation.

</details>


### [11] [DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems](https://arxiv.org/abs/2509.16870)
*Rui Yang,Michael Fu,Chakkrit Tantithamthavorn,Chetan Arora,Gunel Gulmammadova,Joey Chua*

Main category: cs.SE

TL;DR: DecipherGuard is a novel framework that improves runtime safety of LLM-powered systems by integrating a deciphering layer and low-rank adaptation to defend against obfuscation- and template-based jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: LLM-powered systems are increasingly deployed in critical sectors, but current guardrails like LlamaGuard show significant vulnerability (24% DSR drop) under sophisticated jailbreak attacks, creating safety concerns during runtime deployment.

Method: Proposes DecipherGuard framework with two key components: a deciphering layer to counter obfuscation-based prompts and a low-rank adaptation mechanism to enhance effectiveness against template-based attacks.

Result: Empirical evaluation on 22,000+ prompts shows DecipherGuard improves Defense Success Rate (DSR) by 36-65% and Overall Guardrail Performance (OGP) by 20-50% compared to LlamaGuard and other runtime guardrails.

Conclusion: DecipherGuard effectively defends LLM-powered software systems against jailbreak attacks during runtime, significantly outperforming existing state-of-the-art guardrail solutions.

Abstract: Intelligent software systems powered by Large Language Models (LLMs) are
increasingly deployed in critical sectors, raising concerns about their safety
during runtime. Through an industry-academic collaboration when deploying an
LLM-powered virtual customer assistant, a critical software engineering
challenge emerged: how to enhance a safer deployment of LLM-powered software
systems at runtime? While LlamaGuard, the current state-of-the-art runtime
guardrail, offers protection against unsafe inputs, our study reveals a Defense
Success Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak
attacks. In this paper, we propose DecipherGuard, a novel framework that
integrates a deciphering layer to counter obfuscation-based prompts and a
low-rank adaptation mechanism to enhance guardrail effectiveness against
template-based attacks. Empirical evaluation on over 22,000 prompts
demonstrates that DecipherGuard improves DSR by 36% to 65% and Overall
Guardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other
runtime guardrails. These results highlight the effectiveness of DecipherGuard
in defending LLM-powered software systems against jailbreak attacks during
runtime.

</details>


### [12] [Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling](https://arxiv.org/abs/2509.16939)
*Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: DSC-SRGM integrates synthetic data generation with cross-project transfer learning to improve software reliability prediction in data-scarce environments, achieving up to 23.3% improvement over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Software Reliability Growth Models (SRGMs) suffer from degraded predictive accuracy in data-scarce environments, and cross-project transfer learning is limited by dataset scarcity and confidentiality.

Method: Generate synthetic datasets using traditional SRGMs, apply cross-correlation-based clustering to identify similar patterns to target projects, then train deep learning models on these synthetic datasets for reliability prediction.

Result: DSC-SRGM achieves up to 23.3% improvement over traditional SRGMs and 32.2% over cross-project deep learning models trained on real-world datasets, evaluated on 60 real-world datasets.

Conclusion: DSC-SRGM is a promising approach for software reliability prediction in data-scarce environments, though maintaining appropriate data balance between synthetic and real-world data is crucial to avoid performance degradation.

Abstract: Software Reliability Growth Models (SRGMs) are widely used to predict
software reliability based on defect discovery data collected during testing or
operational phases. However, their predictive accuracy often degrades in
data-scarce environments, such as early-stage testing or safety-critical
systems. Although cross-project transfer learning has been explored to mitigate
this issue by leveraging data from past projects, its applicability remains
limited due to the scarcity and confidentiality of real-world datasets. To
overcome these limitations, we propose Deep Synthetic Cross-project SRGM
(DSC-SRGM), a novel approach that integrates synthetic data generation with
cross-project transfer learning. Synthetic datasets are generated using
traditional SRGMs to preserve the statistical characteristics of real-world
defect discovery trends. A cross-correlation-based clustering method is applied
to identify synthetic datasets with patterns similar to the target project.
These datasets are then used to train a deep learning model for reliability
prediction. The proposed method is evaluated on 60 real-world datasets, and its
performance is compared with both traditional SRGMs and cross-project deep
learning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%
improvement in predictive accuracy over traditional SRGMs and 32.2% over
cross-project deep learning models trained on real-world datasets. However,
excessive use of synthetic data or a naive combination of synthetic and
real-world data may degrade prediction performance, highlighting the importance
of maintaining an appropriate data balance. These findings indicate that
DSC-SRGM is a promising approach for software reliability prediction in
data-scarce environments.

</details>


### [13] [SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?](https://arxiv.org/abs/2509.16941)
*Xiang Deng,Jeff Da,Edwin Pan,Yannis Yiming He,Charles Ide,Kanak Garg,Niklas Lauffer,Andrew Park,Nitin Pasari,Chetan Rane,Karmini Sampath,Maya Krishnan,Srivatsa Kundurthy,Sean Hendryx,Zifan Wang,Chen Bo Calvin Zhang,Noah Jacobson,Bing Liu,Brad Kenstler*

Main category: cs.SE

TL;DR: SWE-Bench Pro is a challenging benchmark for software engineering AI agents, featuring 1,865 complex enterprise-level problems from 41 repositories, with current models achieving below 25% success rate.


<details>
  <summary>Details</summary>
Motivation: To create a more realistic and challenging benchmark that captures complex, enterprise-level software engineering problems beyond the scope of existing benchmarks like SWE-BENCH.

Method: Built upon SWE-BENCH best practices, SWE-Bench Pro contains 1,865 problems from 41 repositories across business applications, B2B services, and developer tools. The benchmark is partitioned into public, held-out, and commercial sets with human-verified tasks requiring multi-file patches and substantial code modifications.

Result: Current coding models perform poorly on SWE-Bench Pro, with the highest score (GPT-5) achieving only 23.3% Pass@1. The benchmark reveals significant limitations in current AI models' ability to handle complex software engineering tasks.

Conclusion: SWE-Bench Pro provides a contamination-resistant testbed that better captures real-world software development complexity, advancing the development of truly autonomous software engineering agents at professional levels.

Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that
builds upon the best practices of SWE-BENCH [25], but is explicitly designed to
capture realistic, complex, enterprise-level problems beyond the scope of
SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of
41 actively maintained repositories spanning business applications, B2B
services, and developer tools. The benchmark is partitioned into a public set
with open access to problems sourced from 11 repositories, a held-out set of 12
repositories and a commercial set of 18 proprietary repositories where we have
formal partnership agreements with early-stage startups. Problems in the
held-out and the commercial set are not publicly accessible, but we release
results on the commercial set. Our benchmark features long-horizon tasks that
may require hours to days for a professional software engineer to complete,
often involving patches across multiple files and substantial code
modifications. All tasks are human-verified and augmented with sufficient
context to ensure resolvability. In our evaluation of widely used coding
models, under a unified scaffold, we observe that their performance on
SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest
score to date at 23.3%. To better understand these limitations, we cluster the
failure modes observed in the collected agent trajectories for a clearer
characterization of the error patterns exhibited by current models. Overall,
SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully
captures the complexity and diversity of real-world software development,
advancing the pursuit of truly autonomous software engineering agents at a
professional level.

</details>


### [14] [Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results](https://arxiv.org/abs/2509.16985)
*James J. Cusick*

Main category: cs.SE

TL;DR: An end-to-end process for routinized code scanning and prioritized remediation of software vulnerabilities in both proprietary and open-source software, with tool selection and integration into DevSecOps workflows.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities pose significant security risks in software development, especially when using both proprietary and open-source software components.

Method: Presents an industry-proven generic process for custom instantiation, configuration, and execution of code scanning, with selected tools integrated into an iterative DevSecOps process.

Result: Provides examples of vulnerability instances and their treatment in both industrial proprietary and open-source applications, demonstrating the process effectiveness.

Conclusion: The method can be adopted with minimal adjustments to reduce source code vulnerabilities, decrease supply chain risk, and improve security profiles of both new and legacy solutions, with potential enhancements from automation and AI technologies.

Abstract: Software vulnerabilities remain a significant risk factor in achieving
security objectives within software development organizations. This is
especially true where either proprietary or open-source software (OSS) is
included in the technological environment. In this paper an end-to-end process
with supporting methods and tools is presented. This industry proven generic
process allows for the custom instantiation, configuration, and execution of
routinized code scanning for software vulnerabilities and their prioritized
remediation. A select set of tools are described for this key DevSecOps
function and placed into an iterative process. Examples of both industrial
proprietary applications and open-source applications are provided including
specific vulnerability instances and a discussion of their treatment. The
benefits of each selected tool are considered, and alternative tools are also
introduced. Application of this method in a comprehensive SDLC model is also
reviewed along with prospective enhancements from automation and the
application of advanced technologies including AI. Adoption of this method can
be achieved with minimal adjustments and with maximum flexibility for results
in reducing source code vulnerabilities, reducing supply chain risk, and
improving the security profile of new or legacy solutions.

</details>


### [15] [Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering](https://arxiv.org/abs/2509.17096)
*Ziyou Li,Agnia Sergeyuk,Maliheh Izadi*

Main category: cs.SE

TL;DR: Prompt-with-Me is a structured prompt management system for software engineering that automatically classifies prompts using a four-dimensional taxonomy and provides features like language refinement, sensitive information masking, and template extraction to improve prompt quality and reuse.


<details>
  <summary>Details</summary>
Motivation: Current prompt management in software engineering is ad hoc, hindering reliability, reuse, and integration into industrial workflows, despite LLMs transforming the field.

Method: The system uses a four-dimensional taxonomy (intent, author role, software development lifecycle stage, prompt type) for automatic classification, and includes features for language refinement, sensitive information masking, and template extraction. Validated through taxonomy study of 1108 real-world prompts and user study with 11 participants.

Result: LLMs can accurately classify software engineering prompts. User study showed strong acceptance with high usability (Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in prompt quality and efficiency through reduced repetitive effort.

Conclusion: The paper offers actionable insights for building next-generation prompt management tools for software engineering workflows, demonstrating practical viability of structured prompt management.

Abstract: Large Language Models are transforming software engineering, yet prompt
management in practice remains ad hoc, hindering reliability, reuse, and
integration into industrial workflows. We present Prompt-with-Me, a practical
solution for structured prompt management embedded directly in the development
environment. The system automatically classifies prompts using a
four-dimensional taxonomy encompassing intent, author role, software
development lifecycle stage, and prompt type. To enhance prompt reuse and
quality, Prompt-with-Me suggests language refinements, masks sensitive
information, and extracts reusable templates from a developer's prompt library.
Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can
accurately classify software engineering prompts. Furthermore, our user study
with 11 participants shows strong developer acceptance, with high usability
(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in
prompt quality and efficiency through reduced repetitive effort. Lastly, we
offer actionable insights for building the next generation of prompt management
and maintenance tools for software engineering workflows.

</details>


### [16] [Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs](https://arxiv.org/abs/2509.17314)
*Juyeon Yoon,Somin Kim,Robert Feldt,Shin Yoo*

Main category: cs.SE

TL;DR: CLOTHO is a task-specific adequacy measure that estimates input difficulty from hidden LLM states before generation, using Gaussian Mixture Models to predict failures with high accuracy while reducing labeling costs.


<details>
  <summary>Details</summary>
Motivation: Testing LLMs on specific tasks is difficult and costly due to lack of ground truth and reliance on human judgment. Existing measures require full inference, creating a need for pre-generation adequacy assessment.

Method: Uses Gaussian Mixture Model (GMM) to adaptively sample informative cases from unlabeled inputs for human labeling, then ranks unseen inputs by failure likelihood based on hidden LLM states without generating outputs.

Result: Achieves ROC-AUC of 0.716 across 8 benchmark tasks and 3 LLMs, with reference sets only 5.4% of inputs. Increases failing inputs from 18.7 to 42.5 out of 100 for proprietary models compared to random prioritization.

Conclusion: CLOTHO effectively predicts LLM failures pre-generation, reduces costs, and transfers well from open-weight to proprietary models, complementing existing post-generation uncertainty measures.

Abstract: Software increasingly relies on the emergent capabilities of Large Language
Models (LLMs), from natural language understanding to program analysis and
generation. Yet testing them on specific tasks remains difficult and costly:
many prompts lack ground truth, forcing reliance on human judgment, while
existing uncertainty and adequacy measures typically require full inference. A
key challenge is to assess input adequacy in a way that reflects the demands of
the task, ideally before even generating any output. We introduce CLOTHO, a
task-specific, pre-generation adequacy measure that estimates input difficulty
directly from hidden LLM states. Given a large pool of unlabelled inputs for a
specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample
the most informative cases for human labelling. Based on this reference set the
GMM can then rank unseen inputs by their likelihood of failure. In our
empirical evaluation across eight benchmark tasks and three open-weight LLMs,
CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference
sets that are on average only 5.4% of inputs. It does so without generating any
outputs, thereby reducing costs compared to existing uncertainty measures.
Comparison of CLOTHO and post-generation uncertainty measures shows that the
two approaches complement each other. Crucially, we show that adequacy scores
learnt from open-weight LLMs transfer effectively to proprietary models,
extending the applicability of the approach. When prioritising test inputs for
proprietary models, CLOTHO increases the average number of failing inputs from
18.7 to 42.5 out of 100, compared to random prioritisation.

</details>


### [17] [BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing](https://arxiv.org/abs/2509.17335)
*Mingxuan Xiao,Yan Xiao,Shunhui Ji,Jiahe Tu,Pengcheng Zhang*

Main category: cs.SE

TL;DR: BASFuzz is an efficient fuzz testing method for LLM-based NLP software that addresses challenges in testing behavioral patterns and natural language generation scenarios through text consistency metrics and Beam-Annealing Search algorithm.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzing methods for LLM-based NLP software face challenges: insufficient coupling with behavioral patterns and degraded capability for natural language generation testing, especially as LLMs are deployed in critical industries.

Method: BASFuzz targets complete test inputs (prompts + examples), uses text consistency metrics to guide mutations, employs Beam-Annealing Search (combining beam search and simulated annealing), and incorporates information entropy-based adaptive adjustment with elitism strategy.

Result: BASFuzz achieves 90.335% testing effectiveness while reducing average time overhead by 2,163.852 seconds compared to best baseline, evaluated on six datasets across NLG and NLU scenarios.

Conclusion: BASFuzz enables more effective robustness evaluation for LLM-based NLP software prior to deployment by addressing key testing challenges through tailored fuzzing approach.

Abstract: Fuzzing has shown great success in evaluating the robustness of intelligent
natural language processing (NLP) software. As large language model (LLM)-based
NLP software is widely deployed in critical industries, existing methods still
face two main challenges: 1 testing methods are insufficiently coupled with the
behavioral patterns of LLM-based NLP software; 2 fuzzing capability for the
testing scenario of natural language generation (NLG) generally degrades. To
address these issues, we propose BASFuzz, an efficient Fuzz testing method
tailored for LLM-based NLP software. BASFuzz targets complete test inputs
composed of prompts and examples, and uses a text consistency metric to guide
mutations of the fuzzing loop, aligning with the behavioral patterns of
LLM-based NLP software. A Beam-Annealing Search algorithm, which integrates
beam search and simulated annealing, is employed to design an efficient fuzzing
loop. In addition, information entropy-based adaptive adjustment and an elitism
strategy further enhance fuzzing capability. We evaluate BASFuzz on six
datasets in representative scenarios of NLG and natural language understanding
(NLU). Experimental results demonstrate that BASFuzz achieves a testing
effectiveness of 90.335% while reducing the average time overhead by 2,163.852
seconds compared to the current best baseline, enabling more effective
robustness evaluation prior to software deployment.

</details>


### [18] [SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding](https://arxiv.org/abs/2509.17338)
*Pengfei He,Shaowei Wang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: \ourtool is a novel static program slicing framework that reformulates slicing as a sequence-to-sequence task using lightweight language models with copy mechanism and constrained decoding to address dependency identification and generation constraint challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional static slicing tools require complete parsable source code, limiting real-world applicability. Learning-based approaches face inaccurate dependency identification and unconstrained generation problems.

Method: Proposes \ourtool with copy mechanism for accurate dependency capture and constrained decoding (lexical constraint restricting outputs to input tokens, and syntactic constraint using TSED monotonicity to detect invalid outputs).

Result: Outperforms state-of-the-art baselines on CodeNet and LeetCode datasets, improving ExactMatch scores by up to 27%, and demonstrates strong performance on incomplete code.

Conclusion: \ourtool provides robust and practical static program slicing for real-world development environments, effectively addressing key challenges in learning-based slicing approaches.

Abstract: Static program slicing is a fundamental technique in software engineering.
Traditional static slicing tools rely on parsing complete source code, which
limits their applicability to real-world scenarios where code snippets are
incomplete or unparsable. While recent research developed learning-based
approaches to predict slices, they face critical challenges: (1) Inaccurate
dependency identification, where models fail to precisely capture data and
control dependencies between code elements; and (2) Unconstrained generation,
where models produce slices with extraneous or hallucinated tokens not present
in the input, violating the structural integrity of slices. To address these
challenges, we propose \ourtool, a novel slicing framework that reformulates
static program slicing as a sequence-to-sequence task using lightweight
language models (e.g., CodeT5+). Our approach incorporates two key innovations.
First, we introduce a copy mechanism that enables the model to more accurately
capture inter-element dependencies and directly copy relevant tokens from the
input, improving both dependency reasoning and generation constraint. Second,
we design a constrained decoding process with (a) lexical constraint,
restricting outputs to input tokens only, and (b) syntactic constraint,
leveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect
structurally invalid outputs and discard them. We evaluate \ourtool on CodeNet
and LeetCode datasets and show it consistently outperforms state-of-the-art
baselines, improving ExactMatch scores by up to 27\%. Furthermore, \ourtool
demonstrates strong performance on incomplete code, highlighting its robustness
and practical utility in real-world development environments.

</details>


### [19] [Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings](https://arxiv.org/abs/2509.17548)
*Hugo Villamizar,Jannik Fischbach,Alexander Korn,Andreas Vogelsang,Daniel Mendez*

Main category: cs.SE

TL;DR: This paper proposes a research program to study prompt management in LLM-integrated software engineering workflows, highlighting the current ad-hoc nature of prompt usage and the need for systematic approaches.


<details>
  <summary>Details</summary>
Motivation: Prompts are becoming key software engineering artifacts but little is known about how they're actually used, managed, and whether systematic management is worthwhile. Current practices are largely ad-hoc with trial-and-error refinement.

Method: The research program involves: (a) characterizing current prompt practices and challenges, (b) analyzing prompts as software artifacts, and (c) developing evidence-based guidelines. An exploratory survey with 74 professionals from 6 countries was conducted as a first step.

Result: Survey findings reveal prompt usage in SE is largely ad-hoc: prompts are refined through trial-and-error, rarely reused, and shaped by individual heuristics rather than standardized practices.

Conclusion: The findings highlight the need for more systematic approaches to prompt management and provide empirical foundation for the research program's subsequent stages focused on developing evidence-based guidelines.

Abstract: Developers now routinely interact with large language models (LLMs) to
support a range of software engineering (SE) tasks. This prominent role
positions prompts as potential SE artifacts that, like other artifacts, may
require systematic development, documentation, and maintenance. However, little
is known about how prompts are actually used and managed in LLM-integrated
workflows, what challenges practitioners face, and whether the benefits of
systematic prompt management outweigh the associated effort. To address this
gap, we propose a research programme that (a) characterizes current prompt
practices, challenges, and influencing factors in SE; (b) analyzes prompts as
software artifacts, examining their evolution, traceability, reuse, and the
trade-offs of systematic management; and (c) develops and empirically evaluates
evidence-based guidelines for managing prompts in LLM-integrated workflows. As
a first step, we conducted an exploratory survey with 74 software professionals
from six countries to investigate current prompt practices and challenges. The
findings reveal that prompt usage in SE is largely ad-hoc: prompts are often
refined through trial-and-error, rarely reused, and shaped more by individual
heuristics than standardized practices. These insights not only highlight the
need for more systematic approaches to prompt management but also provide the
empirical foundation for the subsequent stages of our research programme.

</details>


### [20] [From OCL to JSX: declarative constraint modeling in modern SaaS tools](https://arxiv.org/abs/2509.17629)
*Antonio Bucchiarone,Juri Di Rocco,Damiano Di Vincenzo,Alfonso Pierantonio*

Main category: cs.SE

TL;DR: JSX-based constraints outperform OCL.js in web-based modeling tools, offering better expressiveness and front-end integration.


<details>
  <summary>Details</summary>
Motivation: OCL.js faces challenges like partial standard coverage and weak integration with modern front-end toolchains, despite being web-aligned for model validation.

Method: Explore JSX (from React ecosystem) as an alternative to OCL.js for constraint expression in SaaS modeling environments, using empirical evaluation across representative modeling scenarios.

Result: JSX provides broader expressiveness and better fits front-end-first architectures compared to OCL.js.

Conclusion: JSX shows promise as a constraint specification language for modern modeling tools, particularly in front-end-focused architectures.

Abstract: The rise of Node.js in 2010, followed by frameworks like Angular, React, and
Vue.js, has accelerated the growth of low code development platforms. These
platforms harness modern UIX paradigms, component-based architectures, and the
SaaS model to enable non-experts to build software. The widespread adoption of
single-page applications (SPAs), driven by these frameworks, has shaped
low-code tools to deliver responsive, client side experiences. In parallel,
many modeling platforms have moved to the cloud, adopting either server-centric
architectures (e.g., GSLP) or client-side intelligence via SPA frameworks,
anchoring core components in JavaScript or TypeScript. Within this context,
OCL.js, a JavaScript-based implementation of the Object Constraint Language,
offers a web aligned approach to model validation, yet faces challenges such as
partial standard coverage, limited adoption, and weak integration with modern
front-end toolchains. In this paper, we explore JSX, a declarative, functional
subset of JavaScript/TypeScript used in the React ecosystem, as an alternative
to constraint expression in SaaS-based modeling environments. Its
component-oriented structure supports inductive definitions for syntax, code
generation, and querying. Through empirical evaluation, we compare JSX-based
constraints with OCL.js across representative modeling scenarios. Results show
JSX provides broader expressiveness and better fits front-end-first
architectures, indicating a promising path for constraint specification in
modern modeling tools.

</details>


### [21] [Diagnosing Violations of State-based Specifications in iCFTL](https://arxiv.org/abs/2509.17776)
*Cristina Stratan,Claudio Mandrioli,Domenico Bianculli*

Main category: cs.SE

TL;DR: This paper presents iCFTL-Diagnostics, a tool that generates informative diagnostics for runtime verification violations by using backward data-flow analysis to identify relevant statements that caused specification violations.


<details>
  <summary>Details</summary>
Motivation: Runtime verification monitors system behavior but often provides insufficient information when specifications are violated. Current Boolean or quantitative verdicts don't explain why violations occurred, making debugging difficult.

Method: The approach uses backward data-flow analysis to statically determine relevant statements contributing to specification violations, instruments programs to produce enriched execution traces, and performs runtime analysis to identify violation-causing statements.

Result: The tool achieves 90% precision in identifying relevant statements for 100 out of 112 specifications, reduces inspection lines by at least 90%, generates diagnoses within 7 minutes using ≤25MB memory, with less than 30% execution time overhead and below 20% memory overhead from instrumentation.

Conclusion: iCFTL-Diagnostics effectively addresses the diagnostic gap in runtime verification by providing detailed information about specification violations while maintaining reasonable computational costs, making it practical for real-world software verification.

Abstract: As modern software systems grow in complexity and operate in dynamic
environments, the need for runtime analysis techniques becomes a more critical
part of the verification and validation process. Runtime verification monitors
the runtime system behaviour by checking whether an execution trace - a
sequence of recorded events - satisfies a given specification, yielding a
Boolean or quantitative verdict. However, when a specification is violated,
such a verdict is often insufficient to understand why the violation happened.
To fill this gap, diagnostics approaches aim to produce more informative
verdicts. In this paper, we address the problem of generating informative
verdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)
specifications that express constraints over program variable values. We
propose a diagnostic approach based on backward data-flow analysis to
statically determine the relevant statements contributing to the specification
violation. Using this analysis, we instrument the program to produce enriched
execution traces. Using the enriched execution traces, we perform the runtime
analysis and identify the statements whose execution led to the specification
violation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,
and evaluated it on 112 specifications across 10 software projects. Our tool
achieves 90% precision in identifying relevant statements for 100 of the 112
specifications. It reduces the number of lines that have to be inspected for
diagnosing a violation by at least 90%. In terms of computational cost,
iCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than
25 MB of memory. The instrumentation required to support diagnostics incurs an
execution time overhead of less than 30% and a memory overhead below 20%.

</details>
