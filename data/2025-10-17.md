<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: The paper introduces ArbiterOS, a governance-first paradigm for principled agent engineering to address the brittleness and unpredictability of LLM-based autonomous systems in production.


<details>
  <summary>Details</summary>
Motivation: The transition from prototype to production for LLM-based agents is hindered by a 'crisis of craft' - agents are brittle, unpredictable, and untrustworthy in mission-critical applications due to a paradigm mismatch between probabilistic processors and deterministic software engineering models.

Method: Introduces a governance-first paradigm embodied in a formal architecture called ArbiterOS, which provides principled agent engineering to address the fundamental paradigm mismatch.

Result: The paper proposes ArbiterOS as a solution to the crisis of craft in LLM-based agent systems, though specific implementation results are not detailed in the abstract.

Conclusion: A governance-first approach through formal architectures like ArbiterOS is needed to overcome the paradigm mismatch and enable trustworthy deployment of LLM-based agents in production environments.

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [2] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec is the first benchmark for evaluating both correctness and security in multi-turn coding scenarios, showing significant performance drops compared to single-turn tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks only evaluate single-turn code generation, which doesn't reflect the iterative nature of real-world software development where developers work through multiple rounds of feedback and refinement.

Method: Created a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling real-world coding complexity.

Result: Evaluation of 32 models showed consistent 20-27% drop in 'correct and secure' outputs from single-turn to multi-turn settings. Models performed worse on multi-turn code-diff generation with increased rates of functionally incorrect and insecure outputs. Agent scaffoldings were less effective in multi-turn evaluations.

Conclusion: There's a critical need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows, as current models struggle significantly with iterative development processes.

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [3] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn is a method that aligns code-generating LLMs to produce accessibility-compliant web UIs by optimizing a reward function based on WCAG violations.


<details>
  <summary>Details</summary>
Motivation: LLMs often replicate accessibility flaws from training data, creating interfaces that exclude users with diverse needs and contexts.

Method: A11yn optimizes a novel reward function that penalizes WCAG violations scaled by severity, using UIReq-6.8K dataset for training and RealUIReq-300 benchmark for evaluation.

Result: A11yn lowers the Inaccessibility Rate by 60% over the base model while preserving semantic fidelity and visual quality of generated UIs.

Conclusion: Accessibility can be systematically optimized within LLMs, demonstrating feasibility of aligning code generation for accessibility.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [4] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: Spectral Signature defense methods for detecting backdoor attacks in code LLMs are often suboptimal, and a new proxy metric is proposed to better estimate their performance without model retraining.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in software development, they become targets for backdoor attacks. Existing Spectral Signature defenses may not work well for code models, requiring better evaluation and optimization.

Method: Systematically evaluate Spectral Signature defenses under various attack scenarios and defense configurations for code models, exploring different settings of key factors and discovering a new proxy metric.

Result: Found that widely used Spectral Signature settings in code backdoor detection are often suboptimal. Identified a new proxy metric that can more accurately estimate defense performance without requiring model retraining.

Conclusion: Spectral Signature defenses need optimization for code models, and the proposed proxy metric provides better performance estimation without the computational cost of retraining.

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [5] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone is a program analysis system that uses LLVM and LLMs to identify Recurring Pattern Bugs (RPBs) across codebases, achieving high precision in detecting similar vulnerabilities from known bug patterns.


<details>
  <summary>Details</summary>
Motivation: Fixing individual bugs is labor-intensive, and bug reports can inadvertently expose exploitable patterns that remain unresolved in other code segments, widening the attack surface.

Method: Leverages LLVM and Large Language Models to identify consistent error patterns from patched bug instances, then scans the entire program for similar vulnerable code sections.

Result: Identified over 22K potential issues in Linux kernel from 135 unique RPBs, with 246 confirmed valid out of 400 manually analyzed. Achieved 92.2% precision and 79.1% pairwise accuracy on a dataset of 1.9K security bugs.

Conclusion: RPBs are widespread and significantly compromise software security. BugStone effectively identifies and helps fix these recurring patterns at scale with high accuracy.

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [6] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic is a dataset and framework for converting natural language to Scenic code for autonomous driving scenarios, addressing data scarcity and evaluation issues in NL-to-Scenic generation.


<details>
  <summary>Details</summary>
Motivation: Current NL-to-Scenic generation with LLMs suffers from scarce data, limited reproducibility, and inconsistent metrics, making it difficult to reliably test autonomous driving systems.

Method: Created NL2Scenic dataset with 146 NL/Scenic pairs and 30-case test split, developed Example Retriever and 14 prompting variants, evaluated 13 models using text and execution metrics plus expert study.

Result: GPT-4o performs best overall, Qwen2.5Coder-14B reaches 88% of expert score, EDIT-SIM correlates best with human judgments, and retrieval-augmented prompting boosts smaller models.

Conclusion: NL2Scenic and EDIT-COMP provide standardized evaluation for Scenic code generation, showing mid-size open-source models are practical, cost-effective options for autonomous driving scenario programming.

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [7] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca is a system that automatically mines specifications for opaque commands by using LLMs to translate documentation into structured syntax, exploring valid command invocations, and extracting properties through system-call interposition.


<details>
  <summary>Details</summary>
Motivation: Manual specification creation for opaque components like Unix shell commands is laborious and error-prone, limiting the practicality of systems that rely on such specifications for performance, security, and reliability improvements.

Method: Caruca instruments LLMs to translate command documentation into structured invocation syntax, explores syntactically valid command invocations and execution environments, and interposes at system-call/filesystem level to extract command properties like parallelizability and filesystem conditions.

Result: Caruca generated correct specifications for 59 out of 60 GNU Coreutils, POSIX, and third-party commands, completely eliminating manual effort and currently powering specifications for a state-of-the-art static analysis tool.

Conclusion: Caruca successfully automates specification mining for opaque commands, making specification-dependent systems more practical by removing the manual specification creation bottleneck.

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [8] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: A hybrid evolutionary framework that uses offline knowledge extraction to guide online compiler pass auto-tuning, achieving 11.0% better instruction reduction than -Oz baseline.


<details>
  <summary>Details</summary>
Motivation: Traditional compiler optimization flags like -O3 and -Oz use one-size-fits-all approaches that fail to unlock programs' full performance potential, and finding optimal pass sequences is NP-hard.

Method: Hybrid framework with offline knowledge base construction (pass behavioral vectors, pass groups, synergy pass graph, prototype sequences) and online genetic algorithm with knowledge-infused operators for semantically-aware recombination and targeted mutations.

Result: Achieves average 11.0% additional LLVM IR instruction reduction over opt -Oz baseline across seven public datasets.

Conclusion: The framework demonstrates state-of-the-art capability in discovering personalized, high-performance optimization sequences through knowledge-guided evolutionary search.

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [9] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: First large-scale study of Time Limit Exceeded (TLE) errors in online programming platforms, revealing diverse root causes beyond just inefficient algorithms. Introduces Nettle, an automated repair tool that achieves 98.5% fix rate using LLMs combined with compiler feedback and test cases.


<details>
  <summary>Details</summary>
Motivation: TLE errors are a major challenge for users of online programming platforms like Codeforces and LeetCode, but current debugging tools offer little help and error messages provide no diagnostic insight, causing many users to abandon submissions after repeated failures.

Method: Manually analyzed 1000 Codeforces submissions with TLE errors to classify root causes. Developed Nettle, an automated repair tool that integrates LLMs with targeted compiler feedback and test cases to produce small, correct code edits that eliminate TLEs while preserving functionality.

Result: TLE errors arise from multiple causes including infinite loops, improper data structure use, and inefficient I/O, not just inefficient algorithms. Nettle achieved 98.5% fix rate on 1000 real-world cases, far exceeding LLM baselines, with all repairs passing both Nettle-Eval and platform's official checker.

Conclusion: TLE errors are more complex than conventional performance issues, and automated repair tools like Nettle can effectively resolve them by combining LLMs with targeted feedback mechanisms, providing reliable solutions for online programming platforms.

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [10] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix is a new automated program repair method that uses path-sensitive constraints from correct execution paths to generate patches, addressing challenges of too many plausible patches and overfitting to partial tests.


<details>
  <summary>Details</summary>
Motivation: Existing APR methods struggle with generating too many plausible patch candidates and overfitting to partial test cases due to difficulty in generating precise specifications.

Method: PathFix traces fault paths, derives expected paths from correct outputs on control flow graphs, generates patches by solving state constraints along expected paths, and validates patch correctness. It also integrates LLMs to enhance performance and mitigate scalability issues.

Result: Experimental results show PathFix outperforms existing solutions, particularly in handling complex program structures like loops and recursion.

Conclusion: PathFix effectively addresses APR challenges by leveraging path-sensitive constraints and expected execution paths, demonstrating superior performance in repairing complex software defects.

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [11] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: A Domain-Specific Language (DSL) for defining and enforcing governance policies in software development with diverse stakeholders including AI agents.


<details>
  <summary>Details</summary>
Motivation: Increasing diversity of stakeholders in software development (both human and AI agents) creates governance challenges, especially in Open-Source Software where policies are often unclear or missing.

Method: Development of a novel Domain-Specific Language (DSL) designed to define and enforce rich governance policies for systems with diverse stakeholders including agents.

Result: A DSL framework that enables robust, adaptable, and automated governance for collaborative software development environments.

Conclusion: The proposed DSL offers a pathway toward more effective collaboration in software projects, particularly Open-Source Software, by enabling automated governance in mixed human-AI stakeholder environments.

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [12] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev is a comprehensive benchmark for evaluating End-to-End Software Development (E2ESD) frameworks, featuring fine-grained requirements, BDD test scenarios with Python implementations, and automated testing pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the need for more effective and cost-efficient E2ESD solutions by providing a quality benchmark that reduces annotation effort through human-in-the-loop multi-agent annotation.

Method: Uses Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA) to ensure quality while minimizing annotation effort. Includes fine-grained requirements, multiple BDD test scenarios with Python step implementations, and automated testing pipeline built on Behave framework.

Result: Evaluation of various E2ESD frameworks and LLM backbones reveals persistent struggles in effectively solving these tasks, highlighting the need for better E2ESD solutions.

Conclusion: E2EDev provides a valuable benchmark that demonstrates current E2ESD frameworks' limitations and emphasizes the need for more effective and cost-efficient development solutions.

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [13] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: This study examines software testing competency needs in industry, identifies gaps in current testing education, and highlights competencies not addressed in academic literature through focus groups, interviews, and scoping review.


<details>
  <summary>Details</summary>
Motivation: The evolving software development landscape requires testers to continuously adapt to new tools, practices, and skills, creating a need to understand current competency gaps and educational shortcomings.

Method: Conducted two focus group sessions and interviews with professionals across diverse domains (railway, healthcare, software consulting), performed a curated small-scale scoping review, and used thematic qualitative analysis.

Result: Identified knowledge gaps in professional training methods, challenges in industry training, evaluation methods for training quality, gaps between academic education and industry needs, future testing education trends, and knowledge transfer methods. Scoping review confirmed gaps in AI testing, security testing, and soft skills.

Conclusion: There are significant gaps in software testing education that need to be addressed, particularly in emerging areas like AI testing, security testing, and soft skills, requiring better alignment between academic education and industry needs.

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [14] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen is an adversarial reinforcement learning framework that trains a test case generator against an adversarial code generator to create progressively harder bugs, breaking the fixed-difficulty ceiling of static training methods.


<details>
  <summary>Details</summary>
Motivation: Existing test generation methods rely on static datasets, imposing a fixed-difficulty ceiling that limits their ability to uncover novel or more complex bugs beyond their training scope.

Method: ATGen uses adversarial reinforcement learning where a test generator competes against an adversarial code generator that continuously crafts harder bugs. The test generator is optimized via RL to maximize both output accuracy and attack success.

Result: ATGen significantly outperforms state-of-the-art baselines and serves as both a more effective filter for Best-of-N inference and a higher-quality reward source for training code generation models.

Conclusion: ATGen establishes a new dynamic paradigm for improving the reliability of LLM-generated code by breaking the fixed-difficulty ceiling through adversarial training.

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [15] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: A methodology for systematically identifying traffic simulation requirements to ensure realistic traffic conditions through structured sub-goals.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring realistic traffic conditions in automotive development and testing by maintaining high fidelity in simulations.

Method: Uses a structured approach based on sub-goals in each study phase to derive specific technical needs for microscopic levels, agent models, and visual representation.

Result: Provides a clear link between study objectives and traffic simulation design, enhancing validity of experimental outcomes and participant engagement.

Conclusion: This approach supports robust automotive development and testing by systematically identifying traffic simulation requirements.

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [16] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: First comprehensive evaluation of LLM agents for automated web vulnerability reproduction, revealing significant gaps in handling complex service-based vulnerabilities and environmental adaptation.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of LLM agents in transforming vulnerability reports into working exploits for web applications, addressing an underexplored but critical application area.

Method: Systematically evaluated 20 state-of-the-art LLM agents across 16 dimensions on 3 representative vulnerabilities, then conducted in-depth evaluation of top 3 agents on 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies.

Result: LLM agents achieve reasonable success on simple library-based vulnerabilities but consistently fail on complex service-based vulnerabilities requiring multi-component environments. Performance degrades by over 33% under incomplete authentication information.

Conclusion: There is a significant gap between current LLM agent capabilities and reliable automated vulnerability reproduction demands, highlighting the need for advances in environmental adaptation and autonomous problem-solving capabilities.

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [17] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: An unsupervised approach using name-prediction-based cohesion (NPC) metric to detect supply chain attacks by identifying cohesion disruptions in source code caused by malicious code injections.


<details>
  <summary>Details</summary>
Motivation: Supply chain attacks pose significant threats to software security through rare but devastating malicious code injections in legitimate projects, which are difficult to detect automatically due to the need to understand code intention and context.

Method: Proposes NPC metric to quantify cohesion disruptions by analyzing how function cohesion changes when malicious code is introduced compared to natural fluctuations. Analyzed 54,707 functions across 369 open-source C++ repositories.

Result: Code injection reduces cohesion and shifts naming patterns toward shorter, less descriptive names. Achieved Precision@100 of 36.41% at 1:1,000 ratio and 12.47% at 1:10,000 imbalance, showing effective detection of functions with injected code.

Conclusion: Automated cohesion measurements, particularly name-prediction-based cohesion, can help identify supply chain attacks and improve source code integrity by detecting spurious code injections.

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [18] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: This paper analyzes the challenges of ISA migration from x86 to Arm at Google, showing that modern migrations focus on recompilation rather than binary translation, and demonstrates how AI can automate many migration tasks.


<details>
  <summary>Details</summary>
Motivation: To address the major engineering challenge of migrating codebases between instruction set architectures (ISAs), particularly the shift from x86 to Arm in cloud computing, which has received limited academic attention despite its practical importance.

Method: Analyzed a large-scale migration at Google spanning almost 40,000 code commits, derived a taxonomy of ISA migration tasks, and demonstrated automation approaches including AI-based solutions.

Result: Showed that modern ISA migrations can build on open-source ecosystems for recompilation rather than relying on binary translation, identified key migration tasks, and demonstrated successful automation of many steps.

Conclusion: Modern ISA migrations present different challenges from traditional binary translation approaches, with AI playing a major role in automation, though some tasks remain challenging and warrant further research.

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>
