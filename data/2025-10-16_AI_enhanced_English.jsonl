{"id": "2510.12803", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.12803", "abs": "https://arxiv.org/abs/2510.12803", "authors": ["Shang Zhou", "Zihan Zheng", "Kaiyuan Liu", "Zeyu Shen", "Zerui Cheng", "Zexing Chen", "Hansen He", "Jianzhu Yao", "Huanzhi Mao", "Qiuyang Mang", "Tianfu Fu", "Beichen Li", "Dongruixuan Li", "Wenhao Chai", "Zhuang Liu", "Aleksandra Korolova", "Peter Henderson", "Natasha Jaques", "Pramod Viswanath", "Saining Xie", "Jingbo Shang"], "title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "comment": "Project page: https://livecodebenchpro.com/projects/autocode/overview", "summary": "Writing competitive programming problems is exacting. Authors must: set\nconstraints, input distributions, and edge cases that rule out shortcuts;\ntarget specific algorithms (e.g., max-flow, dynamic programming, data\nstructures); and calibrate complexity beyond the reach of most competitors. We\nargue that this makes for an ideal test of general large language model\ncapabilities and study whether they can do this reliably. We introduce\nAutoCode, which uses multiple rounds of validation to yield competition-grade\nproblem statements and test cases. On held-out problems, AutoCode test suites\napproach 99% consistency with official judgments, a significant improvement\nover current state-of-the-art methods like HardTests, which achieve less than\n81%. Furthermore, starting with a random seed problem, AutoCode can create\nnovel variants with reference and brute-force solutions. By cross-verifying\nthese generated solutions against test cases, we can further filter out\nmalformed problems. Our system ensures high correctness, as verified by human\nexperts. AutoCode successfully produces novel problems judged by\nGrandmaster-level (top 0.3%) competitive programmers to be of contest quality.", "AI": {"tldr": "AutoCode is a system that uses multiple validation rounds to automatically generate competition-grade programming problems and test cases, achieving 99% consistency with official judgments and producing novel problems judged by top programmers as contest quality.", "motivation": "Competitive programming problem creation is challenging and requires setting constraints, input distributions, edge cases, targeting specific algorithms, and calibrating complexity - making it an ideal test for large language model capabilities.", "method": "AutoCode uses multiple rounds of validation to generate problem statements and test cases. It can create novel variants from random seed problems with reference and brute-force solutions, then cross-verifies these solutions against test cases to filter out malformed problems.", "result": "AutoCode achieves 99% consistency with official judgments on held-out problems, significantly outperforming state-of-the-art methods like HardTests (81%). Human experts verify high correctness, and Grandmaster-level programmers judge the generated problems as contest quality.", "conclusion": "AutoCode demonstrates that large language models can reliably generate competition-grade programming problems through systematic validation and cross-verification, producing novel problems that meet professional standards."}}
{"id": "2510.12948", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12948", "abs": "https://arxiv.org/abs/2510.12948", "authors": ["Minh Nguyen"], "title": "SpareCodeSearch: Searching for Code Context When You Have No Spare GPU", "comment": "4 pages, 3 figures, 4 tables. Accepted to Context Collection Workshop\n  co-located with ASE'25", "summary": "Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language\nModels (CLMs) by including another module for retrieving relevant context to\nconstruct the input prompt. However, these retrieval modules commonly use\nsemantic search, requiring substantial computational resources for training and\nhosting these embedded models, making them infeasible to integrate into\nlightweight applications such as in-IDE AI-based code completion. In this\nsolution paper, we prove that using keyword-search is sufficient to retrieve\nrelevant and useful code context inside large codebases, without the need for\nextensive GPU resources. The usefulness of code contexts found by our solution\nis demonstrated through their completion results on the Code Context\nCompetition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and\nPython tracks, respectively.", "AI": {"tldr": "Keyword-search is sufficient for code context retrieval in RAG frameworks, outperforming semantic search in resource efficiency while achieving competitive completion results.", "motivation": "Semantic search in RAG frameworks requires substantial computational resources, making them impractical for lightweight applications like in-IDE code completion.", "method": "Proposed using keyword-search instead of semantic search to retrieve relevant code context from large codebases without extensive GPU resources.", "result": "Achieved 0.748 and 0.725 chRF scores on Kotlin and Python tracks respectively in the Code Context Competition benchmark.", "conclusion": "Keyword-search provides a viable, resource-efficient alternative to semantic search for code context retrieval in RAG frameworks."}}
{"id": "2510.13078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13078", "abs": "https://arxiv.org/abs/2510.13078", "authors": ["Tri Minh-Triet Pham", "Diego Elias Costa", "Weiyi Shang", "Jinqiu Yang"], "title": "ADPerf: Investigating and Testing Performance in Autonomous Driving Systems", "comment": "13 pages, accepted by ASE 2025", "summary": "Obstacle detection is crucial to the operation of autonomous driving systems,\nwhich rely on multiple sensors, such as cameras and LiDARs, combined with code\nlogic and deep learning models to detect obstacles for time-sensitive\ndecisions. Consequently, obstacle detection latency is critical to the safety\nand effectiveness of autonomous driving systems. However, the latency of the\nobstacle detection module and its resilience to various changes in the LiDAR\npoint cloud data are not yet fully understood. In this work, we present the\nfirst comprehensive investigation on measuring and modeling the performance of\nthe obstacle detection modules in two industry-grade autonomous driving\nsystems, i.e., Apollo and Autoware. Learning from this investigation, we\nintroduce ADPerf, a tool that aims to generate realistic point cloud data test\ncases that can expose increased detection latency. Increasing latency decreases\nthe availability of the detected obstacles and stresses the capabilities of\nsubsequent modules in autonomous driving systems, i.e., the modules may be\nnegatively impacted by the increased latency in obstacle detection.\n  We applied ADPerf to stress-test the performance of widely used 3D obstacle\ndetection modules in autonomous driving systems, as well as the propagation of\nsuch tests on trajectory prediction modules. Our evaluation highlights the need\nto conduct performance testing of obstacle detection components, especially 3D\nobstacle detection, as they can be a major bottleneck to increased latency of\nthe autonomous driving system. Such an adverse outcome will also further\npropagate to other modules, reducing the overall reliability of autonomous\ndriving systems.", "AI": {"tldr": "ADPerf is a tool that generates realistic LiDAR point cloud test cases to expose increased detection latency in autonomous driving systems, which can propagate to other modules and reduce overall system reliability.", "motivation": "Obstacle detection latency is critical for autonomous driving safety, but current understanding of detection module performance and resilience to LiDAR data changes is limited.", "method": "Conducted comprehensive investigation of Apollo and Autoware systems, then developed ADPerf tool to generate realistic point cloud test cases that stress detection modules and measure latency impact.", "result": "Evaluation showed that 3D obstacle detection modules can be major latency bottlenecks, and increased detection latency negatively propagates to subsequent modules like trajectory prediction.", "conclusion": "Performance testing of obstacle detection components is essential as they can significantly impact autonomous driving system latency and overall reliability."}}
{"id": "2510.13106", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13106", "abs": "https://arxiv.org/abs/2510.13106", "authors": ["Ruoyu Sun", "Da Song", "Jiayang Song", "Yuheng Huang", "Lei Ma"], "title": "TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models", "comment": "4 pages, 2 figures, To appear in ASE 2025 Demo Track", "summary": "As Large Language Models (LLMs) continue to revolutionize Natural Language\nProcessing (NLP) applications, critical concerns about their trustworthiness\npersist, particularly in safety and robustness. To address these challenges, we\nintroduce TRUSTVIS, an automated evaluation framework that provides a\ncomprehensive assessment of LLM trustworthiness. A key feature of our framework\nis its interactive user interface, designed to offer intuitive visualizations\nof trustworthiness metrics. By integrating well-known perturbation methods like\nAutoDAN and employing majority voting across various evaluation methods,\nTRUSTVIS not only provides reliable results but also makes complex evaluation\nprocesses accessible to users. Preliminary case studies on models like\nVicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our\nframework in identifying safety and robustness vulnerabilities, while the\ninteractive interface allows users to explore results in detail, empowering\ntargeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g", "AI": {"tldr": "TRUSTVIS is an automated evaluation framework with interactive visualizations that assesses LLM trustworthiness using perturbation methods and majority voting across multiple evaluation approaches.", "motivation": "Address critical concerns about LLM trustworthiness, particularly in safety and robustness, as LLMs revolutionize NLP applications.", "method": "Integrates well-known perturbation methods like AutoDAN and employs majority voting across various evaluation methods, with an interactive user interface for intuitive visualizations.", "result": "Preliminary case studies on Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate effectiveness in identifying safety and robustness vulnerabilities.", "conclusion": "TRUSTVIS provides reliable results and makes complex evaluation processes accessible, empowering targeted model improvements through detailed result exploration."}}
{"id": "2510.13128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13128", "abs": "https://arxiv.org/abs/2510.13128", "authors": ["Yujie Liu", "Mingxuan Zhu", "Shengyu Cheng", "Dan Hao"], "title": "Isolating Compiler Bugs through Compilation Steps Analysis", "comment": null, "summary": "Compilers are essential to software systems, and their bugs can propagate to\ndependent software. Ensuring compiler correctness is critical. However,\nisolating compiler bugs remains challenging due to the internal complexity of\ncompiler execution. Existing techniques primarily mutate compilation inputs to\ngenerate passing and failing tests, but often lack causal analysis of internal\nsteps, limiting their effectiveness.\n  To address this limitation, we propose CompSCAN, a novel compiler bug\nisolation technique that applies analysis over the sequence of compilation\nsteps. CompSCAN follows a three-stage process: (1) extracting the array of\ncompilation steps that leads to the original failure, (2) identifying\nbug-causing steps and collecting corresponding compiler code elements, and (3)\ncalculating suspicious scores for each code element and outputting a suspicious\nranking list as the bug isolation result.\n  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that\nCompSCAN outperforms state-of-the-art techniques in both effectiveness and\nefficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the\nTop-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two\nstate-of-the-art compiler bug isolation techniques, CompSCAN achieves relative\nimprovements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /\n49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs\nfaster on average per bug than both baselines.", "AI": {"tldr": "CompSCAN is a novel compiler bug isolation technique that analyzes compilation step sequences to identify bug-causing steps and rank suspicious code elements, outperforming state-of-the-art methods on real-world LLVM and GCC bugs.", "motivation": "Compiler bugs are critical as they propagate to dependent software, but existing techniques lack causal analysis of internal compilation steps, limiting their effectiveness in isolating bugs.", "method": "Three-stage process: (1) extract compilation steps leading to failure, (2) identify bug-causing steps and collect compiler code elements, (3) calculate suspicious scores and output ranked list.", "result": "Evaluated on 185 real-world LLVM and GCC bugs, CompSCAN successfully isolated 50, 85, 100, and 123 bugs within Top-1/3/5/10 ranks respectively, achieving 24-50% improvements over ETEM and ODFL while running faster.", "conclusion": "CompSCAN effectively isolates compiler bugs by analyzing compilation step sequences, demonstrating superior performance and efficiency compared to state-of-the-art techniques."}}
{"id": "2510.13176", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13176", "abs": "https://arxiv.org/abs/2510.13176", "authors": ["Haolin Pan", "Chao Zha", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass selection and phase ordering present a significant challenge in\nachieving optimal program performance, particularly for objectives like code\nsize reduction. Standard compiler heuristics offer general applicability but\noften yield suboptimal, program-specific results due to their one-size-fits-all\nnature. While iterative compilation can find tailored solutions, its\nprohibitive search cost limits practical use. Machine learning approaches\npromise faster inference but frequently struggle with generalization to unseen\nprograms. This paper introduces GRACE, a novel framework for compiler\nauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE\neffectively curtails the search space by leveraging pass synergies and a\nweighted scoring method to generate initial high-quality candidate sequences\nand a pass pool. It then employs contrastive learning, using pass\nsequence-based data augmentation, to create program embeddings that facilitate\nsimilarity-aware clustering. Evolutionary search within these clusters yields a\ncoreset of $k$ specialized pass sequences designed for robust generalization to\nunseen programs. At test time, GRACE efficiently selects the best coreset\nsequence and refines it using lightweight techniques. Experimental results on\nseven diverse datasets show that GRACE reduces LLVM IR instruction count by an\naverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,\nwhile incurring an average tuning time of less than 1s per program,\ndemonstrating its state-of-the-art performance and practical effectiveness.", "AI": {"tldr": "GRACE is a compiler auto-tuning framework that uses pass synergies, contrastive learning, and evolutionary search to generate specialized pass sequences, achieving significant code size reduction with minimal tuning time.", "motivation": "Standard compiler heuristics are suboptimal for program-specific optimization, while iterative compilation is too expensive and machine learning approaches struggle with generalization to unseen programs.", "method": "GRACE leverages pass synergies to generate initial candidates, uses contrastive learning for program embeddings and clustering, performs evolutionary search to create a coreset of specialized sequences, and applies lightweight refinement at test time.", "result": "GRACE reduces LLVM IR instruction count by an average of 10.09-10.19% compared to opt -Oz across seven datasets, with average tuning time under 1s per program.", "conclusion": "GRACE demonstrates state-of-the-art performance in compiler auto-tuning, achieving significant code size optimization with practical efficiency and robust generalization to unseen programs."}}
{"id": "2510.13184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13184", "abs": "https://arxiv.org/abs/2510.13184", "authors": ["Haolin Pan", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines", "comment": null, "summary": "Compiler optimization relies on sequences of passes to improve program\nperformance. Selecting and ordering these passes automatically, known as\ncompiler auto-tuning, is challenging due to the large and complex search space.\nExisting approaches generally assume a linear sequence of passes, a model\ncompatible with legacy compilers but fundamentally misaligned with the\nhierarchical design of the LLVM New Pass Manager. This misalignment prevents\nthem from guaranteeing the generation of syntactically valid optimization\npipelines. In this work, we present a new auto-tuning framework built from the\nground up for the New Pass Manager. We introduce a formal grammar to define the\nspace of valid nested pipelines and a forest-based data structure for their\nnative representation. Upon this foundation, we develop a structure-aware\nGenetic Algorithm whose operators manipulate these forests directly, ensuring\nthat all candidate solutions are valid by construction. The framework first\nmines synergistic pass relationships to guide the search. An optional\nrefinement stage further explores subtle performance variations arising from\ndifferent valid structural arrangements.\n  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The\ndiscovered pipelines achieve an average of 13.62% additional instruction count\nreduction compared to the standard opt -Oz optimization level, showing that our\nframework is capable of navigating this complex, constrained search space to\nidentify valid and effective pass pipelines.", "AI": {"tldr": "A new compiler auto-tuning framework designed specifically for LLVM's New Pass Manager that uses a formal grammar and forest-based representation to ensure syntactically valid optimization pipelines, achieving 13.62% better instruction count reduction than standard optimizations.", "motivation": "Existing compiler auto-tuning approaches assume linear pass sequences, which misaligns with LLVM's hierarchical New Pass Manager design and cannot guarantee syntactically valid optimization pipelines.", "method": "Uses a formal grammar to define valid nested pipelines, forest-based data structures for native representation, and a structure-aware Genetic Algorithm that manipulates forests directly to ensure validity. Includes synergistic pass mining and optional refinement for structural variations.", "result": "Achieves average 13.62% additional instruction count reduction compared to standard opt -Oz optimization level across seven benchmark datasets using LLVM 18.1.6.", "conclusion": "The framework successfully navigates the complex, constrained search space to identify valid and effective pass pipelines, demonstrating significant performance improvements over standard compiler optimizations."}}
{"id": "2510.13423", "categories": ["cs.SE", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.13423", "abs": "https://arxiv.org/abs/2510.13423", "authors": ["Matthew Sottile", "Mohit Tekriwal", "John Sarracino"], "title": "Towards Richer Challenge Problems for Scientific Computing Correctness", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Correctness in scientific computing (SC) is gaining increasing attention in\nthe formal methods (FM) and programming languages (PL) community. Existing\nPL/FM verification techniques struggle with the complexities of realistic SC\napplications. Part of the problem is a lack of a common understanding between\nthe SC and PL/FM communities of machine-verifiable correctness challenges and\ndimensions of correctness in SC applications.\n  To address this gap, we call for specialized challenge problems to inform the\ndevelopment and evaluation of FM/PL verification techniques for correctness in\nSC. These specialized challenges are intended to augment existing problems\nstudied by FM/PL researchers for general programs to ensure the needs of SC\napplications can be met. We propose several dimensions of correctness relevant\nto scientific computing, and discuss some guidelines and criteria for designing\nchallenge problems to evaluate correctness in scientific computing.", "AI": {"tldr": "The paper calls for specialized challenge problems to bridge the gap between scientific computing and formal methods communities, proposing dimensions of correctness and guidelines for evaluating verification techniques in scientific computing.", "motivation": "There is a lack of common understanding between scientific computing (SC) and formal methods/programming languages (FM/PL) communities regarding machine-verifiable correctness challenges, and existing verification techniques struggle with realistic SC applications.", "method": "Proposes specialized challenge problems to inform development and evaluation of FM/PL verification techniques, augmenting existing general program verification approaches with SC-specific requirements.", "result": "The paper identifies several dimensions of correctness relevant to scientific computing and discusses guidelines and criteria for designing appropriate challenge problems.", "conclusion": "Specialized challenge problems are needed to ensure FM/PL verification techniques can effectively address the unique correctness requirements of scientific computing applications."}}
{"id": "2510.13424", "categories": ["cs.SE", "D.2.5; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.13424", "abs": "https://arxiv.org/abs/2510.13424", "authors": ["Alexander C. Wilton"], "title": "Verifying a Sparse Matrix Algorithm Using Symbolic Execution", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Scientific software is, by its very nature, complex. It is mathematical and\nhighly optimized which makes it prone to subtle bugs not as easily detected by\ntraditional testing. We outline how symbolic execution can be used to write\ntests similar to traditional unit tests while providing stronger verification\nguarantees and apply this methodology to a sparse matrix algorithm.", "AI": {"tldr": "Using symbolic execution to test complex scientific software, specifically sparse matrix algorithms, providing stronger verification than traditional unit tests.", "motivation": "Scientific software is complex, mathematical, and highly optimized, making it prone to subtle bugs that traditional testing methods may not easily detect.", "method": "Applying symbolic execution to write tests similar to traditional unit tests, but with enhanced verification capabilities.", "result": "The methodology was successfully applied to a sparse matrix algorithm, demonstrating its effectiveness in detecting subtle bugs.", "conclusion": "Symbolic execution offers a powerful approach for testing scientific software, providing stronger verification guarantees compared to traditional unit testing methods."}}
{"id": "2510.13561", "categories": ["cs.SE", "cs.AI", "68N30"], "pdf": "https://arxiv.org/pdf/2510.13561", "abs": "https://arxiv.org/abs/2510.13561", "authors": ["Peng Di", "Faqiang Chen", "Xiao Bai", "Hongjun Yang", "Qingfeng Li", "Ganglin Wei", "Jian Mou", "Feng Shi", "Keting Chen", "Peng Tang", "Zhitao Shen", "Zheng Li", "Wenhui Shi", "Junwei Guo", "Hang Yu"], "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies", "comment": "23 pages", "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/", "AI": {"tldr": "OpenDerisk is a specialized multi-agent framework for Site Reliability Engineering that outperforms existing solutions in accuracy and efficiency, with proven scalability in large-scale production deployment.", "motivation": "Modern software complexity creates unsustainable operational burden on SRE teams, requiring AI-driven automation that can emulate expert diagnostic reasoning, which existing solutions fail to provide.", "method": "OpenDerisk integrates a diagnostic-native collaboration model, pluggable reasoning engine, knowledge engine, and standardized MCP protocol to enable specialist agents to solve complex multi-domain problems.", "result": "Significantly outperforms state-of-the-art baselines in accuracy and efficiency, with successful large-scale deployment at Ant Group serving over 3,000 daily users across diverse scenarios.", "conclusion": "OpenDerisk provides industrial-grade scalability and practical impact for SRE automation, addressing the gap in specialized diagnostic reasoning frameworks."}}
{"id": "2510.13575", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.13575", "abs": "https://arxiv.org/abs/2510.13575", "authors": ["Han Fu", "Sigrid Eldh", "Kristian Wiklund", "Andreas Ermedahl", "Philipp Haller", "Cyrille Artho"], "title": "Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code", "comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)", "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.", "AI": {"tldr": "LLM-driven automated repair can fix 63% of compilation errors in industrial embedded systems, with 83% of successful fixes being reasonable, and reduces debugging time from hours to under 8 minutes.", "motivation": "Co-development of hardware and software in industrial embedded systems causes frequent compilation errors in CI, and existing repair techniques require test cases which are unavailable for non-compilable code.", "method": "Employed LLM-driven automated repair approach, collected 40,000+ commits from product source code, and assessed performance of CI system enhanced by four state-of-the-art LLMs compared to manual human corrections.", "result": "LLM-equipped CI systems resolved up to 63% of compilation errors, with 83% of successful fixes deemed reasonable, and significantly reduced debugging time to under 8 minutes vs hours for manual debugging.", "conclusion": "LLM-driven automated repair is highly effective for fixing compilation errors in industrial embedded systems, providing substantial time savings and reasonable fix quality."}}
{"id": "2510.13692", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13692", "abs": "https://arxiv.org/abs/2510.13692", "authors": ["Deepak A. Cherian"], "title": "Property Testing for Ocean Models. Can We Specify It? (Invited Talk)", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "I take inspiration from the property-testing literature, particularly the\nwork of Prof. John Hughes, and explore how such ideas might be applied to\nnumerical models of the ocean. Specifically, I ask whether geophysical fluid\ndynamics (GFD) theory, expressed as property tests, might be used to address\nthe oracle problem of testing the correctness of ocean models. I propose that a\nnumber of simple idealized GFD problems can be framed as property tests. These\nexamples clearly illustrate how physics naturally lends itself to specifying\nproperty tests. Which of these proposed tests might be most feasible and\nuseful, remains to be seen.", "AI": {"tldr": "Applying property-testing concepts from computer science to ocean modeling by framing geophysical fluid dynamics theory as property tests to address the oracle problem in model verification.", "motivation": "To explore how property-testing ideas from computer science can be applied to numerical ocean models, specifically using geophysical fluid dynamics theory to solve the oracle problem in testing model correctness.", "method": "Proposing that simple idealized geophysical fluid dynamics problems can be framed as property tests, using physics to naturally specify these tests.", "result": "The examples clearly demonstrate how physics lends itself to specifying property tests, though the feasibility and usefulness of specific tests remain to be determined.", "conclusion": "Property testing concepts show promise for ocean model verification by leveraging geophysical fluid dynamics theory, but further investigation is needed to identify the most practical and valuable applications."}}
{"id": "2510.13697", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13697", "abs": "https://arxiv.org/abs/2510.13697", "authors": ["Maksim Sapronov", "Evgeniy Glukhov"], "title": "On Pretraining for Project-Level Code Completion", "comment": null, "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.", "AI": {"tldr": "Repository-level pretraining enhances code completion by using codebase-wide context. OpenCoder, a 1.5B-parameter model, achieves comparable performance with smaller datasets by extending context windows and using RoPE scaling.", "motivation": "To improve code completion accuracy by leveraging repository-level context and investigate how different repository-processing strategies affect in-context learning.", "method": "Extended context window from 4,096 to 16,384 tokens by training on 1B tokens of curated repository-level data. Used rotary positional embedding (RoPE) scaling and compared various repository-processing techniques.", "result": "Achieved comparable performance on Long Code Arena benchmark despite using smaller dataset than competitors. Found that repository-processing techniques yield similar results, with main gains from RoPE scaling adaptation.", "conclusion": "Simpler file-level training at original sequence length remains effective, making repository-level code completion research accessible to settings with constrained data and compute resources."}}
