{"id": "2510.18895", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18895", "abs": "https://arxiv.org/abs/2510.18895", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation", "comment": "12 pages", "summary": "We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)\narchitecture that integrates affective signals to enhance code generation in\nlarge language models (LLMs). Motivated by human and animal learning where\nembarrassment from mistakes drives rapid correction, as observed in training a\npuppy to avoid repeating errors after a single scolding CosmoCore tags code\ngeneration trajectories with valence and surprise using a lightweight\nmulti-layer perceptron (MLP). High-negative valence (cringe) episodes, such as\nbuggy code outputs, are prioritized in a Dream Queue for five-fold replay\nduring off-policy updates, while low-surprise successes are pruned to prevent\noverconfidence and buffer bloat. Evaluated on code generation benchmarks like\nHumanEval and BigCodeBench, alongside simulations with a custom data pipeline\nenvironment, CosmoCore reduces hallucinated code (e.g., syntax errors or\nlogical bugs) by 48\\% and accelerates self-correction by 45\\%. Local\nexperiments using Hugging Face models in a PySpark environment validate these\ngains, with code snippets provided for replication. Ablations confirm valence\ntagging boosts curiosity in exploration, and pruning mitigates inefficiency.\nThis framework extends RL from human feedback (RLHF) for more emotionally aware\ncode assistants, with applications in IDEs and data pipelines. Code and the\ncustom mini-world simulation are released.", "AI": {"tldr": "CosmoCore is a neuroscience-inspired RL architecture that uses affective signals (valence and surprise) to improve code generation in LLMs, reducing hallucinations by 48% and accelerating self-correction by 45%.", "motivation": "Inspired by human and animal learning where embarrassment from mistakes drives rapid correction, similar to training a puppy to avoid repeating errors after a single scolding.", "method": "Tags code generation trajectories with valence and surprise using a lightweight MLP, prioritizes high-negative valence episodes (cringe) in a Dream Queue for 5x replay during off-policy updates, and prunes low-surprise successes to prevent overconfidence.", "result": "Reduces hallucinated code (syntax errors or logical bugs) by 48% and accelerates self-correction by 45% on benchmarks like HumanEval and BigCodeBench.", "conclusion": "Extends RLHF for more emotionally aware code assistants, with applications in IDEs and data pipelines, and shows valence tagging boosts curiosity while pruning mitigates inefficiency."}}
{"id": "2510.18923", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18923", "abs": "https://arxiv.org/abs/2510.18923", "authors": ["Eduard Frankford", "Tobias Antensteiner", "Michael Vierhauser", "Clemens Sauerwein", "Vivien Wallner", "Iris Groher", "Reinhold Pl\u00f6sch", "Ruth Breu"], "title": "A Survey on Feedback Types in Automated Programming Assessment Systems", "comment": null, "summary": "With the recent rapid increase in digitization across all major industries,\nacquiring programming skills has increased the demand for introductory\nprogramming courses. This has further resulted in universities integrating\nprogramming courses into a wide range of curricula, including not only\ntechnical studies but also business and management fields of study.\n  Consequently, additional resources are needed for teaching, grading, and\ntutoring students with diverse educational backgrounds and skills. As part of\nthis, Automated Programming Assessment Systems (APASs) have emerged, providing\nscalable and high-quality assessment systems with efficient evaluation and\ninstant feedback. Commonly, APASs heavily rely on predefined unit tests for\ngenerating feedback, often limiting the scope and level of detail of feedback\nthat can be provided to students. With the rise of Large Language Models (LLMs)\nin recent years, new opportunities have emerged as these technologies can\nenhance feedback quality and personalization.\n  To investigate how different feedback mechanisms in APASs are perceived by\nstudents, and how effective they are in supporting problem-solving, we have\nconducted a large-scale study with over 200 students from two different\nuniversities. Specifically, we compare baseline Compiler Feedback, standard\nUnit Test Feedback, and advanced LLM-based Feedback regarding perceived quality\nand impact on student performance.\n  Results indicate that while students rate unit test feedback as the most\nhelpful, AI-generated feedback leads to significantly better performances.\nThese findings suggest combining unit tests and AI-driven guidance to optimize\nautomated feedback mechanisms and improve learning outcomes in programming\neducation.", "AI": {"tldr": "This paper compares different automated feedback mechanisms in programming education, finding that while students prefer unit test feedback, AI-generated feedback actually leads to better performance outcomes.", "motivation": "The increasing demand for programming courses across various fields requires scalable assessment systems. Traditional APASs relying on unit tests provide limited feedback, and LLMs offer potential for enhanced feedback quality and personalization.", "method": "Conducted a large-scale study with over 200 students from two universities, comparing three feedback types: Compiler Feedback, Unit Test Feedback, and LLM-based Feedback, evaluating perceived quality and impact on student performance.", "result": "Students rated unit test feedback as most helpful, but AI-generated feedback resulted in significantly better student performances despite lower perceived helpfulness ratings.", "conclusion": "Combining unit tests with AI-driven guidance can optimize automated feedback mechanisms and improve learning outcomes in programming education."}}
{"id": "2510.19035", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19035", "abs": "https://arxiv.org/abs/2510.19035", "authors": ["Amirreza Hosseini", "Amro M. Farid"], "title": "Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory", "comment": null, "summary": "Within the project management context, project scheduling serves as an\nindispensable component, functioning as a fundamental tool for planning,\nmonitoring, controlling, and managing projects more broadly. Although the\nresource-constrained project scheduling problem (RCPSP) lies at the core of\nproject management activities, it remains largely disconnected from the broader\nliterature on model-based systems engineering (MBSE), thereby limiting its\nintegration into the design and management of complex systems. The original\ncontribution of this paper is twofold. First, the paper seeks to reconcile the\nRCPSP with the broader literature and vocabulary of model-based systems\nengineering and hetero-functional graph theory (HFGT). A concrete translation\npipeline from an activity-on-node network to a SysML activity diagram, and then\nto an operand net is constructed. Using this representation, it specializes the\nhetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP\ncontext as a systematic means of HFGT for quantitative analysis and proves that\nthe RCPSP is recoverable as a special case of a broader model. Secondly, on an\nillustrative instance with renewable and non-renewable operands, the\nspecialized HFNMCF, while producing similar schedules, yields explicit\nexplanations of the project states that enable richer monitoring and control.\nOverall, the framework preserves the strengths of the classical RCPSP while\naccommodating real-world constraints and enterprise-level decision processes\nencountered in large, complex megaprojects.", "AI": {"tldr": "This paper bridges resource-constrained project scheduling (RCPSP) with model-based systems engineering using hetero-functional graph theory, creating a translation pipeline and specialized formulation that enables richer project monitoring and control.", "motivation": "To reconcile RCPSP with model-based systems engineering (MBSE) literature and integrate project scheduling into complex systems design and management, addressing the current disconnection between these domains.", "method": "Developed a translation pipeline from activity-on-node networks to SysML activity diagrams and operand nets, then specialized the hetero-functional network minimum-cost flow (HFNMCF) formulation for RCPSP context.", "result": "The specialized HFNMCF formulation produces similar schedules to classical RCPSP but provides explicit explanations of project states, enabling richer monitoring and control capabilities.", "conclusion": "The framework preserves classical RCPSP strengths while accommodating real-world constraints and enterprise-level decision processes in large, complex megaprojects, demonstrating RCPSP as a special case of a broader model."}}
{"id": "2510.19089", "categories": ["cs.SE", "D.2.4; K.6.3"], "pdf": "https://arxiv.org/pdf/2510.19089", "abs": "https://arxiv.org/abs/2510.19089", "authors": ["\u00c9rik Martin-Dorel"], "title": "Docker-based CI/CD for Rocq/OCaml projects", "comment": "26 pages, 17 figures, 3 tables, 16 references", "summary": "This paper presents three closely-related software projects, namely:\ndocker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:\nprovide a high-level description of the available features -- to foster the use\nof a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --\nand document the underlying requirements and the main design choices of these\nthree DevOps tools -- to help their future maintainers.", "AI": {"tldr": "This paper describes three Docker-based DevOps tools (docker-coq, docker-coq-action, docker-keeper) for Coq/OCaml projects, focusing on their features for CI/CD and documenting design choices for maintainers.", "motivation": "To provide Docker-based CI/CD solutions for Coq (now Rocq) and OCaml projects, and to document the tools' requirements and design for future maintenance.", "method": "Developed three interconnected software projects: docker-coq (Docker images), docker-coq-action (GitHub Action), and docker-keeper (automated image building service).", "result": "Created a complete DevOps toolchain that enables continuous integration and deployment for Coq/OCaml projects using Docker containers.", "conclusion": "The paper successfully documents both the available features for users and the underlying design choices for maintainers of these three Docker-based DevOps tools."}}
{"id": "2510.19237", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19237", "abs": "https://arxiv.org/abs/2510.19237", "authors": ["Dongming Jin", "Zhi Jin", "Xiaohong Chen", "Zheng Fang", "Linyu Li", "Shengxin Zhao", "Chuihui Wang", "Hongbin Xiao"], "title": "Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study", "comment": "27 pages, 3 figures", "summary": "Cyber-physical systems (CPSs) are characterized by a deep integration of the\ninformation space and the physical world, which makes the extraction of\nrequirements concerns more challenging. Some automated solutions for\nrequirements concern extraction have been proposed to alleviate the burden on\nrequirements engineers. However, evaluating the effectiveness of these\nsolutions, which relies on fair and comprehensive benchmarks, remains an open\nquestion. To address this gap, we propose ReqEBench, a new CPSs requirements\nconcern extraction benchmark, which contains 2,721 requirements from 12\nreal-world CPSs. ReqEBench offers four advantages. It aligns with real-world\nCPSs requirements in multiple dimensions, e.g., scale and complexity. It covers\ncomprehensive concerns related to CPSs requirements. It undergoes a rigorous\nannotation process. It covers multiple application domains of CPSs, e.g.,\naerospace and healthcare. We conducted a comparative study on three types of\nautomated requirements concern extraction solutions and revealed their\nperformance in real-world CPSs using our ReqEBench. We found that the highest\nF1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze\nfailure cases of popular LLM-based solutions, summarize their shortcomings, and\nprovide ideas for improving their capabilities. We believe ReqEBench will\nfacilitate the evaluation and development of automated requirements concern\nextraction.", "AI": {"tldr": "Proposed ReqEBench, a benchmark for CPS requirements concern extraction with 2,721 requirements from 12 real-world systems, and evaluated automated solutions including GPT-4 which achieved only 0.24 F1 score.", "motivation": "Current automated solutions for requirements concern extraction in CPS lack fair and comprehensive benchmarks for evaluation, making effectiveness assessment challenging.", "method": "Created ReqEBench benchmark with 2,721 requirements from 12 real-world CPS across multiple domains, featuring rigorous annotation process and comprehensive concern coverage. Evaluated three types of automated solutions including LLM-based approaches.", "result": "GPT-4 achieved only 0.24 F1 score in entity concern extraction. Analysis revealed shortcomings in popular LLM-based solutions and provided improvement ideas.", "conclusion": "ReqEBench facilitates evaluation and development of automated requirements concern extraction for CPS, highlighting current limitations and future improvement directions."}}
{"id": "2510.19240", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19240", "abs": "https://arxiv.org/abs/2510.19240", "authors": ["Behnam Agahi", "Hamed Farbeh"], "title": "A General Solution for the Implementation of CI/CD in Embedded Linux Development", "comment": null, "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.", "AI": {"tldr": "This paper presents an integrated infrastructure using the Yocto Project for automated development, building, and testing of Linux-based embedded systems, featuring CI/CD pipelines, Docker isolation, and cache optimization.", "motivation": "Growing need for automated platforms to develop and deploy customized Linux-based operating systems for embedded systems across various industries.", "method": "Three-layer Yocto architecture with main repositories, custom layer (meta-custom), and coordinating manifest layer. Implemented CI/CD pipelines with GitLab CI and Docker isolation. Used local cache server with hashserv, downloads and sstate cache. Tested with QEMU simulator.", "result": "Significantly reduced build time through cache optimization. Verified system functionality and stability through six boot test scenarios in QEMU. Ensured reproducibility and scalability of the infrastructure.", "conclusion": "The proposed design provides a reproducible, scalable infrastructure suitable for industrial and research embedded systems projects, with potential for extension to real-time Linux deployment and further optimization through distributed builds and monitoring systems."}}
{"id": "2510.19254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19254", "abs": "https://arxiv.org/abs/2510.19254", "authors": ["Chong Chen", "Jiachi Chen", "Lingfeng Bao", "David Lo", "Yanlin Wang", "Zhenyu Shan", "Ting Chen", "Guangqiang Yin", "Jianxing Yu", "Zibin Zheng"], "title": "Trace: Securing Smart Contract Repository Against Access Control Vulnerability", "comment": null, "summary": "Smart contract vulnerabilities, particularly improper Access Control that\nallows unauthorized execution of restricted functions, have caused billions of\ndollars in losses. GitHub hosts numerous smart contract repositories containing\nsource code, documentation, and configuration files-these serve as intermediate\ndevelopment artifacts that must be compiled and packaged before deployment.\nThird-party developers often reference, reuse, or fork code from these\nrepositories during custom development. However, if the referenced code\ncontains vulnerabilities, it can introduce significant security risks. Existing\ntools for detecting smart contract vulnerabilities are limited in their ability\nto handle complex repositories, as they typically require the target contract\nto be compilable to generate an abstract representation for further analysis.\nThis paper presents TRACE, a tool designed to secure non-compilable smart\ncontract repositories against access control vulnerabilities. TRACE employs\nLLMs to locate sensitive functions involving critical operations (e.g.,\ntransfer) within the contract and subsequently completes function snippets into\na fully compilable contract. TRACE constructs a function call graph from the\nabstract syntax tree (AST) of the completed contract. It uses the control flow\ngraph (CFG) of each function as node information. The nodes of the sensitive\nfunctions are then analyzed to detect Access Control vulnerabilities.\nExperimental results demonstrate that TRACE outperforms state-of-the-art tools\non an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it\nachieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the\nbest existing tool at 76.9%. On 83 real-world repositories, TRACE achieves\n87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.", "AI": {"tldr": "TRACE is a tool that detects access control vulnerabilities in non-compilable smart contract repositories using LLMs to complete function snippets and analyze control flow graphs.", "motivation": "Smart contract vulnerabilities, especially improper access control, have caused billions in losses. Existing tools can't handle non-compilable repositories, limiting their effectiveness for real-world development scenarios.", "method": "TRACE uses LLMs to locate sensitive functions, completes them into compilable contracts, constructs function call graphs from AST, and analyzes control flow graphs to detect access control vulnerabilities.", "result": "TRACE detected 14/15 CVEs on open-source dataset, achieved 89.2% precision on 5,000 on-chain contracts (vs 76.9% for best existing tool), and 87.0% precision on 83 real-world repositories (vs 14.3% for DeepSeek-R1).", "conclusion": "TRACE significantly outperforms state-of-the-art tools in detecting access control vulnerabilities in smart contract repositories, especially for non-compilable code scenarios."}}
{"id": "2510.19274", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19274", "abs": "https://arxiv.org/abs/2510.19274", "authors": ["Saurabh Chauhan", "Zeeshan Rasheed", "Malik Abdul Sami", "Kai-Kristian Kemell", "Muhammad Waseem", "Zheying Zhang", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems", "comment": "9 Figures, 6Tables", "summary": "This paper presents a system that uses Large Language Models (LLMs)-based\nagents to automate the API-first development of RESTful microservices. This\nsystem helps to create an OpenAPI specification, generate server code from it,\nand refine the code through a feedback loop that analyzes execution logs and\nerror messages. The integration of log analysis enables the LLM to detect and\naddress issues efficiently, reducing the number of iterations required to\nproduce functional and robust services. This study's main goal is to advance\nAPI-first development automation for RESTful web services and test the\ncapability of LLM-based multi-agent systems in supporting the API-first\ndevelopment approach. To test the proposed system's potential, we utilized the\nPRAB benchmark. The results indicate that if we keep the OpenAPI specification\nsmall and focused, LLMs are capable of generating complete functional code with\nbusiness logic that aligns to the specification. The code for the system is\npublicly available at https://github.com/sirbh/code-gen", "AI": {"tldr": "LLM-based agents automate RESTful microservices development by generating OpenAPI specs, creating server code, and refining it through log analysis feedback loops.", "motivation": "To advance API-first development automation for RESTful web services and test LLM-based multi-agent systems' capability in supporting API-first development approach.", "method": "Uses LLM-based agents to create OpenAPI specifications, generate server code, and refine code through feedback loops analyzing execution logs and error messages.", "result": "LLMs can generate complete functional code with business logic aligned to specifications when OpenAPI specs are kept small and focused, as demonstrated using PRAB benchmark.", "conclusion": "The system successfully automates API-first development and shows LLM-based multi-agent systems can effectively support RESTful microservices development through iterative refinement."}}
{"id": "2510.19281", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19281", "abs": "https://arxiv.org/abs/2510.19281", "authors": ["Shubham Joshi"], "title": "An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics", "comment": "15 pages, 10 tables, 9 Figures", "summary": "Objectives: This study aims to investigate the readability and\nunderstandability of bitwise operators in programming, with the main hypothesis\nthat there will be a difference in the performance metrics (response time and\nerror rate) between participants exposed to various bitwise operators related\nquestions and those who are not.\n  Participants: Participants in this human research study include people\nwithout programming background, novice programmers, and university students\nwith varying programming experience (from freshmen to PhD level). There were 23\nparticipants for this study.\n  Study Methods: This study uses an Within-Subjects Experimental Design to\nassess how people with diverse programming backgrounds understand and use\nbitwise operators. Participants complete tasks in JavaScript program, and their\ntask completion time and accuracy of the tasks are recorded for analysis.\n  Findings: The results indicate that operators can be one of the factors\npredicting response time, with a small but significant effect, with R-squared\n0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,\nand Left Shift showed statistical significance in task completion times\ncompared to other operators.\n  Conclusions: While the complexity of bitwise operators did not generally\nresult in longer task completion times, certain operators were found to be less\nintuitive, suggesting the need for further investigation and potential redesign\nfor improved understandability.", "AI": {"tldr": "This study examines how different programming experience levels affect understanding of bitwise operators, finding that operator type can predict response time with small but significant effects.", "motivation": "To investigate the readability and understandability of bitwise operators in programming, testing whether there are differences in performance metrics between participants exposed to various bitwise operators.", "method": "Within-subjects experimental design with 23 participants of varying programming backgrounds (from no experience to PhD level), using JavaScript tasks to measure completion time and accuracy.", "result": "Operators can predict response time with R-squared 0.032, F(1,494)=16.5, p<.001. OR, NOT, and Left Shift operators showed statistical significance in task completion times compared to other operators.", "conclusion": "While bitwise operator complexity didn't generally increase completion times, certain operators were less intuitive, suggesting need for further investigation and potential redesign for improved understandability."}}
{"id": "2510.19393", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19393", "abs": "https://arxiv.org/abs/2510.19393", "authors": ["Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Jonas Klauke", "Eric Bodden"], "title": "Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects", "comment": "To be published in: ICSE 2026 Proceedings", "summary": "On average, 71% of the code in typical Java projects comes from open-source\nsoftware (OSS) dependencies, making OSS dependencies the dominant component of\nmodern software code bases. This high degree of OSS reliance comes with a\nconsiderable security risk of adding known security vulnerabilities to a code\nbase. To remedy this risk, researchers and companies have developed various\ndependency scanners, which try to identify inclusions of known-to-be-vulnerable\nOSS dependencies. However, there are still challenges that modern dependency\nscanners do not overcome, especially when it comes to dependency modifications,\nsuch as re-compilations, re-bundlings or re-packagings, which are common in the\nJava ecosystem. To overcome these challenges, we present Jaralyzer, a\nbytecode-centric dependency scanner for Java. Jaralyzer does not rely on the\nmetadata or the source code of the included OSS dependencies being available\nbut directly analyzes a dependency's bytecode. Our evaluation across 56 popular\nOSS components demonstrates that Jaralyzer outperforms other popular dependency\nscanners in detecting vulnerabilities within modified dependencies. It is the\nonly scanner capable of identifying vulnerabilities across all the above\nmentioned types of modifications. But even when applied to unmodified\ndependencies, Jaralyzer outperforms the current state-of-the-art code-centric\nscanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding\n29 fewer false warnings.", "AI": {"tldr": "Jaralyzer is a bytecode-centric dependency scanner for Java that outperforms existing scanners in detecting vulnerabilities, especially in modified dependencies like re-compilations, re-bundlings, and re-packagings.", "motivation": "71% of code in typical Java projects comes from OSS dependencies, creating significant security risks. Current dependency scanners fail to detect vulnerabilities in modified dependencies common in the Java ecosystem.", "method": "Jaralyzer analyzes bytecode directly instead of relying on metadata or source code, making it effective for detecting vulnerabilities in modified dependencies.", "result": "Evaluation across 56 popular OSS components shows Jaralyzer outperforms other scanners, detecting 28 more true vulnerabilities and yielding 29 fewer false warnings compared to Eclipse Steady. It's the only scanner capable of identifying vulnerabilities across all types of modifications.", "conclusion": "Jaralyzer provides superior vulnerability detection for Java OSS dependencies, especially for modified dependencies, addressing critical gaps in current dependency scanning approaches."}}
{"id": "2510.19438", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19438", "abs": "https://arxiv.org/abs/2510.19438", "authors": ["Linfeng Liang", "Chenkai Tan", "Yao Deng", "Yingfeng Cai", "T. Y Chen", "Xi Zheng"], "title": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems", "comment": null, "summary": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be\nsevere. While Metamorphic Testing (MT) is effective for fault detection in ADS,\nexisting methods rely heavily on manual effort and lack automation. We present\nAutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that\nautomates the extraction of Metamorphic Relations (MRs) from local traffic\nrules and the generation of valid follow-up test cases. AutoMT leverages LLMs\nto extract MRs from traffic rules in Gherkin syntax using a predefined\nontology. A vision-language agent analyzes scenarios, and a search agent\nretrieves suitable MRs from a RAG-based database to generate follow-up cases\nvia computer vision. Experiments show that AutoMT achieves up to 5 x higher\ntest diversity in follow-up case generation compared to the best baseline\n(manual expert-defined MRs) in terms of validation rate, and detects up to\n20.55% more behavioral violations. While manual MT relies on a fixed set of\npredefined rules, AutoMT automatically extracts diverse metamorphic relations\nthat augment real-world datasets and help uncover corner cases often missed\nduring in-field testing and data collection. Its modular architecture\nseparating MR extraction, filtering, and test generation supports integration\ninto industrial pipelines and potentially enables simulation-based testing to\nsystematically cover underrepresented or safety-critical scenarios.", "AI": {"tldr": "AutoMT is an automated metamorphic testing framework for autonomous driving systems that uses multi-agent LLMs to extract metamorphic relations from traffic rules and generate diverse test cases, achieving higher test diversity and detecting more behavioral violations than manual approaches.", "motivation": "Existing metamorphic testing methods for autonomous driving systems rely heavily on manual effort and lack automation, limiting their effectiveness in detecting safety-critical failures.", "method": "AutoMT uses a multi-agent framework with LLMs: one agent extracts metamorphic relations from traffic rules using Gherkin syntax and ontology, a vision-language agent analyzes scenarios, and a search agent retrieves suitable relations from a RAG database to generate follow-up test cases via computer vision.", "result": "AutoMT achieves up to 5x higher test diversity in follow-up case generation compared to manual expert-defined MRs, with higher validation rates, and detects up to 20.55% more behavioral violations.", "conclusion": "AutoMT's automated approach extracts diverse metamorphic relations that uncover corner cases missed during in-field testing, and its modular architecture supports integration into industrial pipelines for systematic testing of safety-critical scenarios."}}
{"id": "2510.19460", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19460", "abs": "https://arxiv.org/abs/2510.19460", "authors": ["Thomas I. Strasser", "Edmund Widl", "Carlos Ayon Mac Gregor", "Mirko Ginocchi", "Rene Kuchenbuch"], "title": "Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective", "comment": "2025 IEEE PES Innovative Smart Grid Technologies Conference Europe\n  (ISGT Europe)", "summary": "The ongoing transformation of the European energy landscape, driven by the\nintegration of renewable energy sources, digital technologies, and\ndecentralized systems, requires a high degree of interoperability across\ndiverse components and systems. Ensuring that these elements can exchange\ninformation and operate together reliably is essential for achieving a secure,\nflexible, and efficient energy supply infrastructure. While several initiatives\nhave contributed to the development of smart grid testing infrastructures, they\ndo not provide a dedicated or comprehensive focus on interoperability testing.\nA structured and harmonized overview of interoperability testing capabilities\nacross Europe is therefore still missing. This work therefore presents a novel\ncontribution by analyzing the European interoperability testing facility\nlandscape through a structured survey of 30 facilities. It provides a\ncategorized inventory of testing infrastructures, applied methodologies, and\nreference test cases, and introduces a blueprint for the development of future\ntesting environments. The findings contribute to the establishment of a\ncoordinated European ecosystem for interoperability testing, supporting\ncollaboration, innovation, and alignment with the goals of the energy\ntransition.", "AI": {"tldr": "Analysis of 30 European interoperability testing facilities for smart grids, providing a categorized inventory and blueprint for future testing environments to support energy transition goals.", "motivation": "The European energy transition requires high interoperability across renewable energy systems, but existing initiatives lack dedicated focus on interoperability testing, creating a gap in structured testing capabilities.", "method": "Conducted a structured survey of 30 European interoperability testing facilities, analyzing testing infrastructures, methodologies, and reference test cases.", "result": "Created a comprehensive inventory and categorization of testing capabilities across Europe, identifying gaps and opportunities for coordination.", "conclusion": "The study provides a blueprint for developing future testing environments and contributes to establishing a coordinated European ecosystem for interoperability testing to support energy transition collaboration and innovation."}}
{"id": "2510.19593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19593", "abs": "https://arxiv.org/abs/2510.19593", "authors": ["Aoyang Fang", "Haowen Yang", "Haoze Dong", "Qisheng Lu", "Junjielong Xu", "Pinjia He"], "title": "A Goal-Driven Survey on Root Cause Analysis", "comment": null, "summary": "Root Cause Analysis (RCA) is a crucial aspect of incident management in\nlarge-scale cloud services. While the term root cause analysis or RCA has been\nwidely used, different studies formulate the task differently. This is because\nthe term \"RCA\" implicitly covers tasks with distinct underlying goals. For\ninstance, the goal of localizing a faulty service for rapid triage is\nfundamentally different from identifying a specific functional bug for a\ndefinitive fix. However, previous surveys have largely overlooked these\ngoal-based distinctions, conventionally categorizing papers by input data types\n(e.g., metric-based vs. trace-based methods). This leads to the grouping of\nworks with disparate objectives, thereby obscuring the true progress and gaps\nin the field. Meanwhile, the typical audience of an RCA survey is either laymen\nwho want to know the goals and big picture of the task or RCA researchers who\nwant to figure out past research under the same task formulation. Thus, an RCA\nsurvey that organizes the related papers according to their goals is in high\ndemand. To this end, this paper presents a goal-driven framework that\neffectively categorizes and integrates 135 papers on RCA in the context of\ncloud incident management based on their diverse goals, spanning the period\nfrom 2014 to 2025. In addition to the goal-driven categorization, it discusses\nthe ultimate goal of all RCA papers as an umbrella covering different RCA\nformulations. Moreover, the paper discusses open challenges and future\ndirections in RCA.", "AI": {"tldr": "This paper presents a goal-driven framework for categorizing 135 Root Cause Analysis (RCA) papers in cloud incident management, addressing limitations of previous surveys that grouped papers by data types rather than their distinct underlying goals.", "motivation": "Previous RCA surveys have overlooked goal-based distinctions, grouping papers with disparate objectives by input data types, which obscures true progress and gaps in the field. There's a need for a survey that organizes papers according to their goals to better serve both laymen and researchers.", "method": "The paper develops a goal-driven framework that categorizes and integrates 135 RCA papers from 2014-2025 based on their diverse goals in cloud incident management, discussing the ultimate goal of all RCA papers as an umbrella concept.", "result": "The framework effectively categorizes RCA papers according to their goals, providing a clearer understanding of the field's progress and identifying gaps that were previously obscured by data-type-based categorization.", "conclusion": "A goal-driven approach to organizing RCA literature provides more meaningful insights than traditional data-type-based categorization, better serving the needs of both newcomers and experienced researchers in understanding the field's true progress and remaining challenges."}}
{"id": "2510.19600", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19600", "abs": "https://arxiv.org/abs/2510.19600", "authors": ["Qianli Ma", "Siyu Wang", "Yilin Chen", "Yinhao Tang", "Yixiang Yang", "Chang Guo", "Bingjie Gao", "Zhening Xing", "Yanan Sun", "Zhipeng Zhang"], "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1", "comment": null, "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.", "AI": {"tldr": "AutoPage is a multi-agent system that automatically converts research papers into interactive webpages through a hierarchical pipeline with verification agents to ensure accuracy.", "motivation": "Researchers spend excessive time manually creating project webpages to make dense papers accessible, and existing automation tools don't handle the dynamic, interactive nature of webpages.", "method": "A multi-agent system with coarse-to-fine pipeline: narrative planning, multimodal content generation, interactive rendering, plus dedicated Checker agents for verification against source papers and optional human checkpoints.", "result": "AutoPage generates high-quality, visually appealing webpages in under 15 minutes for less than $0.1, with PageBench benchmark showing strong performance.", "conclusion": "AutoPage successfully transforms paper-to-page creation from manual chore to efficient collaboration, serving as a powerful assistant rather than just a tool."}}
{"id": "2510.19615", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19615", "abs": "https://arxiv.org/abs/2510.19615", "authors": ["Zhiping Zhou", "Xiaohong Li", "Ruitao Feng", "Yao Zhang", "Yuekang Li", "Wenbu Feng", "Yunqian Wang", "Yuqing Li"], "title": "FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation", "comment": null, "summary": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering.", "AI": {"tldr": "FidelityGPT is a framework that improves decompiled code accuracy and readability by detecting and correcting semantic distortions using distortion-aware prompts, RAG with dynamic semantic intensity, and variable dependency analysis.", "motivation": "Existing decompilation methods have fidelity issues that degrade readability and semantic accuracy, especially for complex closed-source binaries. Current approaches like variable renaming provide only partial improvements and lack robust detection and correction mechanisms.", "method": "Uses distortion-aware prompt templates for closed-source settings, integrates Retrieval-Augmented Generation (RAG) with dynamic semantic intensity algorithm to locate distorted lines and retrieve similar code, and employs variable dependency algorithm to handle long-context limitations.", "result": "Achieved 89% average detection accuracy and 83% precision on 620 function pairs. Outperformed state-of-the-art DeGPT with 94% Fix Rate (vs 83%) and 64% Corrected Fix Rate (vs 37%).", "conclusion": "FidelityGPT demonstrates significant improvements in decompilation accuracy and readability, showing strong potential to advance LLM-based decompilation and reverse engineering."}}
{"id": "2510.19692", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19692", "abs": "https://arxiv.org/abs/2510.19692", "authors": ["Rashina Hoda"], "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary", "comment": "5 pages", "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run.", "AI": {"tldr": "Agentic AI will transform Software Engineering, requiring expansion beyond code-focused approaches to address socio-technical concerns and establish strong foundations for agentic SE.", "motivation": "As technologists rush to develop agentic AI, SE researchers need to establish agentic SE as a research area and address practical socio-technical concerns beyond just code activities.", "method": "The paper proposes: (a) expanding scope to 'whole of process' vision grounded in SE foundations, (b) establishing values and principles to guide efforts, and (c) providing guidance on well-defined vocabulary for agentic SE.", "result": "The paper contributes to the emerging community vision by providing foundational guidance for agentic SE research and practice.", "conclusion": "These ideas aim to encourage community collaborations and steer SE toward establishing strong foundations for agentic SE, making it deliberate and desirable rather than just inevitable."}}
{"id": "2510.19747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19747", "abs": "https://arxiv.org/abs/2510.19747", "authors": ["Priyaranjan Pattnayak", "Hussain Bohra"], "title": "Review of Tools for Zero-Code LLM Based Application Development", "comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)", "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.", "AI": {"tldr": "Survey of LLM-powered zero-code development platforms, categorizing them by interface style, backend integration, output type, and extensibility, comparing dedicated LLM app builders with general no-code platforms.", "motivation": "To understand how Large Language Models are transforming software creation through zero-code development platforms, enabling users to build applications without writing code.", "method": "Broad survey methodology categorizing platforms based on key dimensions: interface style, backend integration, output type, and extensibility. Analysis includes both dedicated LLM-based app builders and general no-code platforms with LLM integration.", "result": "Created a taxonomy categorizing platforms by interface (conversational, visual), supported LLM backends, output type (chatbot, full application, workflow), and extensibility. Found that while zero-code LLM platforms reduce barriers to creating AI-powered applications, they face challenges in flexibility and reliability.", "conclusion": "The landscape of LLM-powered zero-code platforms is rapidly evolving, offering opportunities to empower non-programmers to create sophisticated software, though trade-offs exist in customizability, scalability, and vendor lock-in compared to traditional approaches."}}
{"id": "2510.19777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19777", "abs": "https://arxiv.org/abs/2510.19777", "authors": ["S M Sadrul Islam Asif", "James Chen", "Earl T. Barr", "Mark Marron"], "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation", "comment": null, "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.", "AI": {"tldr": "BOSQTGEN is a black-box API test generation tool that uses LLMs to decompose API specifications into primitives and combinatorial testing to efficiently sample values, achieving 82% code coverage on RESTful benchmarks.", "motivation": "Modern software relies heavily on API composition, but inadequate contracts lead to failures. Current test generation faces challenges with polyglot systems, source code inaccessibility, cost-reliability trade-offs, and generating structured inputs.", "method": "Decomposes API specifications into primitives, uses LLMs to suggest coherent strata for them, and employs combinatorial testing to efficiently sample over these values to ensure coverage of critical interactions.", "result": "Achieves average 82% code coverage on RESTful benchmarks, often 20%+ increase over prior state-of-the-art systems, nearing parity with hand-written test suites.", "conclusion": "Provides a fully API-driven approach to automatically create high-quality test cases for validation or test-driven development, addressing key challenges in API conformance testing."}}
