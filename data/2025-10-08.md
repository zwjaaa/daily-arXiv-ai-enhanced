<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: A reinforcement learning framework for adaptive configuration allocation in software testing that outperforms static methods by combining Q-learning with hybrid rewards and online-offline training.


<details>
  <summary>Details</summary>
Motivation: Existing combinatorial optimization approaches are static and poorly suited for non-stationary testing environments where failure probabilities drift over time, requiring adaptive resource allocation.

Method: Reinforcement learning framework with Q-learning, hybrid reward design (simulated outcomes + real-time feedback), and adaptive online-offline training scheme for tracking probability shifts.

Result: Extensive simulations show consistent outperformance over static and optimization-based baselines, approaching oracle performance in dynamic testing scenarios.

Conclusion: Establishes RL as a powerful new paradigm for adaptive configuration allocation, advancing beyond traditional methods with broad applicability to dynamic testing and resource scheduling.

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [2] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard is a framework providing formal safety guarantees for LLM-based agents through offline policy validation and online action monitoring.


<details>
  <summary>Details</summary>
Motivation: Autonomous AI agents in sensitive domains like healthcare pose critical safety, security, and privacy risks, including deviation from objectives, policy violations, and adversarial attacks that existing systems don't fully address.

Method: Dual-stage architecture: 1) Offline stage with user intent clarification, safety specification establishment, policy synthesis, and formal verification; 2) Online stage with runtime monitoring of actions against pre-verified policies.

Result: The framework enables formal safety guarantees to be practically applied, separating exhaustive offline validation from lightweight online monitoring.

Conclusion: VeriGuard substantially improves the trustworthiness of LLM agents by providing robust safeguards through verifiable correctness guarantees.

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [3] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: LLMs show limited reasoning in software testing - they reproduce patterns but fail with semantic changes, though structured technical elements help more than narrative descriptions.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLM reasoning capabilities in test case generation beyond memorization, using Bloom's taxonomy to assess cognitive layers.

Method: Evaluated StarCoder and GPT-4o on Defects4J, GHRB, and mutated datasets with linguistic/semantic challenges, using LIBRO framework and Bloom's taxonomy cognitive layers.

Result: Models reproduce prior results (Remember) but suffer 60%+ performance drops with identifier mutations (Apply). Few-shot examples triple success rates, and technical elements are more impactful than narratives.

Conclusion: LLMs have limited reasoning in test generation, but structured evaluation reveals cognitive processes and suggests performance improvement directions.

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [4] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: The paper introduces data-driven RSE personas to identify common patterns in Research Software Engineering development using repository mining, applied to 115,174 contributors across 1,284 GitHub repositories.


<details>
  <summary>Details</summary>
Motivation: To help individuals and research software project teams understand their contributions, impact, and repository dynamics, which is important for improving Research Software Engineering practices.

Method: Combines software repository mining and data-driven personas applied to research software repositories on GitHub, analyzing collaborative interaction behaviors of contributors to mid-sized public repositories (10-300 committers).

Result: Identified and characterized seven distinct RSE personas from low to high interactivity: Ephemeral Contributor, Occasional Contributor, Project Organiser, Moderate Contributor, Low-Process Closer, Low-Coding Closer, and Active Contributor.

Conclusion: The method successfully analyzes large datasets despite challenges in comparing software projects with different management factors, research domains, and contributor backgrounds, demonstrating its effectiveness for characterizing RSE development patterns.

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [5] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX is an open-source AI multi-agent system that automatically generates unit tests for legacy code using AI agents, formal methods, and LLMs to improve test coverage and critical value testing.


<details>
  <summary>Details</summary>
Motivation: Address challenges of complex and legacy codebases where manual test generation is difficult, and improve software reliability and maintainability despite LLM limitations in bug detection.

Method: Leverages combination of AI agents, formal methods, and Large Language Models (LLMs) to automate test generation for legacy code.

Result: Effective in generating high-quality tests, identifying potential issues, and enhancing readability and documentation of legacy code.

Conclusion: UnitTenX provides a robust framework for improving software reliability and maintainability through automated test generation for legacy systems.

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [6] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: LLM-generated code review comments are often actionable, with readability, bug, and maintainability comments having higher resolution rates than design-focused comments. LLM and human reviewers show complementary strengths.


<details>
  <summary>Details</summary>
Motivation: To understand which types of LLM-generated code review comments are most likely to trigger actual code changes by developers, and to compare the effectiveness of LLM vs human reviewers.

Method: Developed an LLM-as-a-Judge system to automatically classify review comments into five categories based on a custom taxonomy, then analyzed resolution rates across comment types.

Result: Readability, bugs, and maintainability-related comments had higher resolution rates than code design comments. LLM and human reviewers exhibit distinct strengths depending on project context.

Conclusion: A substantial proportion of LLM-generated comments are actionable and can be resolved by developers. The work highlights complementarity between LLM and human reviewers and provides suggestions for improving LLM-powered code review tools.

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [7] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: Analysis of SECURITY.md files in open-source projects reveals that 79.5% of related issues are requests to add the file, and reports with links are resolved 2 days faster.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness and operational challenges of SECURITY.md files in vulnerability reporting within open-source communities.

Method: Classified and analyzed content of 711 randomly sampled SECURITY.md-related issues, and conducted quantitative comparative analysis of close time and response counts for six community health files.

Result: 79.5% of SECURITY.md issues were requests to add the file; reports with links closed 2 days faster on median; SECURITY.md had shorter close times compared to other community files.

Conclusion: Findings provide practical insights for improving security reporting policies and community management, contributing to a more secure open-source ecosystem.

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [8] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: The Software Observatory at OpenEBench is a web portal that aggregates software metadata to analyze trends in Life Sciences research software and evaluate adherence to FAIR principles.


<details>
  <summary>Details</summary>
Motivation: To help the scientific community understand current trends in research software development and identify gaps that could hinder scientific progress by using FAIR principles as a proxy.

Method: Developed a web portal that consolidates software metadata from various sources and includes a FAIRsoft Evaluator component to assess software according to FAIR principles for research software.

Result: The platform enables analysis of trends, identification of patterns and advancements in Life Sciences research software ecosystem, and provides FAIRness scores for different indicators at various granularity levels.

Conclusion: The Software Observatory serves as a valuable resource for researchers, developers, and stakeholders to promote better software development practices and adherence to FAIR principles.

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [9] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: Digital twins can improve software engineering processes by better representing, understanding, and optimizing them to help software experts use their time efficiently and support domain experts in producing high-quality software.


<details>
  <summary>Details</summary>
Motivation: Software engineering is complex with stakeholders from many domains collaborating, and there's a shortage of skilled software engineers. Digital twins can help address these challenges by providing better process representation and optimization.

Method: The paper outlines what a software engineering digital twin could look like and discusses the requirements for realizing and deploying such digital twins.

Result: The paper presents a vision for how digital twins could benefit software engineering processes but does not report on implemented results since it's outlining the concept and requirements.

Conclusion: Digital twins show promise for improving software engineering processes, but there are missing components and challenges that need to be addressed for their successful realization and deployment.

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [10] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum models are 4B-parameter code completion models designed for JetBrains IDEs, featuring careful data curation, staged training, and editor-critical capabilities to meet interactive use requirements.


<details>
  <summary>Details</summary>
Motivation: To create high-quality, cost-effective code completion models that can operate within the latency constraints of interactive IDE use while maintaining production quality for hundreds of thousands of users.

Method: End-to-end industrial pipeline including disciplined data governance, multi-stage training (pre-training on 4T tokens, fill-in-the-middle, project context via supervised fine-tuning), and alignment via direct preference optimization using real-world feedback.

Result: Mellum models demonstrate that careful data curation and staged training significantly improve quality, and compact task-focused models can meet cost/latency constraints while providing high-quality suggestions in production environments.

Conclusion: The Mellum project provides a pragmatic blueprint for taking focused, open models from research prototype to large-scale production deployment, with models released under Apache-2.0 license for community use.

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [11] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: Study at Ericsson shows remote onboarding during COVID-19 led to higher resignation rates, especially for employees with <5 years tenure. Hybrid work with in-office presence for new hires and mentors improves retention.


<details>
  <summary>Details</summary>
Motivation: COVID-19 normalized remote work but created challenges for software teams. Need to understand how different work modalities (onsite, remote, hybrid) affect employee retention patterns.

Method: Analyzed HR data from Ericsson Sweden (2016-2025) to examine resignation patterns before, during, and after pandemic. Studied work modalities and used exit surveys to understand organizational attachment.

Result: Significant increase in resignations from 2021-2023, especially among employees with <5 years tenure. Remote-onboarded employees were more likely to resign within first 3 years, even after returning to office. Exit surveys indicated remote onboarding failed to establish organizational attachment.

Conclusion: Hybrid models with selective in-office requirements for new hires, supported by team members and senior staff mentorship, can sustain retention. Organizational attachment and integration practices are crucial in hybrid environments.

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [12] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: Patterns for building LLM-powered reporting systems that decouple query generation from data retrieval using dual-response patterns with ResourceLink.


<details>
  <summary>Details</summary>
Motivation: Context window limitations prevent direct deployment of LLMs in reporting systems where complete datasets exceed available tokens, and practical implementation patterns for scalable architectures are undocumented.

Method: Introduces dual-response pattern extending ResourceLink to support iterative query refinement and out-of-band data access, with patterns for multi-tenant security and resource lifecycle management.

Result: Provides practical patterns that address fundamental challenges in LLM-driven reporting applications.

Conclusion: These patterns offer practical guidance for developers building scalable LLM-powered reporting systems that overcome context window limitations.

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [13] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: A systematic study of how software engineers integrate GenAI tools into their workflows, revealing that while code generation is universal, proficiency correlates with using AI for nuanced tasks like debugging and code review.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on individual prompt engineering techniques rather than software developers' broader workflows with GenAI tools.

Method: Large-scale survey of 91 software engineers (72 active GenAI users) examining prompting strategies, conversation patterns, and reliability assessments across various software engineering tasks.

Result: 14 key findings show code generation is nearly universal, proficiency correlates with using AI for nuanced tasks, developers prefer iterative multi-turn conversations, documentation tasks are most reliable, while complex code generation and debugging present challenges.

Conclusion: Provides an empirical baseline of current developer practices from simple code generation to deeper workflow integration, with actionable insights for future improvements.

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [14] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: LLMs can translate fault-prediction metrics into human-readable risk explanations to help OSS contributors make safer code changes.


<details>
  <summary>Details</summary>
Motivation: OSS contributors face challenges interpreting static analysis metrics when making code changes, especially in complex object-oriented systems with interdependencies.

Method: Investigate using LLMs to translate fault-prediction metrics into three types of explanations: descriptive, contextual, and actionable guidance.

Result: The paper outlines explanation types and proposes a task-based study to assess usefulness compared to metric-only baselines.

Conclusion: LLMs show promise for improving OSS contribution safety by providing clear risk explanations and actionable guidance for code modifications.

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [15] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: LLMs can fix uncompilable student code while preserving structural intent, enabling better student modeling in CS1 courses.


<details>
  <summary>Details</summary>
Motivation: Many student programming submissions are uncompilable and excluded from analysis, limiting understanding of student learning processes.

Method: Used LLMs (GPT-5, Claude 3.5, Gemini 2.5) with high/low-context prompting to repair uncompilable code, evaluating compilability, edit distance, and structural preservation.

Result: All LLMs produced compilable repairs but differed in preserving students' control flow and code structure, affecting pedagogical utility.

Conclusion: Automated program repair enables richer analysis of learners' coding processes by recovering uncompilable submissions for student modeling.

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>
