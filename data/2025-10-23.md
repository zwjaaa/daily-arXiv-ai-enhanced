<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore is a neuroscience-inspired RL architecture that uses affective signals (valence and surprise) to improve code generation in LLMs, reducing hallucinations by 48% and accelerating self-correction by 45%.


<details>
  <summary>Details</summary>
Motivation: Inspired by human and animal learning where embarrassment from mistakes drives rapid correction, similar to training a puppy to avoid repeating errors after a single scolding.

Method: Tags code generation trajectories with valence and surprise using a lightweight MLP, prioritizes high-negative valence episodes (cringe) in a Dream Queue for 5x replay during off-policy updates, and prunes low-surprise successes to prevent overconfidence.

Result: Reduces hallucinated code (syntax errors or logical bugs) by 48% and accelerates self-correction by 45% on benchmarks like HumanEval and BigCodeBench.

Conclusion: Extends RLHF for more emotionally aware code assistants, with applications in IDEs and data pipelines, and shows valence tagging boosts curiosity while pruning mitigates inefficiency.

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [2] [A Survey on Feedback Types in Automated Programming Assessment Systems](https://arxiv.org/abs/2510.18923)
*Eduard Frankford,Tobias Antensteiner,Michael Vierhauser,Clemens Sauerwein,Vivien Wallner,Iris Groher,Reinhold Plösch,Ruth Breu*

Main category: cs.SE

TL;DR: This paper compares different automated feedback mechanisms in programming education, finding that while students prefer unit test feedback, AI-generated feedback actually leads to better performance outcomes.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for programming courses across various fields requires scalable assessment systems. Traditional APASs relying on unit tests provide limited feedback, and LLMs offer potential for enhanced feedback quality and personalization.

Method: Conducted a large-scale study with over 200 students from two universities, comparing three feedback types: Compiler Feedback, Unit Test Feedback, and LLM-based Feedback, evaluating perceived quality and impact on student performance.

Result: Students rated unit test feedback as most helpful, but AI-generated feedback resulted in significantly better student performances despite lower perceived helpfulness ratings.

Conclusion: Combining unit tests with AI-driven guidance can optimize automated feedback mechanisms and improve learning outcomes in programming education.

Abstract: With the recent rapid increase in digitization across all major industries,
acquiring programming skills has increased the demand for introductory
programming courses. This has further resulted in universities integrating
programming courses into a wide range of curricula, including not only
technical studies but also business and management fields of study.
  Consequently, additional resources are needed for teaching, grading, and
tutoring students with diverse educational backgrounds and skills. As part of
this, Automated Programming Assessment Systems (APASs) have emerged, providing
scalable and high-quality assessment systems with efficient evaluation and
instant feedback. Commonly, APASs heavily rely on predefined unit tests for
generating feedback, often limiting the scope and level of detail of feedback
that can be provided to students. With the rise of Large Language Models (LLMs)
in recent years, new opportunities have emerged as these technologies can
enhance feedback quality and personalization.
  To investigate how different feedback mechanisms in APASs are perceived by
students, and how effective they are in supporting problem-solving, we have
conducted a large-scale study with over 200 students from two different
universities. Specifically, we compare baseline Compiler Feedback, standard
Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality
and impact on student performance.
  Results indicate that while students rate unit test feedback as the most
helpful, AI-generated feedback leads to significantly better performances.
These findings suggest combining unit tests and AI-driven guidance to optimize
automated feedback mechanisms and improve learning outcomes in programming
education.

</details>


### [3] [Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory](https://arxiv.org/abs/2510.19035)
*Amirreza Hosseini,Amro M. Farid*

Main category: cs.SE

TL;DR: This paper bridges resource-constrained project scheduling (RCPSP) with model-based systems engineering using hetero-functional graph theory, creating a translation pipeline and specialized formulation that enables richer project monitoring and control.


<details>
  <summary>Details</summary>
Motivation: To reconcile RCPSP with model-based systems engineering (MBSE) literature and integrate project scheduling into complex systems design and management, addressing the current disconnection between these domains.

Method: Developed a translation pipeline from activity-on-node networks to SysML activity diagrams and operand nets, then specialized the hetero-functional network minimum-cost flow (HFNMCF) formulation for RCPSP context.

Result: The specialized HFNMCF formulation produces similar schedules to classical RCPSP but provides explicit explanations of project states, enabling richer monitoring and control capabilities.

Conclusion: The framework preserves classical RCPSP strengths while accommodating real-world constraints and enterprise-level decision processes in large, complex megaprojects, demonstrating RCPSP as a special case of a broader model.

Abstract: Within the project management context, project scheduling serves as an
indispensable component, functioning as a fundamental tool for planning,
monitoring, controlling, and managing projects more broadly. Although the
resource-constrained project scheduling problem (RCPSP) lies at the core of
project management activities, it remains largely disconnected from the broader
literature on model-based systems engineering (MBSE), thereby limiting its
integration into the design and management of complex systems. The original
contribution of this paper is twofold. First, the paper seeks to reconcile the
RCPSP with the broader literature and vocabulary of model-based systems
engineering and hetero-functional graph theory (HFGT). A concrete translation
pipeline from an activity-on-node network to a SysML activity diagram, and then
to an operand net is constructed. Using this representation, it specializes the
hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP
context as a systematic means of HFGT for quantitative analysis and proves that
the RCPSP is recoverable as a special case of a broader model. Secondly, on an
illustrative instance with renewable and non-renewable operands, the
specialized HFNMCF, while producing similar schedules, yields explicit
explanations of the project states that enable richer monitoring and control.
Overall, the framework preserves the strengths of the classical RCPSP while
accommodating real-world constraints and enterprise-level decision processes
encountered in large, complex megaprojects.

</details>


### [4] [Docker-based CI/CD for Rocq/OCaml projects](https://arxiv.org/abs/2510.19089)
*Érik Martin-Dorel*

Main category: cs.SE

TL;DR: This paper describes three Docker-based DevOps tools (docker-coq, docker-coq-action, docker-keeper) for Coq/OCaml projects, focusing on their features for CI/CD and documenting design choices for maintainers.


<details>
  <summary>Details</summary>
Motivation: To provide Docker-based CI/CD solutions for Coq (now Rocq) and OCaml projects, and to document the tools' requirements and design for future maintenance.

Method: Developed three interconnected software projects: docker-coq (Docker images), docker-coq-action (GitHub Action), and docker-keeper (automated image building service).

Result: Created a complete DevOps toolchain that enables continuous integration and deployment for Coq/OCaml projects using Docker containers.

Conclusion: The paper successfully documents both the available features for users and the underlying design choices for maintainers of these three Docker-based DevOps tools.

Abstract: This paper presents three closely-related software projects, namely:
docker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:
provide a high-level description of the available features -- to foster the use
of a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --
and document the underlying requirements and the main design choices of these
three DevOps tools -- to help their future maintainers.

</details>


### [5] [Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study](https://arxiv.org/abs/2510.19237)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Shengxin Zhao,Chuihui Wang,Hongbin Xiao*

Main category: cs.SE

TL;DR: Proposed ReqEBench, a benchmark for CPS requirements concern extraction with 2,721 requirements from 12 real-world systems, and evaluated automated solutions including GPT-4 which achieved only 0.24 F1 score.


<details>
  <summary>Details</summary>
Motivation: Current automated solutions for requirements concern extraction in CPS lack fair and comprehensive benchmarks for evaluation, making effectiveness assessment challenging.

Method: Created ReqEBench benchmark with 2,721 requirements from 12 real-world CPS across multiple domains, featuring rigorous annotation process and comprehensive concern coverage. Evaluated three types of automated solutions including LLM-based approaches.

Result: GPT-4 achieved only 0.24 F1 score in entity concern extraction. Analysis revealed shortcomings in popular LLM-based solutions and provided improvement ideas.

Conclusion: ReqEBench facilitates evaluation and development of automated requirements concern extraction for CPS, highlighting current limitations and future improvement directions.

Abstract: Cyber-physical systems (CPSs) are characterized by a deep integration of the
information space and the physical world, which makes the extraction of
requirements concerns more challenging. Some automated solutions for
requirements concern extraction have been proposed to alleviate the burden on
requirements engineers. However, evaluating the effectiveness of these
solutions, which relies on fair and comprehensive benchmarks, remains an open
question. To address this gap, we propose ReqEBench, a new CPSs requirements
concern extraction benchmark, which contains 2,721 requirements from 12
real-world CPSs. ReqEBench offers four advantages. It aligns with real-world
CPSs requirements in multiple dimensions, e.g., scale and complexity. It covers
comprehensive concerns related to CPSs requirements. It undergoes a rigorous
annotation process. It covers multiple application domains of CPSs, e.g.,
aerospace and healthcare. We conducted a comparative study on three types of
automated requirements concern extraction solutions and revealed their
performance in real-world CPSs using our ReqEBench. We found that the highest
F1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze
failure cases of popular LLM-based solutions, summarize their shortcomings, and
provide ideas for improving their capabilities. We believe ReqEBench will
facilitate the evaluation and development of automated requirements concern
extraction.

</details>


### [6] [A General Solution for the Implementation of CI/CD in Embedded Linux Development](https://arxiv.org/abs/2510.19240)
*Behnam Agahi,Hamed Farbeh*

Main category: cs.SE

TL;DR: This paper presents an integrated infrastructure using the Yocto Project for automated development, building, and testing of Linux-based embedded systems, featuring CI/CD pipelines, Docker isolation, and cache optimization.


<details>
  <summary>Details</summary>
Motivation: Growing need for automated platforms to develop and deploy customized Linux-based operating systems for embedded systems across various industries.

Method: Three-layer Yocto architecture with main repositories, custom layer (meta-custom), and coordinating manifest layer. Implemented CI/CD pipelines with GitLab CI and Docker isolation. Used local cache server with hashserv, downloads and sstate cache. Tested with QEMU simulator.

Result: Significantly reduced build time through cache optimization. Verified system functionality and stability through six boot test scenarios in QEMU. Ensured reproducibility and scalability of the infrastructure.

Conclusion: The proposed design provides a reproducible, scalable infrastructure suitable for industrial and research embedded systems projects, with potential for extension to real-time Linux deployment and further optimization through distributed builds and monitoring systems.

Abstract: With the growing use of embedded systems in various industries, the need for
automated platforms for the development and deployment of customized
Linux-based operating systems has become more important. This research was
conducted with the aim of designing and implementing an integrated and
reproducible infrastructure for the development, building, and testing of a
Linux-based operating system using the Yocto Project. The proposed structure
was implemented based on a three-layer architecture consisting of the main
Yocto repositories, a custom layer (meta-custom), and a coordinating manifest
layer to ensure version synchronization, scalability, and reproducibility.
Three sample projects, including libhelloworld, helloworld, and the kernel
module hello mod, were developed and integrated into the build process.
Continuous Integration and Continuous Deployment pipelines were implemented
with GitLab CI and combined with an isolated Docker environment to automate and
streamline the build and testing workflows. Using a local cache server
containing hashserv, downloads and sstate cache significantly reduced the build
time. The functionality and stability of the system were verified through six
boot test scenarios in the QEMU simulator. The results show that the proposed
design not only ensures reproducibility but also can be extended to advanced
applications such as continuous deployment of real-time Linux versions. Future
recommendations include expanding automated tests, implementing system
monitoring with Prometheus and Grafana, using distributed builds, optimizing
with Docker multi-stage builds, and enabling continuous deployment of real-time
Linux changes to provide a stable and scalable model for industrial and
research projects in embedded systems with a rapid and reliable development
cycle.

</details>


### [7] [Trace: Securing Smart Contract Repository Against Access Control Vulnerability](https://arxiv.org/abs/2510.19254)
*Chong Chen,Jiachi Chen,Lingfeng Bao,David Lo,Yanlin Wang,Zhenyu Shan,Ting Chen,Guangqiang Yin,Jianxing Yu,Zibin Zheng*

Main category: cs.SE

TL;DR: TRACE is a tool that detects access control vulnerabilities in non-compilable smart contract repositories using LLMs to complete function snippets and analyze control flow graphs.


<details>
  <summary>Details</summary>
Motivation: Smart contract vulnerabilities, especially improper access control, have caused billions in losses. Existing tools can't handle non-compilable repositories, limiting their effectiveness for real-world development scenarios.

Method: TRACE uses LLMs to locate sensitive functions, completes them into compilable contracts, constructs function call graphs from AST, and analyzes control flow graphs to detect access control vulnerabilities.

Result: TRACE detected 14/15 CVEs on open-source dataset, achieved 89.2% precision on 5,000 on-chain contracts (vs 76.9% for best existing tool), and 87.0% precision on 83 real-world repositories (vs 14.3% for DeepSeek-R1).

Conclusion: TRACE significantly outperforms state-of-the-art tools in detecting access control vulnerabilities in smart contract repositories, especially for non-compilable code scenarios.

Abstract: Smart contract vulnerabilities, particularly improper Access Control that
allows unauthorized execution of restricted functions, have caused billions of
dollars in losses. GitHub hosts numerous smart contract repositories containing
source code, documentation, and configuration files-these serve as intermediate
development artifacts that must be compiled and packaged before deployment.
Third-party developers often reference, reuse, or fork code from these
repositories during custom development. However, if the referenced code
contains vulnerabilities, it can introduce significant security risks. Existing
tools for detecting smart contract vulnerabilities are limited in their ability
to handle complex repositories, as they typically require the target contract
to be compilable to generate an abstract representation for further analysis.
This paper presents TRACE, a tool designed to secure non-compilable smart
contract repositories against access control vulnerabilities. TRACE employs
LLMs to locate sensitive functions involving critical operations (e.g.,
transfer) within the contract and subsequently completes function snippets into
a fully compilable contract. TRACE constructs a function call graph from the
abstract syntax tree (AST) of the completed contract. It uses the control flow
graph (CFG) of each function as node information. The nodes of the sensitive
functions are then analyzed to detect Access Control vulnerabilities.
Experimental results demonstrate that TRACE outperforms state-of-the-art tools
on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it
achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the
best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves
87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.

</details>


### [8] [From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems](https://arxiv.org/abs/2510.19274)
*Saurabh Chauhan,Zeeshan Rasheed,Malik Abdul Sami,Kai-Kristian Kemell,Muhammad Waseem,Zheying Zhang,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: LLM-based agents automate RESTful microservices development by generating OpenAPI specs, creating server code, and refining it through log analysis feedback loops.


<details>
  <summary>Details</summary>
Motivation: To advance API-first development automation for RESTful web services and test LLM-based multi-agent systems' capability in supporting API-first development approach.

Method: Uses LLM-based agents to create OpenAPI specifications, generate server code, and refine code through feedback loops analyzing execution logs and error messages.

Result: LLMs can generate complete functional code with business logic aligned to specifications when OpenAPI specs are kept small and focused, as demonstrated using PRAB benchmark.

Conclusion: The system successfully automates API-first development and shows LLM-based multi-agent systems can effectively support RESTful microservices development through iterative refinement.

Abstract: This paper presents a system that uses Large Language Models (LLMs)-based
agents to automate the API-first development of RESTful microservices. This
system helps to create an OpenAPI specification, generate server code from it,
and refine the code through a feedback loop that analyzes execution logs and
error messages. The integration of log analysis enables the LLM to detect and
address issues efficiently, reducing the number of iterations required to
produce functional and robust services. This study's main goal is to advance
API-first development automation for RESTful web services and test the
capability of LLM-based multi-agent systems in supporting the API-first
development approach. To test the proposed system's potential, we utilized the
PRAB benchmark. The results indicate that if we keep the OpenAPI specification
small and focused, LLMs are capable of generating complete functional code with
business logic that aligns to the specification. The code for the system is
publicly available at https://github.com/sirbh/code-gen

</details>


### [9] [An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics](https://arxiv.org/abs/2510.19281)
*Shubham Joshi*

Main category: cs.SE

TL;DR: This study examines how different programming experience levels affect understanding of bitwise operators, finding that operator type can predict response time with small but significant effects.


<details>
  <summary>Details</summary>
Motivation: To investigate the readability and understandability of bitwise operators in programming, testing whether there are differences in performance metrics between participants exposed to various bitwise operators.

Method: Within-subjects experimental design with 23 participants of varying programming backgrounds (from no experience to PhD level), using JavaScript tasks to measure completion time and accuracy.

Result: Operators can predict response time with R-squared 0.032, F(1,494)=16.5, p<.001. OR, NOT, and Left Shift operators showed statistical significance in task completion times compared to other operators.

Conclusion: While bitwise operator complexity didn't generally increase completion times, certain operators were less intuitive, suggesting need for further investigation and potential redesign for improved understandability.

Abstract: Objectives: This study aims to investigate the readability and
understandability of bitwise operators in programming, with the main hypothesis
that there will be a difference in the performance metrics (response time and
error rate) between participants exposed to various bitwise operators related
questions and those who are not.
  Participants: Participants in this human research study include people
without programming background, novice programmers, and university students
with varying programming experience (from freshmen to PhD level). There were 23
participants for this study.
  Study Methods: This study uses an Within-Subjects Experimental Design to
assess how people with diverse programming backgrounds understand and use
bitwise operators. Participants complete tasks in JavaScript program, and their
task completion time and accuracy of the tasks are recorded for analysis.
  Findings: The results indicate that operators can be one of the factors
predicting response time, with a small but significant effect, with R-squared
0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,
and Left Shift showed statistical significance in task completion times
compared to other operators.
  Conclusions: While the complexity of bitwise operators did not generally
result in longer task completion times, certain operators were found to be less
intuitive, suggesting the need for further investigation and potential redesign
for improved understandability.

</details>


### [10] [Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects](https://arxiv.org/abs/2510.19393)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Jaralyzer is a bytecode-centric dependency scanner for Java that outperforms existing scanners in detecting vulnerabilities, especially in modified dependencies like re-compilations, re-bundlings, and re-packagings.


<details>
  <summary>Details</summary>
Motivation: 71% of code in typical Java projects comes from OSS dependencies, creating significant security risks. Current dependency scanners fail to detect vulnerabilities in modified dependencies common in the Java ecosystem.

Method: Jaralyzer analyzes bytecode directly instead of relying on metadata or source code, making it effective for detecting vulnerabilities in modified dependencies.

Result: Evaluation across 56 popular OSS components shows Jaralyzer outperforms other scanners, detecting 28 more true vulnerabilities and yielding 29 fewer false warnings compared to Eclipse Steady. It's the only scanner capable of identifying vulnerabilities across all types of modifications.

Conclusion: Jaralyzer provides superior vulnerability detection for Java OSS dependencies, especially for modified dependencies, addressing critical gaps in current dependency scanning approaches.

Abstract: On average, 71% of the code in typical Java projects comes from open-source
software (OSS) dependencies, making OSS dependencies the dominant component of
modern software code bases. This high degree of OSS reliance comes with a
considerable security risk of adding known security vulnerabilities to a code
base. To remedy this risk, researchers and companies have developed various
dependency scanners, which try to identify inclusions of known-to-be-vulnerable
OSS dependencies. However, there are still challenges that modern dependency
scanners do not overcome, especially when it comes to dependency modifications,
such as re-compilations, re-bundlings or re-packagings, which are common in the
Java ecosystem. To overcome these challenges, we present Jaralyzer, a
bytecode-centric dependency scanner for Java. Jaralyzer does not rely on the
metadata or the source code of the included OSS dependencies being available
but directly analyzes a dependency's bytecode. Our evaluation across 56 popular
OSS components demonstrates that Jaralyzer outperforms other popular dependency
scanners in detecting vulnerabilities within modified dependencies. It is the
only scanner capable of identifying vulnerabilities across all the above
mentioned types of modifications. But even when applied to unmodified
dependencies, Jaralyzer outperforms the current state-of-the-art code-centric
scanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding
29 fewer false warnings.

</details>


### [11] [AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems](https://arxiv.org/abs/2510.19438)
*Linfeng Liang,Chenkai Tan,Yao Deng,Yingfeng Cai,T. Y Chen,Xi Zheng*

Main category: cs.SE

TL;DR: AutoMT is an automated metamorphic testing framework for autonomous driving systems that uses multi-agent LLMs to extract metamorphic relations from traffic rules and generate diverse test cases, achieving higher test diversity and detecting more behavioral violations than manual approaches.


<details>
  <summary>Details</summary>
Motivation: Existing metamorphic testing methods for autonomous driving systems rely heavily on manual effort and lack automation, limiting their effectiveness in detecting safety-critical failures.

Method: AutoMT uses a multi-agent framework with LLMs: one agent extracts metamorphic relations from traffic rules using Gherkin syntax and ontology, a vision-language agent analyzes scenarios, and a search agent retrieves suitable relations from a RAG database to generate follow-up test cases via computer vision.

Result: AutoMT achieves up to 5x higher test diversity in follow-up case generation compared to manual expert-defined MRs, with higher validation rates, and detects up to 20.55% more behavioral violations.

Conclusion: AutoMT's automated approach extracts diverse metamorphic relations that uncover corner cases missed during in-field testing, and its modular architecture supports integration into industrial pipelines for systematic testing of safety-critical scenarios.

Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be
severe. While Metamorphic Testing (MT) is effective for fault detection in ADS,
existing methods rely heavily on manual effort and lack automation. We present
AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that
automates the extraction of Metamorphic Relations (MRs) from local traffic
rules and the generation of valid follow-up test cases. AutoMT leverages LLMs
to extract MRs from traffic rules in Gherkin syntax using a predefined
ontology. A vision-language agent analyzes scenarios, and a search agent
retrieves suitable MRs from a RAG-based database to generate follow-up cases
via computer vision. Experiments show that AutoMT achieves up to 5 x higher
test diversity in follow-up case generation compared to the best baseline
(manual expert-defined MRs) in terms of validation rate, and detects up to
20.55% more behavioral violations. While manual MT relies on a fixed set of
predefined rules, AutoMT automatically extracts diverse metamorphic relations
that augment real-world datasets and help uncover corner cases often missed
during in-field testing and data collection. Its modular architecture
separating MR extraction, filtering, and test generation supports integration
into industrial pipelines and potentially enables simulation-based testing to
systematically cover underrepresented or safety-critical scenarios.

</details>


### [12] [Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective](https://arxiv.org/abs/2510.19460)
*Thomas I. Strasser,Edmund Widl,Carlos Ayon Mac Gregor,Mirko Ginocchi,Rene Kuchenbuch*

Main category: cs.SE

TL;DR: Analysis of 30 European interoperability testing facilities for smart grids, providing a categorized inventory and blueprint for future testing environments to support energy transition goals.


<details>
  <summary>Details</summary>
Motivation: The European energy transition requires high interoperability across renewable energy systems, but existing initiatives lack dedicated focus on interoperability testing, creating a gap in structured testing capabilities.

Method: Conducted a structured survey of 30 European interoperability testing facilities, analyzing testing infrastructures, methodologies, and reference test cases.

Result: Created a comprehensive inventory and categorization of testing capabilities across Europe, identifying gaps and opportunities for coordination.

Conclusion: The study provides a blueprint for developing future testing environments and contributes to establishing a coordinated European ecosystem for interoperability testing to support energy transition collaboration and innovation.

Abstract: The ongoing transformation of the European energy landscape, driven by the
integration of renewable energy sources, digital technologies, and
decentralized systems, requires a high degree of interoperability across
diverse components and systems. Ensuring that these elements can exchange
information and operate together reliably is essential for achieving a secure,
flexible, and efficient energy supply infrastructure. While several initiatives
have contributed to the development of smart grid testing infrastructures, they
do not provide a dedicated or comprehensive focus on interoperability testing.
A structured and harmonized overview of interoperability testing capabilities
across Europe is therefore still missing. This work therefore presents a novel
contribution by analyzing the European interoperability testing facility
landscape through a structured survey of 30 facilities. It provides a
categorized inventory of testing infrastructures, applied methodologies, and
reference test cases, and introduces a blueprint for the development of future
testing environments. The findings contribute to the establishment of a
coordinated European ecosystem for interoperability testing, supporting
collaboration, innovation, and alignment with the goals of the energy
transition.

</details>


### [13] [A Goal-Driven Survey on Root Cause Analysis](https://arxiv.org/abs/2510.19593)
*Aoyang Fang,Haowen Yang,Haoze Dong,Qisheng Lu,Junjielong Xu,Pinjia He*

Main category: cs.SE

TL;DR: This paper presents a goal-driven framework for categorizing 135 Root Cause Analysis (RCA) papers in cloud incident management, addressing limitations of previous surveys that grouped papers by data types rather than their distinct underlying goals.


<details>
  <summary>Details</summary>
Motivation: Previous RCA surveys have overlooked goal-based distinctions, grouping papers with disparate objectives by input data types, which obscures true progress and gaps in the field. There's a need for a survey that organizes papers according to their goals to better serve both laymen and researchers.

Method: The paper develops a goal-driven framework that categorizes and integrates 135 RCA papers from 2014-2025 based on their diverse goals in cloud incident management, discussing the ultimate goal of all RCA papers as an umbrella concept.

Result: The framework effectively categorizes RCA papers according to their goals, providing a clearer understanding of the field's progress and identifying gaps that were previously obscured by data-type-based categorization.

Conclusion: A goal-driven approach to organizing RCA literature provides more meaningful insights than traditional data-type-based categorization, better serving the needs of both newcomers and experienced researchers in understanding the field's true progress and remaining challenges.

Abstract: Root Cause Analysis (RCA) is a crucial aspect of incident management in
large-scale cloud services. While the term root cause analysis or RCA has been
widely used, different studies formulate the task differently. This is because
the term "RCA" implicitly covers tasks with distinct underlying goals. For
instance, the goal of localizing a faulty service for rapid triage is
fundamentally different from identifying a specific functional bug for a
definitive fix. However, previous surveys have largely overlooked these
goal-based distinctions, conventionally categorizing papers by input data types
(e.g., metric-based vs. trace-based methods). This leads to the grouping of
works with disparate objectives, thereby obscuring the true progress and gaps
in the field. Meanwhile, the typical audience of an RCA survey is either laymen
who want to know the goals and big picture of the task or RCA researchers who
want to figure out past research under the same task formulation. Thus, an RCA
survey that organizes the related papers according to their goals is in high
demand. To this end, this paper presents a goal-driven framework that
effectively categorizes and integrates 135 papers on RCA in the context of
cloud incident management based on their diverse goals, spanning the period
from 2014 to 2025. In addition to the goal-driven categorization, it discusses
the ultimate goal of all RCA papers as an umbrella covering different RCA
formulations. Moreover, the paper discusses open challenges and future
directions in RCA.

</details>


### [14] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage is a multi-agent system that automatically converts research papers into interactive webpages through a hierarchical pipeline with verification agents to ensure accuracy.


<details>
  <summary>Details</summary>
Motivation: Researchers spend excessive time manually creating project webpages to make dense papers accessible, and existing automation tools don't handle the dynamic, interactive nature of webpages.

Method: A multi-agent system with coarse-to-fine pipeline: narrative planning, multimodal content generation, interactive rendering, plus dedicated Checker agents for verification against source papers and optional human checkpoints.

Result: AutoPage generates high-quality, visually appealing webpages in under 15 minutes for less than $0.1, with PageBench benchmark showing strong performance.

Conclusion: AutoPage successfully transforms paper-to-page creation from manual chore to efficient collaboration, serving as a powerful assistant rather than just a tool.

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [15] [FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation](https://arxiv.org/abs/2510.19615)
*Zhiping Zhou,Xiaohong Li,Ruitao Feng,Yao Zhang,Yuekang Li,Wenbu Feng,Yunqian Wang,Yuqing Li*

Main category: cs.SE

TL;DR: FidelityGPT is a framework that improves decompiled code accuracy and readability by detecting and correcting semantic distortions using distortion-aware prompts, RAG with dynamic semantic intensity, and variable dependency analysis.


<details>
  <summary>Details</summary>
Motivation: Existing decompilation methods have fidelity issues that degrade readability and semantic accuracy, especially for complex closed-source binaries. Current approaches like variable renaming provide only partial improvements and lack robust detection and correction mechanisms.

Method: Uses distortion-aware prompt templates for closed-source settings, integrates Retrieval-Augmented Generation (RAG) with dynamic semantic intensity algorithm to locate distorted lines and retrieve similar code, and employs variable dependency algorithm to handle long-context limitations.

Result: Achieved 89% average detection accuracy and 83% precision on 620 function pairs. Outperformed state-of-the-art DeGPT with 94% Fix Rate (vs 83%) and 64% Corrected Fix Rate (vs 37%).

Conclusion: FidelityGPT demonstrates significant improvements in decompilation accuracy and readability, showing strong potential to advance LLM-based decompilation and reverse engineering.

Abstract: Decompilation converts machine code into human-readable form, enabling
analysis and debugging without source code. However, fidelity issues often
degrade the readability and semantic accuracy of decompiled output. Existing
methods, such as variable renaming or structural simplification, provide
partial improvements but lack robust detection and correction, particularly for
complex closed-source binaries. We present FidelityGPT, a framework that
enhances decompiled code accuracy and readability by systematically detecting
and correcting semantic distortions. FidelityGPT introduces distortion-aware
prompt templates tailored to closed-source settings and integrates
Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity
algorithm to locate distorted lines and retrieve semantically similar code from
a database. A variable dependency algorithm further mitigates long-context
limitations by analyzing redundant variables and integrating their dependencies
into the prompt context. Evaluated on 620 function pairs from a binary
similarity benchmark, FidelityGPT achieved an average detection accuracy of 89%
and a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,
Corrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating
significant gains in accuracy and readability. These results highlight its
potential to advance LLM-based decompilation and reverse engineering.

</details>


### [16] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: Agentic AI will transform Software Engineering, requiring expansion beyond code-focused approaches to address socio-technical concerns and establish strong foundations for agentic SE.


<details>
  <summary>Details</summary>
Motivation: As technologists rush to develop agentic AI, SE researchers need to establish agentic SE as a research area and address practical socio-technical concerns beyond just code activities.

Method: The paper proposes: (a) expanding scope to 'whole of process' vision grounded in SE foundations, (b) establishing values and principles to guide efforts, and (c) providing guidance on well-defined vocabulary for agentic SE.

Result: The paper contributes to the emerging community vision by providing foundational guidance for agentic SE research and practice.

Conclusion: These ideas aim to encourage community collaborations and steer SE toward establishing strong foundations for agentic SE, making it deliberate and desirable rather than just inevitable.

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [17] [Review of Tools for Zero-Code LLM Based Application Development](https://arxiv.org/abs/2510.19747)
*Priyaranjan Pattnayak,Hussain Bohra*

Main category: cs.SE

TL;DR: Survey of LLM-powered zero-code development platforms, categorizing them by interface style, backend integration, output type, and extensibility, comparing dedicated LLM app builders with general no-code platforms.


<details>
  <summary>Details</summary>
Motivation: To understand how Large Language Models are transforming software creation through zero-code development platforms, enabling users to build applications without writing code.

Method: Broad survey methodology categorizing platforms based on key dimensions: interface style, backend integration, output type, and extensibility. Analysis includes both dedicated LLM-based app builders and general no-code platforms with LLM integration.

Result: Created a taxonomy categorizing platforms by interface (conversational, visual), supported LLM backends, output type (chatbot, full application, workflow), and extensibility. Found that while zero-code LLM platforms reduce barriers to creating AI-powered applications, they face challenges in flexibility and reliability.

Conclusion: The landscape of LLM-powered zero-code platforms is rapidly evolving, offering opportunities to empower non-programmers to create sophisticated software, though trade-offs exist in customizability, scalability, and vendor lock-in compared to traditional approaches.

Abstract: Large Language Models (LLMs) are transforming software creation by enabling
zero code development platforms. Our survey reviews recent platforms that let
users build applications without writing code, by leveraging LLMs as the brains
of the development process. We adopt a broad survey methodology, categorizing
platforms based on key dimensions such as interface style, backend integration,
output type, and extensibility. We analyze both dedicated LLM based app
builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and
general no code platforms (e.g., Bubble, Glide) that integrate LLM
capabilities. We present a taxonomy categorizing these platforms by their
interface (conversational, visual, etc.), supported LLM backends, output type
(chatbot, full application, workflow), and degree of extensibility. Core
features such as autonomous agents, memory management, workflow orchestration,
and API integrations are in scope of the survey. We provide a detailed
comparison, highlighting each platform's strengths and limitations. Trade offs
(customizability, scalability, vendor lock-in) are discussed in comparison with
traditional and low code development approaches. Finally, we outline future
directions, including multimodal interfaces, on device LLMs, and improved
orchestration for democratizing app creation with AI. Our findings indicate
that while zero code LLM platforms greatly reduce the barrier to creating AI
powered applications, they still face challenges in flexibility and
reliability. Overall, the landscape is rapidly evolving, offering exciting
opportunities to empower non programmers to create sophisticated software.

</details>


### [18] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: BOSQTGEN is a black-box API test generation tool that uses LLMs to decompose API specifications into primitives and combinatorial testing to efficiently sample values, achieving 82% code coverage on RESTful benchmarks.


<details>
  <summary>Details</summary>
Motivation: Modern software relies heavily on API composition, but inadequate contracts lead to failures. Current test generation faces challenges with polyglot systems, source code inaccessibility, cost-reliability trade-offs, and generating structured inputs.

Method: Decomposes API specifications into primitives, uses LLMs to suggest coherent strata for them, and employs combinatorial testing to efficiently sample over these values to ensure coverage of critical interactions.

Result: Achieves average 82% code coverage on RESTful benchmarks, often 20%+ increase over prior state-of-the-art systems, nearing parity with hand-written test suites.

Conclusion: Provides a fully API-driven approach to automatically create high-quality test cases for validation or test-driven development, addressing key challenges in API conformance testing.

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>
