<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: This paper examines the feasibility of using open-source LLMs as locally deployable components for intent-based operating systems, comparing them against proprietary models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Current LLM implementations rely on cloud-based proprietary models, which introduce privacy, autonomy, and scalability limitations. Local deployment is essential for robust and trusted language-first interaction paradigms.

Method: Comparative analysis of several open-source and open-access LLMs against OpenAI's GPT-4-based systems, evaluating their performance in generating workflows for various user intentions.

Result: The study provides empirical insights into the practical viability and performance trade-offs of open LLMs as autonomous, locally operable components.

Conclusion: Open LLMs show potential for enabling more seamless, adaptive, and privacy-conscious user-device interaction through locally embedded intelligence, contributing to AI decentralization and democratization.

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [2] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: This study empirically evaluates how different dependency version constraint types (pinning vs floating) affect the likelihood of dependencies becoming outdated or vulnerable in npm, PyPI, and Cargo ecosystems.


<details>
  <summary>Details</summary>
Motivation: Developers face a trade-off between pinning dependencies (reduces breaking changes but requires manual updates) and floating dependencies (automatically gets fixes but risks breaking changes). Security practitioners advocate pinning to prevent supply chain attacks, but it's unknown how constraint types affect dependency outdatedness and vulnerability.

Method: Analyzed dependency version constraint usage trends and patterns in npm, PyPI, and Cargo ecosystems. Used survival analysis to model dependency state transitions and estimate likelihood of becoming outdated or vulnerable when using pinning vs other constraint types.

Result: Among outdated and vulnerable dependencies, floating-minor is the most common constraint type, followed by pinning. Floating-major is least likely to result in outdated dependencies, while floating-minor is least likely to result in vulnerable dependencies.

Conclusion: The study provides empirical evidence to help developers make informed dependency version constraint choices by understanding the trade-offs between different constraint types regarding outdatedness and vulnerability risks.

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [3] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: The paper proposes a context collection strategy for code completion using LLMs, focusing on preprocessing repositories into code chunks and using syntactic/semantic similarity retrieval with relative positioning.


<details>
  <summary>Details</summary>
Motivation: Current IDEs lack research on optimal context for code completion with LLMs, despite code completion being crucial for developer efficiency and development lifecycle.

Method: Preprocess repository into smaller code chunks, then use syntactic and semantic similarity-based code chunk retrieval with relative positioning to collect context for LLMs.

Result: Code chunking and relative positioning of chunks in the final context improve the performance of code completion tasks.

Conclusion: The proposed context collection strategy effectively enhances LLM performance in code completion by leveraging code chunking and relative positioning techniques.

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [4] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: LLMs are transforming software development team collaboration by improving efficiency, communication, and decision-making throughout the SDLC, while introducing challenges like model limitations and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs affect team collaboration in software development and understand their potential to transform team workflows and productivity across the entire Software Development Life Cycle.

Method: Literature review, industry examples, team survey, and two case studies examining LLM-assisted tools like code generation assistants and AI-powered project management agents.

Result: LLMs significantly improve efficiency by automating repetitive tasks and documentation, enhance communication clarity, and aid cross-functional collaboration, though they introduce challenges including model limitations and privacy concerns.

Conclusion: LLMs offer substantial benefits for collaborative software engineering but require addressing challenges through domain-specific customization, better tool integration, and robust trust/security strategies for future development.

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [5] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: GradleFixer is an LLM agent with domain-specific tools that achieves 81.4% success rate in fixing Android build errors, outperforming general-purpose coding agents through Tool Bridging strategy.


<details>
  <summary>Details</summary>
Motivation: Android is the largest mobile platform but automatically building applications remains challenging. LLMs show promise for code repair but their use for fixing Android build errors is underexplored.

Method: Introduced AndroidBuildBench benchmark with 1,019 build failures from 43 open-source Android projects. Proposed GradleFixer LLM agent with domain-specific tools for inspecting and manipulating Gradle build environment, using Tool Bridging strategy that replaces general-purpose shell commands with domain-aware abstractions.

Result: GradleFixer achieved 81.4% resolve rate (pass@1), significantly outperforming state-of-the-art coding agents that rely on general-purpose shells.

Conclusion: LLMs possess high-level knowledge to solve build failures but struggle to translate this into effective low-level actions using general-purpose shells. Tool Bridging works by providing API-like tools that LLMs use more reliably and constraining the action space to relevant operations, bridging the gap between high-level reasoning and effective low-level execution.

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [6] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: Faver is a function abstracted verifiable middleware that improves RTL generation accuracy in LLM-based workflows by decoupling circuit verification details and allowing LLMs to focus on functionality.


<details>
  <summary>Details</summary>
Motivation: LLM-based RTL generation faces challenges due to semantic gap between high-level specifications and RTL, limited training data, and scarce RTL testbench data, making verification difficult for LLMs.

Method: Proposes Faver middleware that mixes LLM-friendly code structures with rule-based templates to decouple circuit verification details, enabling LLMs to focus on functionality rather than low-level verification.

Result: In experiments on SFT model and open-source models, Faver improved generation accuracy by up to 14%.

Conclusion: Faver effectively addresses the semantic gap and verification challenges in LLM-based RTL generation, significantly improving accuracy by separating verification concerns from functional implementation.

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [7] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: A multi-agent code generation framework using ReAct paradigm that enhances safety, accuracy, and controllability through specialized agents and dynamic tool integration.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing code generation models regarding safety, accuracy, controllability, and lack of dynamic external tool integration and transparent reasoning.

Method: Multi-agent system with four specialized agents: Planner for task decomposition, ReAct-based Searcher for reasoning and tool integration, CodeGen agent for code generation, and Extractor for data retrieval. Uses ReAct paradigm for alternating reasoning and action execution.

Result: Achieved 94.8% security rate on SVEN dataset with CodeQL, outperforming existing approaches. Effective across multiple programming languages with transparent reasoning process.

Conclusion: The framework enables efficient, precise, and interpretable code generation through dynamic LLM-external resource interactions, fostering user trust and improving controllability.

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [8] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: A RAG framework using Sentence-Transformers and FAISS to provide context-aware ticket resolution recommendations by integrating JIRA tickets, discussions, and GitHub PRs.


<details>
  <summary>Details</summary>
Motivation: Address delays in resolving recurring software issues due to fragmented knowledge across JIRA tickets, developer discussions, and GitHub PRs.

Method: Integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to retrieve similar past cases, then synthesizes them using LLM into grounded resolution suggestions.

Result: Experimental evaluation shows significant improvements in resolution accuracy, fix quality, and knowledge reuse using precision, recall, resolution time reduction, and developer acceptance metrics.

Conclusion: The proposed RAG framework effectively improves ticket resolution in DevOps environments by unifying fragmented knowledge sources and providing explainable recommendations.

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [9] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena is an open human evaluation platform for code generation that enables execution of LLM-generated code and human interaction with execution outcomes, collecting over 14,000 code conversations across 10 LLMs.


<details>
  <summary>Details</summary>
Motivation: Manual evaluation of LLM-generated code is challenging due to the need to understand raw code and simulate execution, requiring a better evaluation platform for code generation.

Method: Built on Chatbot Arena, BigCodeArena provides execution environments for LLM-generated code and collects human preferences. It also introduces BigCodeReward for reward model evaluation and AutoCodeArena for automatic coding quality assessment.

Result: Collected 14,000+ code conversations across 10 LLMs, identified 4,700+ multi-turn samples with human preferences. Found proprietary LLMs (GPT-5, Claude-Sonnet-4, Claude-Opus-4) lead in code generation performance.

Conclusion: BigCodeArena enables comprehensive evaluation of code generation capabilities, revealing underexplored preferences and showing that execution results significantly improve coding preference judgments.

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [10] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: Differential evolution is used to tune hyperparameters of DynaMOSA and MIO algorithms in Pynguin, showing improved test coverage and efficiency over grid search.


<details>
  <summary>Details</summary>
Motivation: Default hyperparameter values in search-based test generation may not yield optimal results, and traditional tuning methods are resource-intensive.

Method: Using differential evolution to tune hyperparameters of DynaMOSA and MIO algorithms implemented in Pynguin framework.

Result: Tuned DynaMOSA algorithm achieved significant improvement in test suite coverage, and differential evolution was more efficient than grid search.

Conclusion: Differential evolution is an effective method for hyperparameter tuning in search-based test generation, providing better results with improved efficiency.

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [11] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: PyMigTool is an automated Python library migration tool that uses LLMs combined with static and dynamic analysis to replace libraries in software projects, achieving 32% complete correctness on real-world applications.


<details>
  <summary>Details</summary>
Motivation: Manual library migration is time-consuming and error-prone, requiring understanding of both libraries' APIs and code transformations. Existing automated tools are limited to API mapping or support only specific libraries.

Method: Uses Large Language Models (LLMs) as the primary engine for migration, combined with static analysis and dynamic analysis in a command-line application called PyMigTool.

Result: Evaluated on 717 real-world Python applications, PyMigTool achieved 32% complete correctness. For remaining migrations, only 14% of migration-related changes needed manual fixing in more than half of projects.

Conclusion: LLMs can effectively perform library migration, and PyMigTool demonstrates that combining LLMs with post-processing steps provides accurate automated library migration for Python projects.

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [12] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: McMining is a new task for mining programming misconceptions from student code samples, with a benchmark dataset and LLM-based approaches showing effectiveness.


<details>
  <summary>Details</summary>
Motivation: Students develop programming misconceptions that cause bugs, inefficient code, and hinder learning related concepts.

Method: Created an extensible benchmark dataset of misconceptions and code samples, then developed two LLM-based McMiner approaches using Gemini, Claude, and GPT models.

Result: Extensive evaluations show that models from Gemini, Claude, and GPT families are effective at discovering misconceptions in student code.

Conclusion: LLM-based approaches can successfully identify programming misconceptions in student code, enabling better detection and remediation of learning gaps.

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [13] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: Study of debugging practices in video game development, identifying time spent on inspection (36.6%) and bug reproduction (35.1%) as major bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To understand unique debugging techniques in video game development and identify bottlenecks in bug resolution processes.

Method: Recorded debugging sessions from 20 seasoned game developers, focusing on critical bugs (Crashes, Object Behaviors, Object Persistence), and conducted thematic analysis.

Result: Game developers spend 36.6% of time inspecting game artifacts and 35.1% reproducing bugs locally; identified key debugging tools and collaboration patterns.

Conclusion: Video game debugging requires specialized techniques, with inspection and reproduction consuming significant time, and technical roles being central to the debugging process.

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [14] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: Fine-tuned Qwen3-8B with QLoRA and Unsloth optimizations for semantic file path retrieval from natural language queries, achieving up to 91% exact match and 93% recall on Python projects.


<details>
  <summary>Details</summary>
Motivation: Traditional code search methods miss semantic context and cross-file links, while LLMs lack repository-specific details, making it hard to find relevant source files for questions about features or bug introductions.

Method: Used six code-aware strategies with AST structure and repository content to generate training data, then fine-tuned Qwen3-8B using QLoRA and Unsloth optimizations to predict file paths from natural language queries.

Result: Achieved high retrieval accuracy: up to 91% exact match and 93% recall on held-out queries, with 59% recall on large codebases like PyTorch (4,000 files), outperforming single-strategy training.

Conclusion: Multi-level code signals help LLMs reason over cross-file context; the approach shows scalability but faces limits with context length in very large repositories, suggesting future integration with LLM-based code intelligence.

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [15] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: A system that converts large software repositories into vectorized knowledge graphs to automate repository development by capturing semantic relationships and enabling intelligent querying.


<details>
  <summary>Details</summary>
Motivation: To enable automation in software repository development by creating a structured representation that captures architectural and semantic relationships, facilitating better understanding and manipulation of codebases.

Method: Decomposes repositories into knowledge graphs with syntactic relations (containment, implementation, references, calls, inheritance), augments nodes with LLM summaries and embeddings, uses hybrid retrieval combining semantic search with graph expansion, and employs LLM assistant for constrained queries.

Result: Creates a comprehensive vectorized knowledge graph that mirrors project structure and enables automated repository development through intelligent querying and explanation capabilities.

Conclusion: The repository decomposition system successfully transforms codebases into structured knowledge graphs, enabling significant automation in software development through semantic understanding and intelligent retrieval mechanisms.

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [16] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: SEER is a framework that addresses sustainability concerns in early software development using LLMs and agentic RAG to identify, evaluate, and optimize sustainability requirements.


<details>
  <summary>Details</summary>
Motivation: Current sustainability methods are high-level, time-consuming, and focus on later development phases, while sustainability assessment should start at requirements engineering to achieve UN Sustainable Development Goals.

Method: Three-stage framework: (1) identifies sustainability requirements from taxonomy, (2) evaluates system requirements against SRs, (3) optimizes failing requirements using LLM reasoning and agentic RAG approach.

Result: Tested on four software projects across domains using Gemini 2.5, showing effective identification of broad sustainability concerns across diverse domains.

Conclusion: SEER successfully addresses sustainability in early development phases and demonstrates practical applicability across different software domains.

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [17] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: This paper presents a comprehensive taxonomy of sustainability requirements across environmental, technical, social, and economic dimensions through a systematic literature review, including definitions, metrics, and a correlation matrix showing synergies and conflicts.


<details>
  <summary>Details</summary>
Motivation: Existing sustainability requirements research is fragmented, domain-specific, and lacks a unified taxonomy, creating a critical gap for systematic software sustainability development.

Method: Conducted a Systematic Literature Review (SLR) to extract and organize sustainability requirements from state-of-the-art research.

Result: Developed a comprehensive taxonomy of sustainability requirements across four dimensions with definitions, metrics, measures, and a correlation matrix showing inter-dimensional influences.

Conclusion: The taxonomy provides a systematized reference for software developers and researchers to effectively formulate, manage, and reconcile trade-offs in sustainable software development.

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [18] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: The paper introduces a new benchmarking framework that transforms formal GitHub issue-based benchmarks into realistic user queries for chat-based coding assistants, revealing significant overestimation of agent capabilities in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks derived from GitHub issues don't accurately reflect real developer interactions with chat-based coding assistants in IDEs, leading to systematic overestimation of agent capabilities in real-world scenarios.

Method: Developed a flexible benchmarking framework that transforms existing formal benchmarks into realistic user queries through systematic analysis of developer interaction patterns with chat-based agents, applied to SWE-Bench Verified, TypeScript subset of Multi-SWE-Bench, and a private SWE-Bench C# benchmark.

Result: Existing benchmarks significantly overestimate agent capabilities by >50% over baseline for public benchmarks and ~10-16% for internal benchmarks when transformed into realistic user-style queries.

Conclusion: This work establishes a new paradigm for evaluating interactive chat-based software engineering agents through benchmark mutation techniques that better reflect real-world usage patterns.

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [19] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: A zero-shot code translation method using identifier replacement to handle long source codes that exceed LLM context windows, improving translation accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long source codes that don't fit into context windows, leading to inaccurate translations in software development tasks like code translation between programming languages.

Method: Proposes identifier replacement by substituting long identifiers with generalized placeholders during translation, allowing LLMs to focus on logical structure while reducing token count and memory usage.

Result: Empirical results show the approach preserves syntactical and hierarchical information while producing translation results with reduced tokens.

Conclusion: The method improves efficiency and cost-effectiveness of long code translation by enabling LLMs to handle code that exceeds their context windows through identifier replacement.

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [20] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Survey of 131 software practitioners reveals LLMs are used as assistive tools for coding tasks, providing productivity benefits but raising concerns about accuracy, context limitations, and ethical risks.


<details>
  <summary>Details</summary>
Motivation: Despite growing LLM integration in software development, there's limited understanding of how these tools are actually used in practice and how professionals perceive their benefits and limitations.

Method: Global survey of 131 software practitioners to understand LLM usage patterns and perceptions in software development workflows.

Result: LLMs are used for various coding tasks with reported benefits including increased productivity, reduced cognitive load, and faster learning. However, practitioners raise concerns about inaccurate outputs, limited context awareness, and ethical risks.

Conclusion: Most developers treat LLMs as assistive tools rather than standalone solutions, reflecting a cautious yet practical approach to integration. Findings provide early practitioner perspective on LLM adoption for future research and responsible use.

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [21] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: Literate tracing is a program documentation paradigm that uses annotated execution traces to explain software systems, complementing traditional documentation methods.


<details>
  <summary>Details</summary>
Motivation: Traditional documentation methods like in-code comments lack global context, while design documents lack concrete connection to code, making it difficult to communicate how complex systems work.

Method: Developed TReX tool for creating interactive, visual literate traces that are guaranteed to be faithful to program semantics through construction.

Result: Successfully applied TReX to document components of large systems including Linux kernel, Git source control, and GCC compiler.

Conclusion: Literate tracing provides an effective approach for system experts to communicate program behavior to novices by combining concrete execution examples with annotations.

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [22] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML enhances Pynguin test generator by incorporating ML API constraints to generate compliant inputs, significantly improving code coverage for PyTorch and TensorFlow libraries.


<details>
  <summary>Details</summary>
Motivation: ML libraries have strict input constraints that automated test generators like Pynguin don't understand, leading to early test failures and limited code coverage.

Method: Extract constraints from official API documentation and integrate them into Pynguin to generate compliant inputs for ML APIs.

Result: Evaluation on 165 PyTorch and TensorFlow modules shows PynguinML achieves up to 63.9% higher code coverage compared to original Pynguin.

Conclusion: Leveraging API constraints in test generation significantly improves test effectiveness and code coverage for ML libraries.

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [23] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krmer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: The paper introduces PMDT, an ontology-driven digital twin framework that integrates multimodal health data for personalized chronic care while ensuring semantic interoperability and privacy preservation.


<details>
  <summary>Details</summary>
Motivation: Current digital twin applications are organ-specific or use isolated data types, lacking unified and privacy-preserving foundations for comprehensive chronic care management.

Method: Developed an OWL 2.0 ontology structured around modular Blueprints (patient, disease, treatment, trajectories, safety, pathways, adverse events) with conceptual views, validated through expert workshops, questionnaires, and a pilot study with immunotherapy patients.

Result: Evaluation confirmed ontology coverage, reasoning correctness, usability, and GDPR compliance. PMDT successfully unified heterogeneous data, operationalized competency questions, and supported descriptive, predictive, and prescriptive analytics in a federated manner.

Conclusion: PMDT provides a validated foundation for next-generation digital health ecosystems, transforming chronic care toward proactive, continuously optimized, and equitable management by bridging data fragmentation and semantic standardization gaps.

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [24] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krmer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: A model-driven engineering framework using domain-specific languages and federated learning enables healthcare AI development while maintaining data privacy and interoperability.


<details>
  <summary>Details</summary>
Motivation: AI adoption in healthcare is limited by fragmented data, privacy constraints, and technical complexity of building reliable clinical systems.

Method: Developed a model-driven engineering framework with formal metamodels, domain-specific languages (including MILA), automated transformations, and federated learning architecture.

Result: In multi-center cancer immunotherapy study, generated pipelines achieved up to 98.5% accuracy with support vector machines while reducing manual coding effort.

Conclusion: MDE principles provide a practical path toward interoperable, reproducible, and trustworthy digital health platforms.

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [25] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: TIT is a Tree-structured Instruction Tuning method that improves LLM-based code translation by addressing syntactic confusion and semantic misalignment through language-agnostic syntactic features and fine-grained parallel data augmentation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code translation methods suffer from sensitivity to language-specific features (causing syntactic confusion) and lack of fine-grained semantic alignment due to over-reliance on function-level parallel datasets.

Method: TIT consists of three modules: 1) syntactic information representation using language-agnostic features via structured parsing, 2) fine-grained parallel dataset augmentation through statement-level segmentation and contrastive matching, and 3) dual-stage tree instruction tuning with syntax-aware fine-tuning and code generation fine-tuning.

Result: The method significantly outperforms existing approaches in multiple LLMs, achieving 1.22x-1.75x higher success rate in code translation while markedly reducing syntactic confusion.

Conclusion: TIT effectively overcomes limitations of current LLM-based code translation by integrating structured syntactic information and fine-grained semantic alignment, leading to more accurate and reliable code translation.

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>
