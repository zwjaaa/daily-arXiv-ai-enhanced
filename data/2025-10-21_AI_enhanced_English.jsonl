{"id": "2510.16059", "categories": ["cs.SE", "cs.CL", "D.2.2; D.2.3"], "pdf": "https://arxiv.org/pdf/2510.16059", "abs": "https://arxiv.org/abs/2510.16059", "authors": ["Xin Cao", "Nan Yu"], "title": "SIADAFIX: issue description response for adaptive program repair", "comment": "20 pages, 3 figures", "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of\nlarge language model-based agents on complex tasks such as program repair. In\nparticular, we design an adaptive program repair method based on issue\ndescription response, called SIADAFIX. The proposed method utilizes slow\nthinking bug fix agent to complete complex program repair tasks, and employs\nfast thinking workflow decision components to optimize and classify issue\ndescriptions, using issue description response results to guide the\norchestration of bug fix agent workflows. SIADAFIX adaptively selects three\nrepair modes, i.e., easy, middle and hard mode, based on problem complexity. It\nemploys fast generalization for simple problems and test-time scaling\ntechniques for complex problems. Experimental results on the SWE-bench Lite\nshow that the proposed method achieves 60.67% pass@1 performance using the\nClaude-4 Sonnet model, reaching state-of-the-art levels among all open-source\nmethods. SIADAFIX effectively balances repair efficiency and accuracy,\nproviding new insights for automated program repair. Our code is available at\nhttps://github.com/liauto-siada/siada-cli.", "AI": {"tldr": "SIADAFIX is an adaptive program repair method that uses fast and slow thinking approaches to handle different complexity levels of bugs, achieving state-of-the-art performance on SWE-bench Lite.", "motivation": "To enhance large language model-based agents' capabilities on complex tasks like program repair by leveraging dual thinking modes for better efficiency and accuracy.", "method": "Uses slow thinking bug fix agent for complex repairs and fast thinking workflow decision components to classify issue descriptions. Adaptively selects easy, middle, and hard repair modes based on problem complexity, employing fast generalization for simple problems and test-time scaling for complex ones.", "result": "Achieves 60.67% pass@1 performance on SWE-bench Lite using Claude-4 Sonnet model, reaching state-of-the-art levels among open-source methods.", "conclusion": "SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair through adaptive thinking modes."}}
{"id": "2510.16242", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.16242", "abs": "https://arxiv.org/abs/2510.16242", "authors": ["Eva Maxfield Brown", "Isaac Slaughter", "Nicholas Weber"], "title": "Code Contribution and Credit in Science", "comment": null, "summary": "Software development has become essential to scientific research, but its\nrelationship to traditional metrics of scholarly credit remains poorly\nunderstood. We develop a dataset of approximately 140,000 paired research\narticles and code repositories, as well as a predictive model that matches\nresearch article authors with software repository developer accounts. We use\nthis data to investigate how software development activities influence credit\nallocation in collaborative scientific settings. Our findings reveal\nsignificant patterns distinguishing software contributions from traditional\nauthorship credit. We find that nearly 30% of articles include non-author code\ncontributors- individuals who participated in software development but received\nno formal authorship recognition. While code-contributing authors show a modest\n$\\sim$4.2% increase in article citations, this effect becomes non-significant\nwhen controlling for domain, article type, and open access status. First\nauthors are significantly more likely to be code contributors than other author\npositions. Notably, we identify a negative relationship between coding\nfrequency and scholarly impact metrics. Authors who contribute code more\nfrequently exhibit progressively lower h-indices than non-coding colleagues,\neven when controlling for publication count, author position, domain, and\narticle type. These results suggest a disconnect between software contributions\nand credit, highlighting important implications for institutional reward\nstructures and science policy.", "AI": {"tldr": "Analysis of 140,000 research articles and code repositories reveals that software development contributions are poorly recognized in scholarly credit systems, with 30% of articles having non-author code contributors and frequent coders showing lower h-indices despite similar publication counts.", "motivation": "To understand the relationship between software development activities and traditional scholarly credit metrics in scientific research, as software has become essential but its contribution recognition remains unclear.", "method": "Created a dataset of 140,000 paired research articles and code repositories, developed a predictive model to match article authors with software repository developers, and analyzed how software contributions affect credit allocation.", "result": "30% of articles have non-author code contributors; code-contributing authors show only 4.2% citation increase (non-significant when controlled); first authors are more likely to be code contributors; frequent coders have progressively lower h-indices than non-coding colleagues even when controlling for publication count and other factors.", "conclusion": "There is a significant disconnect between software contributions and scholarly credit, with implications for institutional reward structures and science policy that need to better recognize software development work."}}
{"id": "2510.16357", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis.", "AI": {"tldr": "MLCPD is a large-scale, language-agnostic dataset with over 7 million parsed source files across 10 programming languages, providing unified AST representations for cross-language code analysis.", "motivation": "To create a unified dataset that enables consistent cross-language reasoning and structural learning, addressing limitations of existing corpora that focus on token-level code or isolated parsers.", "method": "Developed a universal Abstract Syntax Tree schema to normalize syntactic representations across languages, with complete pipelines for dataset reproduction, grammar compilation, and visualization tools.", "result": "Empirical analyses show strong cross-language structural regularities, demonstrating that syntactic graphs from diverse languages can be aligned under a shared schema.", "conclusion": "MLCPD establishes an open, reproducible foundation for future research in cross-language representation learning and program analysis, with publicly released dataset and codebase."}}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility.", "AI": {"tldr": "SemOpt is a novel framework that uses static program analysis and LLMs to improve automated code optimization by precisely identifying optimizable code segments and retrieving relevant optimization strategies, significantly outperforming existing approaches.", "motivation": "Existing code optimization approaches using LLMs often fail because semantically equivalent optimizations can appear in syntactically different code, making current retrieval methods ineffective at finding relevant examples.", "method": "SemOpt has three LLM-powered components: (1) strategy library builder that extracts and clusters optimization strategies from real-world code modifications, (2) rule generator that creates Semgrep static analysis rules to identify optimization conditions, and (3) optimizer that uses the strategy library to generate optimized code.", "result": "On 151 optimization tasks, SemOpt increased successful optimizations by 1.38 to 28 times compared to baselines. On large-scale C/C++ projects, it improved performance metrics by 5.04% to 218.07%.", "conclusion": "SemOpt effectively addresses the limitations of current retrieval-based optimization approaches by leveraging static analysis and demonstrates significant practical utility in real-world code optimization scenarios."}}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems.", "AI": {"tldr": "The paper proposes a Code Digital Twin framework to bridge AI capabilities with enterprise software development realities by modeling both physical and conceptual layers of software, preserving tacit knowledge, and enabling co-evolution with codebases.", "motivation": "Enterprise software development faces challenges beyond routine coding, relying heavily on tacit knowledge, design decisions, and historical trade-offs. Current LLMs lack alignment with practical enterprise development realities.", "method": "Proposes Code Digital Twin - a living framework with hybrid knowledge representations, multi-stage extraction pipelines, incremental updates, LLM-empowered applications, and human-in-the-loop feedback to transform fragmented knowledge into explicit representations.", "result": "The framework provides a systematic approach to address challenges in enterprise software evolution by integrating AI capabilities with structured knowledge frameworks for enhanced decision-making in tasks like issue localization and impact analysis.", "conclusion": "The Code Digital Twin serves as a bridge between AI advancements and enterprise software realities, offering a roadmap for sustainable, intelligent, and resilient development of ultra-complex systems."}}
{"id": "2510.16433", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16433", "abs": "https://arxiv.org/abs/2510.16433", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Yasutaka Kamei", "Hajimu Iida"], "title": "Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions", "comment": null, "summary": "Software vulnerabilities are constantly being reported and exploited in\nsoftware products, causing significant impacts on society. In recent years, the\nmain approach to vulnerability detection, fuzzing, has been integrated into the\ncontinuous integration process to run in short and frequent cycles. This\ncontinuous fuzzing allows for fast identification and remediation of\nvulnerabilities during the development process. Despite adoption by thousands\nof projects, however, it is unclear how continuous fuzzing contributes to\nvulnerability detection. This study aims to elucidate the role of continuous\nfuzzing in vulnerability detection. Specifically, we investigate the coverage\nand the total number of fuzzing sessions when fuzzing bugs are discovered. We\ncollect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an\nonline service provided by Google that performs fuzzing during continuous\nintegration. Through an empirical study of a total of approximately 1.12\nmillion fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal\nthat (i) a substantial number of fuzzing bugs exist prior to the integration of\ncontinuous fuzzing, leading to a high detection rate in the early stages; (ii)\ncode coverage continues to increase as continuous fuzzing progresses; and (iii)\nchanges in coverage contribute to the detection of fuzzing bugs. This study\nprovides empirical insights into how continuous fuzzing contributes to fuzzing\nbug detection, offering practical implications for future strategies and tool\ndevelopment in continuous fuzzing.", "AI": {"tldr": "This study analyzes continuous fuzzing's role in vulnerability detection using OSS-Fuzz data, finding high early detection rates, increasing coverage over time, and coverage changes contributing to bug detection.", "motivation": "To understand how continuous fuzzing contributes to vulnerability detection despite widespread adoption, as it's unclear how effective it is in practice.", "method": "Collected issue reports, coverage reports, and fuzzing logs from OSS-Fuzz (Google's continuous fuzzing service), analyzing approximately 1.12 million fuzzing sessions from 878 projects.", "result": "(i) Many fuzzing bugs existed before continuous fuzzing integration, leading to high early detection rates; (ii) Code coverage increases progressively with continuous fuzzing; (iii) Coverage changes contribute to fuzzing bug detection.", "conclusion": "Continuous fuzzing effectively contributes to vulnerability detection through increasing coverage and coverage changes, providing empirical insights for future strategies and tool development."}}
{"id": "2510.16502", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16502", "abs": "https://arxiv.org/abs/2510.16502", "authors": ["Sebasti\u00e1n Pizard", "Ramiro Moreira", "Federico Galiano", "Ignacio Sastre", "Lorena Etcheverry"], "title": "On the Use of Large Language Models for Qualitative Synthesis", "comment": null, "summary": "Large language models (LLMs) show promise for supporting systematic reviews\n(SR), even complex tasks such as qualitative synthesis (QS). However, applying\nthem to a stage that is unevenly reported and variably conducted carries\nimportant risks: misuse can amplify existing weaknesses and erode confidence in\nthe SR findings. To examine the challenges of using LLMs for QS, we conducted a\ncollaborative autoethnography involving two trials. We evaluated each trial for\nmethodological rigor and practical usefulness, and interpreted the results\nthrough a technical lens informed by how LLMs are built and their current\nlimitations.", "AI": {"tldr": "Examining challenges of using LLMs for qualitative synthesis in systematic reviews through collaborative autoethnography trials.", "motivation": "LLMs show promise for supporting systematic reviews including qualitative synthesis, but applying them to unevenly reported stages carries risks of amplifying weaknesses and eroding confidence in findings.", "method": "Conducted collaborative autoethnography involving two trials, evaluating each for methodological rigor and practical usefulness, interpreted through technical lens of LLM limitations.", "result": "Identified challenges in using LLMs for qualitative synthesis due to uneven reporting and variable conduct in systematic reviews.", "conclusion": "Careful consideration needed when applying LLMs to qualitative synthesis in systematic reviews to avoid amplifying existing weaknesses and maintain confidence in findings."}}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendk\u00fbuni C. Ou\u00e9draogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers.", "AI": {"tldr": "CoReEval is the first large-scale benchmark for evaluating LLM-based code readability assessment, showing that developer-guided prompting improves alignment with human judgments and enhances explanation quality, though with increased score variability.", "motivation": "Code readability is crucial for software maintenance but difficult to assess at scale. Traditional static metrics fail to capture subjective human judgments, and LLMs' behavior as readability evaluators remains underexplored.", "method": "Created CoReEval benchmark with 1.4M evaluations across 10 LLMs, spanning 3 programming languages, 2 code types, 4 prompting strategies, 9 decoding settings, and developer-guided prompts. Compared LLM outputs against human annotations and static models using numerical alignment metrics and justification quality analysis.", "result": "Developer-guided prompting grounded in human-defined readability dimensions improves alignment in structured contexts, enhances explanation quality, and enables lightweight personalization through persona framing. However, increased score variability highlights trade-offs between alignment, stability, and interpretability.", "conclusion": "CoReEval provides a robust foundation for prompt engineering, model alignment studies, and human-in-the-loop evaluation, with applications in education, onboarding, and CI/CD pipelines where LLMs can serve as explainable, adaptable reviewers."}}
{"id": "2510.16665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16665", "abs": "https://arxiv.org/abs/2510.16665", "authors": ["Mohamed Sami Rakha", "Andriy Miranskyy", "Daniel Alencar da Costa"], "title": "Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios", "comment": "Accepted to IEEE Transactions on Software Engineering", "summary": "Software defect prediction (SDP) is crucial for delivering high-quality\nsoftware products. Recent research has indicated that prediction performance\nimprovements in SDP are achievable by applying hyperparameter tuning to a\nparticular SDP scenario. However, the positive impact resulting from the\nhyperparameter tuning step may differ based on the targeted SDP scenario.\nComparing the impact of hyperparameter tuning across SDP scenarios is necessary\nto provide comprehensive insights and enhance the robustness, generalizability,\nand, eventually, the practicality of SDP modeling for quality assurance.\n  Therefore, in this study, we contrast the impact of hyperparameter tuning\nacross two pivotal and consecutive SDP scenarios: (1) Inner Version Defect\nPrediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main\ndistinctions between the two scenarios lie in the scope of defect prediction\nand the selected evaluation setups. This study's experiments use common\nevaluation setups, 28 machine learning (ML) algorithms, 53 post-release\nsoftware datasets, two tuning algorithms, and five optimization metrics. We\napply statistical analytics to compare the SDP performance impact differences\nby investigating the overall impact, the single ML algorithm impact, and\nvariations across different software dataset sizes.\n  The results indicate that the SDP gains within the IVDP scenario are\nsignificantly larger than those within the CVDP scenario. The results reveal\nthat asserting performance gains for up to 24 out of 28 ML algorithms may not\nhold across multiple SDP scenarios. Furthermore, we found that small software\ndatasets are more susceptible to larger differences in performance impacts.\nOverall, the study findings recommend software engineering researchers and\npractitioners to consider the effect of the selected SDP scenario when\nexpecting performance gains from hyperparameter tuning.", "AI": {"tldr": "This study compares the impact of hyperparameter tuning across two software defect prediction scenarios (IVDP and CVDP), finding that performance gains are significantly larger in IVDP than CVDP, and that these gains don't consistently apply across all ML algorithms or dataset sizes.", "motivation": "To provide comprehensive insights into how hyperparameter tuning impacts different SDP scenarios, enhancing the robustness and practicality of SDP modeling for quality assurance.", "method": "Contrasted hyperparameter tuning impact across IVDP and CVDP scenarios using 28 ML algorithms, 53 datasets, two tuning algorithms, and five optimization metrics with statistical analytics.", "result": "SDP gains in IVDP are significantly larger than in CVDP; performance gains don't hold consistently across 24/28 ML algorithms; small datasets show larger performance impact differences.", "conclusion": "Researchers and practitioners should consider the effect of the selected SDP scenario when expecting performance gains from hyperparameter tuning."}}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs.", "AI": {"tldr": "QuanBench is a benchmark for evaluating LLMs on quantum code generation, covering 44 tasks across quantum algorithms, state preparation, gate decomposition, and quantum machine learning. Current LLMs show limited capability with overall accuracy below 40% and frequent semantic errors.", "motivation": "Large language models have shown good performance in general code generation, but their capabilities in quantum code generation remain insufficiently studied, creating a need for systematic evaluation.", "method": "Created QuanBench with 44 programming tasks, each with executable canonical solutions, evaluated using functional correctness (Pass@K) and quantum semantic equivalence (Process Fidelity). Evaluated several recent LLMs including general-purpose and code-specialized models.", "result": "Current LLMs have limited capability in generating correct quantum code, with overall accuracy below 40%. Common failure cases include outdated API usage, circuit construction errors, and incorrect algorithm logic.", "conclusion": "QuanBench provides a basis for future work on improving quantum code generation with LLMs, highlighting the current limitations and need for better quantum programming capabilities in language models."}}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.", "AI": {"tldr": "LLM-powered coding agents face high costs due to inefficient turn usage. This paper analyzes turn-control strategies and finds dynamic-turn allocation reduces costs by 12%-24% while maintaining performance.", "motivation": "Practical deployment of LLM coding agents is hindered by unpredictable costs from quadratic token growth, expensive models, and inefficient turn usage. Existing research focuses on individual turns but neglects strategic turn control.", "method": "Comprehensive empirical study on SWE-bench using three state-of-the-art models, evaluating three turn-control strategies: unrestricted baseline, fixed-turn limit with reminders, and novel dynamic-turn strategy with on-demand extensions.", "result": "Fixed-turn limit at 75th percentile reduces costs by 24%-68% with minimal solve rate impact. Dynamic-turn strategy outperforms fixed limits, achieving comparable/better solve rates while further reducing costs by 12%-24% through intelligent resource allocation.", "conclusion": "Dynamic resource allocation is a superior, easy-to-implement approach for deploying economically viable coding agents, providing systematic guidelines for balancing cost and efficacy."}}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.", "AI": {"tldr": "Many-shot prompting doesn't always improve code translation performance - functional correctness peaks with just 5-25 examples, while more examples can degrade performance despite modest improvements in similarity metrics.", "motivation": "To investigate whether providing many examples (many-shot prompting) actually enhances performance for complex tasks like code translation, challenging the common assumption that \"more is better\" for in-context learning with large language models.", "method": "Large-scale empirical study of over 90,000 code translations, systematically evaluating scaling from zero-shot to many-shot configurations (up to 625 examples) with prompts spanning 100,000 to 800,000 tokens.", "result": "Revealed a \"many-shot paradox\" - while static similarity metrics modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples) and degrades with substantially more examples.", "conclusion": "For code translation, quality of a few well-chosen examples outweighs sheer quantity, challenging universal efficacy of \"more is better\" for ICL and highlighting task-dependent nature of optimal prompting strategies."}}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs.", "AI": {"tldr": "LLMs generate Chrome extensions with high vulnerability rates (18%-50%), particularly in Authentication & Identity (83%) and Cookie Management (78%) scenarios, exposing sensitive browser data despite advanced reasoning models performing worse than simpler ones.", "motivation": "As LLMs increasingly drive software development, developers focus on functionality over security, missing hidden implementation vulnerabilities in framework-constrained programs like Chrome extensions with complex security models.", "method": "Built ChromeSecBench dataset with 140 prompts from known vulnerable extensions, used 9 state-of-the-art LLMs to generate Chrome extensions, analyzed vulnerabilities across scenario types, model differences, and vulnerability categories.", "result": "LLMs produced vulnerable programs at alarmingly high rates (18%-50%), with highest vulnerability rates in Authentication & Identity (83%) and Cookie Management (78%) scenarios. Most vulnerabilities exposed sensitive browser data like cookies, history, or bookmarks to untrusted code.", "conclusion": "There's a critical gap between LLMs' coding skills and their ability to write secure framework-constrained programs, with advanced reasoning models performing worse than simpler models in generating secure code."}}
{"id": "2510.17056", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17056", "abs": "https://arxiv.org/abs/2510.17056", "authors": ["Luis F. G. Campos", "Leonardo C. Marques", "Walter T. Nakamura"], "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection", "comment": "Accepted and to be published in SBQS25 - Brazilian Symposium on\n  Software Quality 2025", "summary": "Usability inspection is a well-established technique for identifying\ninteraction issues in software interfaces, thereby contributing to improved\nproduct quality. However, it is a costly process that requires time and\nspecialized knowledge from inspectors. With advances in Artificial Intelligence\n(AI), new opportunities have emerged to support this task, particularly through\ngenerative models capable of interpreting interfaces and performing inspections\nmore efficiently. This study examines the performance of generative AIs in\nidentifying usability problems, comparing them to those of experienced human\ninspectors. A software prototype was evaluated by four specialists and two AI\nmodels (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,\nand F1-score. While inspectors achieved the highest levels of precision and\noverall coverage, the AIs demonstrated high individual performance and\ndiscovered many novel defects, but with a higher rate of false positives and\nredundant reports. The combination of AIs and human inspectors produced the\nbest results, revealing their complementarity. These findings suggest that AI,\nin its current stage, cannot replace human inspectors but can serve as a\nvaluable augmentation tool to improve efficiency and expand defect coverage.\nThe results provide evidence based on quantitative analysis to inform the\ndiscussion on the role of AI in usability inspections, pointing to viable paths\nfor its complementary use in software quality assessment contexts.", "AI": {"tldr": "AI models (GPT-4o and Gemini 2.5 Flash) show promise in usability inspection but cannot replace human inspectors due to higher false positives and redundant reports. The best results come from combining AI with human expertise.", "motivation": "Usability inspection is costly and requires specialized knowledge. Advances in AI offer opportunities to support this task through generative models that can interpret interfaces more efficiently.", "method": "Evaluated a software prototype using four human specialists and two AI models (GPT-4o and Gemini 2.5 Flash), comparing performance using precision, recall, and F1-score metrics.", "result": "Human inspectors achieved highest precision and overall coverage. AIs demonstrated high individual performance and discovered novel defects but had higher false positive rates and redundant reports. The combination of AI and humans produced best results.", "conclusion": "AI cannot replace human inspectors but serves as valuable augmentation tool to improve efficiency and expand defect coverage. Results support complementary use of AI in software quality assessment."}}
{"id": "2510.17110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17110", "abs": "https://arxiv.org/abs/2510.17110", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs", "comment": "This paper was accepted by ASE2025", "summary": "With the growing interest in quantum computing, the emergence of quantum\nsupremacy has marked a pivotal milestone in the field. As a result, numerous\nquantum programming languages (QPLs) have been introduced to support the\ndevelopment of quantum algorithms. However, the application of Model-Driven\nDevelopment (MDD) in quantum system engineering remains largely underexplored.\nThis paper presents an MDD-based approach to support the structured design and\nimplementation of quantum systems. Our framework enables the automatic\ngeneration of quantum code for multiple QPLs, thereby enhancing development\nefficiency and consistency across heterogeneous quantum platforms. The\neffectiveness and practicality of our approach have been demonstrated through\nmultiple case studies.", "AI": {"tldr": "This paper introduces a Model-Driven Development (MDD) approach for quantum system engineering, enabling automatic code generation for multiple quantum programming languages to improve development efficiency and platform consistency.", "motivation": "Despite the emergence of quantum supremacy and numerous quantum programming languages, Model-Driven Development remains largely unexplored in quantum system engineering, creating a gap for structured development approaches.", "method": "The authors propose an MDD-based framework that supports structured design and implementation of quantum systems, with automatic generation of quantum code for multiple quantum programming languages.", "result": "The effectiveness and practicality of the approach have been demonstrated through multiple case studies, showing enhanced development efficiency and consistency across heterogeneous quantum platforms.", "conclusion": "The MDD-based approach successfully addresses the gap in quantum system engineering by providing a structured development methodology that supports multiple quantum programming languages through automatic code generation."}}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems.", "AI": {"tldr": "SEER is a self-exploring reasoning framework that addresses limitations in Chain-of-Thought code generation by exploring diverse reasoning paths, training quality-aware models, and enabling adaptive reasoning switching.", "motivation": "Current CoT approaches for code generation have limitations: limited exploration of diverse reasoning paths, lack of quality assessment for intermediate steps, and potential negative impact of 'overthinking' leading to complex incorrect solutions.", "method": "SEER frames CoT code generation as decision making with three components: (1) Diverse reasoning path exploration without manual experts, (2) Training policy model for generating reasoning steps and value model for quality assessment, (3) Adaptive switching between direct generation and step-by-step reasoning.", "result": "The framework enables accurate and adaptive reasoning for code generation by addressing the three key limitations of existing approaches.", "conclusion": "SEER provides a comprehensive solution for improving Chain-of-Thought reasoning in code generation through self-exploration, quality assessment, and adaptive reasoning strategies."}}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.", "AI": {"tldr": "Peace is a hybrid framework for project-level code efficiency optimization through automatic code editing, addressing limitations of previous function-level approaches by considering inter-function interactions and ensuring project correctness.", "motivation": "LLMs show strong code generation capabilities but their potential in code efficiency optimization is underexplored. Previous approaches only focused on function-level optimization and ignored interactions between functions, failing to generalize to real-world scenarios.", "method": "Peace integrates three phases: dependency-aware optimizing function sequence construction, valid associated edits identification, and efficiency optimization editing iteration. It uses a hybrid framework for project-level optimization while ensuring overall correctness and integrity.", "result": "Peace achieves 69.2% correctness rate (pass@1), +46.9% optimization rate, and 0.840 speedup in execution efficiency on the PeacExec benchmark with 146 real-world optimization tasks from 47 GitHub Python projects, significantly outperforming all baselines.", "conclusion": "Peace demonstrates superior performance in project-level code efficiency optimization, particularly excelling in complex tasks with multiple functions, and validates the effectiveness of its hybrid framework design through extensive experiments."}}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;", "AI": {"tldr": "TREAT is an evaluation framework for assessing the trustworthiness and reliability of large foundation models in software engineering tasks, addressing limitations in existing benchmarks through multi-task, multi-language, multi-modality, and robustness assessments.", "motivation": "Existing benchmarks for evaluating code foundation models have limited task scope and fail to assess critical aspects like robustness and reliability, creating a gap in comprehensive trustworthiness evaluation for real-world software engineering scenarios.", "method": "The TREAT framework provides holistic assessment through four main improvements: multi-task evaluation across diverse software engineering activities, multi-language and multi-modality assessment, robustness evaluation under code transformations, and rigorous methodology with diverse prompts and adaptive solution extraction.", "result": "Evaluation of 26 state-of-the-art models revealed substantial performance variation across programming tasks and identified specific limitations in multi-modal models for UI code generation and editing.", "conclusion": "The TREAT framework successfully addresses key limitations in existing evaluation approaches and provides comprehensive insights into model trustworthiness, revealing both strengths and limitations of current code foundation models."}}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks.", "AI": {"tldr": "This study investigates how software testing professionals use LLMs in practice and proposes a preliminary guideline for integrating LLMs into testing workflows.", "motivation": "The adoption of large language models in software testing is growing rapidly but often relies on informal experimentation rather than structured guidance, creating a need for practitioner-informed guidelines.", "method": "Qualitative study with 15 software testers from diverse roles and domains using semi-structured interviews and grounded theory-based thematic analysis.", "result": "Testers described an iterative process involving defining objectives, prompt engineering, refinement, output evaluation, and learning over time, with emphasis on human oversight due to LLM limitations like hallucinations.", "conclusion": "LLM adoption in software testing is growing but shaped by evolving practices and risk caution; this study provides a starting point for structured LLM use and invites future research refinement."}}
{"id": "2510.17184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17184", "abs": "https://arxiv.org/abs/2510.17184", "authors": ["Nicolas Robert", "Fabien Gandon", "Maxime Lefran\u00e7ois"], "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development", "comment": null, "summary": "Agile and collaborative approaches to ontologies design are crucial because\nthey contribute to making them userdriven, up-to-date, and able to evolve\nalongside the systems they support, hence proper continuous validation tooling\nis required to ensure ontologies match developers' requirements all along their\ndevelopment. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV\nWorkflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C\nStandards to assist the development of modular ontologies through GitHub\nComposite Actions, pre-commit hooks, or a command line interface. OLIVAW was\ntested on several ontology projects to ensure its usefulness, genericity and\nreusability. A template repository is available for a quick start. OLIVAW is", "AI": {"tldr": "OLIVAW is a tool supporting the ACIMOV methodology for agile ontology development on GitHub, using W3C standards to assist modular ontology development through various interfaces.", "motivation": "Agile and collaborative approaches are needed for ontology design to ensure they remain user-driven, up-to-date, and evolve with systems, requiring continuous validation tooling.", "method": "OLIVAW relies on W3C Standards and supports development through GitHub Composite Actions, pre-commit hooks, or a command line interface.", "result": "OLIVAW was tested on several ontology projects and demonstrated usefulness, genericity, and reusability. A template repository is available.", "conclusion": "OLIVAW provides a practical tool for continuous validation and agile development of ontologies using the ACIMOV methodology on GitHub."}}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.", "AI": {"tldr": "AdapTrack improves code generation by incorporating backtracking to avoid distorting model output intent while maintaining constraint compliance, achieving significant improvements over constrained decoding methods.", "motivation": "Constrained decoding techniques for code generation often distort the model's output intent, producing code that satisfies constraints but doesn't match development intent, leading to incorrect results.", "method": "AdapTrack incorporates backtracking into the generation process to avoid distorting the model's output intent while ensuring constraint compliance. It provides theoretical proof that the distribution aligns with the model's distribution given generated tokens.", "result": "Achieved up to 360.87% improvement on synthetic API completion dataset, 38.93% on real-world API completion dataset, 7.84% on HumanEval, and 6.42% on MBPP benchmarks compared to constrained decoding.", "conclusion": "AdapTrack significantly improves code generation by better adhering to the model's output intent while maintaining constraint compliance, providing more semantically aligned results than constrained decoding methods."}}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.", "AI": {"tldr": "Developed a Scalable CI/CD Pipeline to address Japan's 2025 cliff problem by modernizing legacy IT systems through automated, containerized development environments using GitHub, Jenkins, AWS, and Docker.", "motivation": "Address the Japan 2025 cliff problem where legacy core IT systems reaching end-of-service life threaten to increase maintenance costs by up to 12 trillion yen annually and hinder Digital Transformation progress, as experienced by Asahi with outdated systems and manual workflows.", "method": "Implemented a Scalable CI/CD Pipeline using GitHub for source control, Jenkins for automation, AWS for scalable infrastructure, and Docker for containerization, enabling dynamic creation and deletion of isolated development environments.", "result": "Developers can now freely test maintenance procedures and experiment with new technologies in isolated environments, reducing maintenance costs and driving Digital Transformation.", "conclusion": "The Scalable CI/CD Pipeline successfully addresses legacy system modernization challenges by providing flexible, automated development environments that reduce costs and enable digital transformation progress."}}
