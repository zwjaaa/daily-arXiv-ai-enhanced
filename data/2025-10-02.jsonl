{"id": "2510.00002", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00002", "abs": "https://arxiv.org/abs/2510.00002", "authors": ["Dong Liu"], "title": "PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering", "comment": "184 pages; 35 figures; A DOI-linked version of this paper and all\n  supplementary materials are available on Zenodo at\n  https://zenodo.org/records/16883985", "summary": "This paper introduces Primary Breadth-First Development (PBFD) and Primary\nDepth-First Development (PDFD), two formally defined and verified methodologies\nfor scalable, industrial-grade full-stack software engineering. These\napproaches bridge a longstanding gap between formal methods and real-world\ndevelopment practice by enforcing structural correctness through\ngraph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD\noperate over layered directed graphs and are formalized using unified state\nmachines and Communicating Sequential Processes (CSP) to ensure critical\nproperties, including bounded-refinement termination and structural\ncompleteness. To coordinate hierarchical data at scale, we propose Three-Level\nEncapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers\nprovably constant-time updates. TLE's formal guarantees underpin PBFD's\nindustrial-scale performance and scalability. PBFD was empirically validated\nthrough an eight-year enterprise deployment, demonstrating over 20x faster\ndevelopment than Salesforce OmniScript and 7-8x faster query performance\ncompared to conventional relational models. Additionally, both methodologies\nare supported by open-source MVPs, with PDFD's implementation conclusively\ndemonstrating its correctness-first design principles. Together, PBFD and PDFD\nestablish a reproducible, transparent framework that integrates formal\nverification into practical software development. All formal specifications,\nMVPs, and datasets are publicly available to foster academic research and\nindustrial-grade adoption."}
{"id": "2510.00003", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00003", "abs": "https://arxiv.org/abs/2510.00003", "authors": ["Malte Hansen", "Jens Bamberg", "Noe Baumann", "Wilhelm Hasselbring"], "title": "Semantic Zoom and Mini-Maps for Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization tools can facilitate program comprehension by\nproviding visual metaphors, or abstractions that reduce the amount of textual\ndata that needs to be processed mentally. One way they do this is by enabling\ndevelopers to build an internal representation of the visualized software and\nits architecture. However, as the amount of displayed data in the visualization\nincreases, the visualization itself can become more difficult to comprehend.\nThe ability to display small and large amounts of data in visualizations is\ncalled visual scalability.\n  In this paper, we present two approaches to address the challenge of visual\nscalability in 3D software cities. First, we present an approach to semantic\nzoom, in which the graphical representation of the software landscape changes\nbased on the virtual camera's distance from visual objects. Second, we augment\nthe visualization with a miniature two-dimensional top-view projection called\nmini-map. We demonstrate our approach using an open-source implementation in\nour software visualization tool ExplorViz. ExplorViz is web-based and uses the\n3D city metaphor, focusing on live trace visualization.\n  We evaluated our approaches in two separate user studies. The results\nindicate that semantic zoom and the mini-map are both useful additions. User\nfeedback indicates that semantic zoom and mini-maps are especially useful for\nlarge software landscapes and collaborative software exploration. The studies\nindicate a good usability of our implemented approaches. However, some\nshortcomings in our implementations have also been discovered, to be addressed\nin future work.\n  Video URL: https://youtu.be/LYtUeWvizjU"}
{"id": "2510.00004", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00004", "abs": "https://arxiv.org/abs/2510.00004", "authors": ["Malte Hansen", "David Moreno-Lumbreras", "Wilhelm Hasselbring"], "title": "HTML Structure Exploration in 3D Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization, which uses data from dynamic program analysis, can\nhelp to explore and understand the behavior of software systems. It is common\nthat large software systems offer a web interface for user interaction.\nUsually, available web interfaces are not regarded in software visualization\ntools. This paper introduces additions to the web-based live tracing software\nvisualization tool ExplorViz: We add an embedded web view for instrumented\napplications in the 3D visualization to ease interaction with the given\napplications and enable the exploration of the thereby displayed HTML content.\nNamely, the Document Object Model (DOM) is visualized via a three-dimensional\nrepresentation of the HTML structure in same-origin contexts.\n  Our visualization approach is evaluated in a preliminary user study. The\nstudy results give insights into the potential use cases, benefits, and\nshortcomings of our implemented approach. Based on our study results, we\npropose directions for further research to support the visual exploration of\nweb interfaces and explore use cases for the combined visualization of software\ncities and HTML structure.\n  Video URL: https://youtu.be/wBWKlbvzOOE"}
{"id": "2510.00031", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.00031", "abs": "https://arxiv.org/abs/2510.00031", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "comment": null, "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on\nmulti-agent LLMs for code generation. VibeCodeHPC tunes programs through\nmulti-agent role allocation and iterative prompt refinement. We describe the\nsystem configuration with four roles: Project Manager (PM), System Engineer\n(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent\ndeployment and activity monitoring functions to facilitate effective\nmulti-agent collaboration. In our case study, we convert and optimize CPU-based\nmatrix-matrix multiplication code written in C to GPU code using CUDA. The\nmulti-agent configuration of VibeCodeHPC achieved higher-quality code\ngeneration per unit time compared to a solo-agent configuration. Additionally,\nthe dynamic agent deployment and activity monitoring capabilities facilitated\nmore effective identification of requirement violations and other issues."}
{"id": "2510.00092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00092", "abs": "https://arxiv.org/abs/2510.00092", "authors": ["Shufeng Chen", "Mariat James Elizebeth", "Robab Aghazadeh Chakherlou", "Xingyu Zhao", "Eric Barbier", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0", "comment": null, "summary": "Assurance 2.0 is a modern framework developed to address the assurance\nchallenges of increasingly complex, adaptive, and autonomous systems. Building\non the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable\nassurance theories and explicit counterarguments (defeaters) to enhance rigor,\ntransparency, and adaptability. It supports continuous, incremental assurance,\nenabling innovation without compromising safety. However, limitations persist\nin confidence measurement, residual doubt management, automation support, and\nthe practical handling of defeaters and confirmation bias. This paper presents\n\\textcolor{black}{a set of decomposition frameworks to identify a complete set\nof safety arguments and measure their corresponding evidence.} Grounded in the\nAssurance 2.0 paradigm, the framework is instantiated through a structured\ntemplate and employs a three-tiered decomposition strategy. \\textcolor{black}{A\ncase study regarding the application of the decomposition framework in the\nend-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also\npresented in this paper.} At the top level, the SDV development is divided into\nthree critical phases: Requirements Engineering (RE), Verification and\nValidation (VnV), and Post-Deployment (PD). Each phase is further decomposed\naccording to its Product Development Lifecycle (PDLC). To ensure comprehensive\ncoverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,\nMethod, Material, Measurement, and Environment). Originally developed for\nmanufacturing quality control, the 5M1E model is reinterpreted and contextually\nmapped to the assurance domain. This enables a multi-dimensional decomposition\nthat supports fine-grained traceability of safety claims, evidence, and\npotential defeaters."}
{"id": "2510.00197", "categories": ["cs.SE", "D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00197", "abs": "https://arxiv.org/abs/2510.00197", "authors": ["Diogo Maia", "Filipe Correia", "André Restivo", "Paulo Queiroz"], "title": "Container Orchestration Patterns for Optimizing Resource Use", "comment": null, "summary": "Service-based architectures provide substantial benefits, yet service\norchestration remains a challenge, particularly for newcomers. While various\nresources on orchestration techniques exist, they often lack clarity and\nstandardization, making best practices difficult to implement and limiting\ntheir adoption within the software industry.\n  To address this gap, we analyzed existing literature and tools to identify\ncommon orchestration practices. Based on our findings, we define three key\norchestration resource optimization patterns: {\\sc Preemptive Scheduling}, {\\sc\nService Balancing}, and {\\sc Garbage Collection}. {\\sc Preemptive Scheduling}\nallows the allocation of sufficient resources for services of higher priority\nin stressful situations, while {\\sc Service Balancing} enables a restructuring\nof the nodes to allow better resource usage. To end, {\\sc Garbage Collection}\ncreates cleanup mechanisms to better understand the system's resource usage and\noptimize it. These patterns serve as foundational elements for improving\norchestration practices and fostering broader adoption in service-based\narchitectures."}
{"id": "2510.00324", "categories": ["cs.SE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00324", "abs": "https://arxiv.org/abs/2510.00324", "authors": ["Lucas Roberts", "Denisa Roberts"], "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?", "comment": "Accepted as a full paper at SIGIR-AP 2025", "summary": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}."}
{"id": "2510.00328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00328", "abs": "https://arxiv.org/abs/2510.00328", "authors": ["Ahmed Fawzy", "Amjed Tahir", "Kelly Blincoe"], "title": "Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review", "comment": null, "summary": "AI code generation tools are transforming software development, especially\nfor novice and non-software developers, by enabling them to write code and\nbuild applications faster and with little to no human intervention. Vibe coding\nis the practice where users rely on AI code generation tools through intuition\nand trial-and-error without necessarily understanding the underlying code.\nDespite widespread adoption, no research has systematically investigated why\nusers engage in vibe coding, what they experience while doing so, and how they\napproach quality assurance (QA) and perceive the quality of the AI-generated\ncode. To this end, we conduct a systematic grey literature review of 101\npractitioner sources, extracting 518 firsthand behavioral accounts about vibe\ncoding practices, challenges, and limitations. Our analysis reveals a\nspeed-quality trade-off paradox, where vibe coders are motivated by speed and\naccessibility, often experiencing rapid ``instant success and flow'', yet most\nperceive the resulting code as fast but flawed. QA practices are frequently\noverlooked, with many skipping testing, relying on the models' or tools'\noutputs without modification, or delegating checks back to the AI code\ngeneration tools. This creates a new class of vulnerable software developers,\nparticularly those who build a product but are unable to debug it when issues\narise. We argue that vibe coding lowers barriers and accelerates prototyping,\nbut at the cost of reliability and maintainability. These insights carry\nimplications for tool designers and software development teams. Understanding\nhow vibe coding is practiced today is crucial for guiding its responsible use\nand preventing a broader QA crisis in AI-assisted development."}
{"id": "2510.00450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00450", "abs": "https://arxiv.org/abs/2510.00450", "authors": ["Sheikh Md. Mushfiqur Rahman", "Nasir Eisty"], "title": "Beyond Pass/Fail: The Story of Learning-Based Testing", "comment": null, "summary": "Learning-Based Testing (LBT) merges learning and testing processes to achieve\nboth testing and behavioral adequacy. LBT utilizes active learning to infer the\nmodel of the System Under Test (SUT), enabling scalability for large and\ncomplex programs by requiring only a minimal set of initial test cases. The\ncore principle of LBT is that the SUT's behavior can be thoroughly inferred by\nprogressively generating test cases and subjecting the SUT to testing, thereby\nensuring comprehensive testing. Despite being in its early stages, LBT has a\nsolid foundation of theoretical research demonstrating its efficacy in testing\nboth procedural and reactive programs. This paper provides a systematic\nliterature review of various LBT implementations across different program types\nand evaluates the current state of research in this field. We explore diverse\ntheoretical frameworks, existing tools, and libraries within the LBT domain to\nillustrate the concept's evolution and current research status. Additionally,\nwe examine case studies involving the application of LBT tools in industrial\nsettings, highlighting their potential and effectiveness in commercial software\ntesting. This systematic literature review aims to offer researchers a\ncomprehensive perspective on the inception and development of LBT, presenting\nit as a promising technique in software testing. By unveiling LBT's\nunderutilized potential, this paper seeks to significantly benefit the\npractitioners and research community."}
{"id": "2510.00476", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00476", "abs": "https://arxiv.org/abs/2510.00476", "authors": ["Arushi Sharma", "Vedant Pungliya", "Christopher J. Quinn", "Ali Jannesari"], "title": "Analyzing Latent Concepts in Code Language Models", "comment": null, "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients."}
{"id": "2510.00501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00501", "abs": "https://arxiv.org/abs/2510.00501", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Aishan Liu", "Xianglong Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "and Bin Shi"], "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly used in code\ngeneration tasks across a wide range of applications. However, their\nperformance is often inconsistent across different programming languages (PLs),\nwith low-resource PLs suffering the most due to limited training data. In this\npaper, we present CodeChemist, a novel and efficient framework for test-time\nscaling that enables functional knowledge transfer from high-resource to\nlow-resource PLs using generated test cases. CodeChemist first generates and\nexecutes code in high-resource PLs to create test cases that encapsulate\nfunctional knowledge. It then uses multi-temperature hedged sampling to\ngenerate code snippets in the low-resource PL and selects the best one based on\nthe pass rate of the test cases. Our extensive experiments show that\nCodeChemist outperforms existing test-time scaling approaches, boosting the\nperformance of code generation for low-resource PLs without requiring any model\nretraining."}
{"id": "2510.00519", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00519", "abs": "https://arxiv.org/abs/2510.00519", "authors": ["Hadiza Umar Yusuf", "Khouloud Gaaloul"], "title": "Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems", "comment": null, "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification."}
{"id": "2510.00532", "categories": ["cs.SE", "cs.CR", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.00532", "abs": "https://arxiv.org/abs/2510.00532", "authors": ["Hengcheng Zhu", "Songqiang Chen", "Valerio Terragni", "Lili Wei", "Jiarong Wu", "Yepang Liu", "Shing-Chi Cheung"], "title": "LSPFuzz: Hunting Bugs in Language Servers", "comment": "This paper has been accepted for publication in The 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025)", "summary": "The Language Server Protocol (LSP) has revolutionized the integration of code\nintelligence in modern software development. There are approximately 300 LSP\nserver implementations for various languages and 50 editors offering LSP\nintegration. However, the reliability of LSP servers is a growing concern, as\ncrashes can disable all code intelligence features and significantly impact\nproductivity, while vulnerabilities can put developers at risk even when\nediting untrusted source code. Despite the widespread adoption of LSP, no\nexisting techniques specifically target LSP server testing. To bridge this gap,\nwe present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.\nOur key insight is that effective LSP server testing requires holistic mutation\nof source code and editor operations, as bugs often manifest from their\ncombinations. To satisfy the sophisticated constraints of LSP and effectively\nexplore the input space, we employ a two-stage mutation pipeline: syntax-aware\nmutations to source code, followed by context-aware dispatching of editor\noperations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz\ndemonstrated superior performance compared to baseline fuzzers, and uncovered\npreviously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,\n42 have been confirmed, 26 have been fixed by developers, and two have been\nassigned CVE numbers. Our work advances the quality assurance of LSP servers,\nproviding both a practical tool and foundational insights for future research\nin this domain."}
{"id": "2510.00591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00591", "abs": "https://arxiv.org/abs/2510.00591", "authors": ["Liyi Cai", "Yijie Ren", "Yitong Zhang", "Jia Li"], "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation", "comment": null, "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software."}
{"id": "2510.00674", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00674", "abs": "https://arxiv.org/abs/2510.00674", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "comment": "Accepted at ASE 2025 (Tool Demonstration Track)", "summary": "Dependency bloat is a persistent challenge in Python projects, which\nincreases maintenance costs and security risks. While numerous tools exist for\ndetecting unused dependencies in Python, removing these dependencies across the\nsource code and configuration files of a project requires manual effort and\nexpertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to\nautomate this process. PYTRIM eliminates unused imports and package\ndeclarations across a variety of file types, including Python source and\nconfiguration files such as requirements.txt and setup.py. PYTRIM's modular\ndesign makes it agnostic to the source of dependency bloat information,\nenabling integration with any detection tool. Beyond its contribution when it\ncomes to automation, PYTRIM also incorporates a novel dynamic analysis\ncomponent that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset\nof 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%\naccuracy in replicating human-made changes. To show its practical impact, we\nrun PYTRIM on 971 open-source packages, identifying and trimming bloated\ndependencies in 39 of them. For each case, we submit a corresponding pull\nrequest, 6 of which have already been accepted and merged. PYTRIM is available\nas an open-source project, encouraging community contributions and further\ndevelopment.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim"}
{"id": "2510.00680", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00680", "abs": "https://arxiv.org/abs/2510.00680", "authors": ["Hang Cui", "Jingjing Li", "Haotian Si", "Quan Zhou", "Changhua Pei", "Gaogang Xie", "Dan Pei"], "title": "TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies", "comment": null, "summary": "Time series anomaly detection (TSAD) is critical for maintaining the\nreliability of modern IT infrastructures, where complex anomalies frequently\narise in highly dynamic environments. In this paper, we present TShape, a novel\nframework designed to address the challenges in industrial time series anomaly\ndetection. Existing methods often struggle to detect shapelet anomalies that\nmanifest as complex shape deviations, which appear obvious to human experts but\nprove challenging for machine learning algorithms. TShape introduces a\npatch-wise dual attention mechanism with multi-scale convolution to model\nintricate sub-sequence variations by balancing local, fine-grained shape\nfeatures with global contextual dependencies. Our extensive evaluation on five\ndiverse benchmarks demonstrates that TShape outperforms existing\nstate-of-the-art models, achieving an average 10\\% F1 score improvement in\nanomaly detection. Additionally, ablation studies and attention visualizations\nconfirm the essential contributions of each component, highlighting the\nrobustness and adaptability of TShape to complex shapelet shapes in time series\ndata."}
{"id": "2510.00730", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00730", "abs": "https://arxiv.org/abs/2510.00730", "authors": ["Larissa Schmid", "Elias Lundell", "Yogya Gamage", "Benoit Baudry", "Martin Monperrus"], "title": "Maven-Lockfile: High Integrity Rebuild of Past Java Releases", "comment": null, "summary": "Modern software projects depend on many third-party libraries, complicating\nreproducible and secure builds. Several package managers address this with the\ngeneration of a lockfile that freezes dependency versions and can be used to\nverify the integrity of dependencies. Yet, Maven, one of the most important\npackage managers in the Java ecosystem, lacks native support for a lockfile. We\npresent Maven-Lockfile to generate and update lockfiles, with support for\nrebuilding projects from past versions. Our lockfiles capture all direct and\ntransitive dependencies with their checksums, enabling high integrity builds.\nOur evaluation shows that Maven-Lockfile can reproduce builds from historical\ncommits and is able to detect tampered artifacts. With minimal configuration,\nMaven-Lockfile equips Java projects with modern build integrity and build\nreproducibility, and fosters future research on software supply chain security\nin Java."}
{"id": "2510.00762", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00762", "abs": "https://arxiv.org/abs/2510.00762", "authors": ["Rudrajit Choudhuri", "Carmen Badea", "Christian Bird", "Jenna Butler", "Rob DeLine", "Brian Houck"], "title": "AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work", "comment": null, "summary": "Generative AI is reshaping software work, yet we lack clear guidance on where\ndevelopers most need and want support, and how to design it responsibly. We\nreport a large-scale, mixed-methods study of N=860 developers that examines\nwhere, why, and how they seek or limit AI help, providing the first task-aware,\nempirically validated mapping from developers' perceptions of their tasks to AI\nadoption patterns and responsible AI priorities. Using cognitive appraisal\ntheory, we show that task evaluations predict openness to and use of AI,\nrevealing distinct patterns: strong current use and a desire for improvement in\ncore work (e.g., coding, testing); high demand to reduce toil (e.g.,\ndocumentation, operations); and clear limits for identity- and\nrelationship-centric work (e.g., mentoring). Priorities for responsible AI\nsupport vary by context: reliability and security for systems-facing tasks;\ntransparency, alignment, and steerability to maintain control; and fairness and\ninclusiveness for human-facing work. Our results offer concrete, contextual\nguidance for delivering AI where it matters to developers and their work."}
{"id": "2510.00881", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00881", "abs": "https://arxiv.org/abs/2510.00881", "authors": ["Patrizio Migliarini", "Mashal Afzal Memon", "Marco Autili", "Paola Inverardi"], "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning", "comment": "Accepted at ASE 2025", "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling."}
{"id": "2510.00920", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00920", "abs": "https://arxiv.org/abs/2510.00920", "authors": ["Songqiang Chen", "Congying Xu", "Jingyi Chen", "Jialun Cao", "Jiarong Wu", "Shing-Chi Cheung"], "title": "On Effective Semantic Translation for Code: A Study Based on Pseudocode", "comment": null, "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode."}
{"id": "2510.00946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00946", "abs": "https://arxiv.org/abs/2510.00946", "authors": ["Shiza Andleeb", "Brandon Kantorski", "Jeffrey C. Carver"], "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions", "comment": "Accepted to SIGCITE'25", "summary": "Background: Large language models (LLMs) such as ChatGPT are increasingly\nused in introductory programming courses to provide real-time code generation,\ndebugging, and explanations. While these tools can boost productivity and code\nquality, concerns remain about over-reliance and potential impacts on\nconceptual learning. Objective: To investigate how ChatGPT access affects code\nquality, conceptual understanding, task completion times, and student\nperceptions in a CS1 course. Methods: We conducted a counterbalanced,\nquasi-experimental study in which students alternated between ChatGPT and\nnon-ChatGPT conditions across two programming assignments in C (functions and\nstructures). We evaluated their code submissions using multidimensional\nrubrics, conceptual post-surveys, and task completion time. Results: Students\nwho had access to ChatGPT produced significantly higher rubric scores for code\nquality and completed tasks in less time compared to those without access.\nHowever, gains in conceptual understanding were mixed, lower for the functions\ntopic but higher for the structures topic. Students reported positive\nexperiences with ChatGPT, citing its value for debugging and practice, while\nexpressing concerns about accuracy and long-term skill development.\nConclusions: ChatGPT can enhance code quality and efficiency for novice\nprogrammers, but may not uniformly improve conceptual understanding. Structured\nintegration and complementary instructional strategies are recommended to\nfoster independent problem-solving skills."}
{"id": "2510.00957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00957", "abs": "https://arxiv.org/abs/2510.00957", "authors": ["Shiza Andleeb", "Teo Mendoza", "Lucas Cordova", "Gursimran Walia", "Jeffrey C. Carver"], "title": "Enhancing Software Testing Education: Understanding Where Students Struggle", "comment": "Accepted to SIGCITE'25", "summary": "Effective software testing is critical for producing reliable and secure\nsoftware, yet many computer science students struggle to master the\nfoundational concepts required to construct comprehensive test suites. While\nautomated feedback tools are widely used to support student learning, it\nremains unclear which testing concepts are most frequently misunderstood and\nhow these misunderstandings are reflected in students' test suite revisions.\nThis study examines the specific testing concepts that lead students to make\nineffective changes, those that fail to improve code coverage, during test\nsuite development. Leveraging an automated feedback tool in a senior-level\nsoftware testing course, we analyzed student submissions from two assignments\nto identify prevalent conceptual gaps and patterns of unproductive\nmodification. Our results reveal that decision coverage and exception handling\nare persistent challenges, and that students most often make superficial or\nmethod-level changes that do not enhance coverage. These findings provide\nactionable insights for educators, researchers, and tool designers. By\npinpointing the concepts that most often contribute to poor testing outcomes,\nwe can refine feedback systems, target instruction to address persistent\nmisconceptions, and more effectively support students in developing robust,\nmaintainable test suites."}
{"id": "2510.01002", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01002", "abs": "https://arxiv.org/abs/2510.01002", "authors": ["Chengran Yang", "Ting Zhang", "Jinfeng Jiang", "Xin Zhou", "Haoye Tian", "Jieke Shi", "Junkai Chen", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework", "comment": null, "summary": "Current learning-based Automated Vulnerability Repair (AVR) approaches, while\npromising, often fail to generalize effectively in real-world scenarios. Our\ndiagnostic analysis reveals three fundamental weaknesses in state-of-the-art\nAVR approaches: (1) limited cross-repository generalization, with performance\ndrops on unseen codebases; (2) inability to capture long-range dependencies,\ncausing a performance degradation on complex, multi-hunk repairs; and (3)\nover-reliance on superficial lexical patterns, leading to significant\nperformance drops on vulnerabilities with minor syntactic variations like\nvariable renaming.\n  To address these limitations, we propose SeCuRepair, a semantics-aligned,\ncurriculum-driven, and reasoning-enhanced framework for vulnerability repair.\nAt its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model\nto articulate why and how a vulnerability should be fixed before generating the\npatch. This explicit reasoning enforces a genuine understanding of repair logic\nrather than superficial memorization of lexical patterns. SeCuRepair also moves\nbeyond traditional supervised fine-tuning and employs semantics-aware\nreinforcement learning, rewarding patches for their syntactic and semantic\nalignment with the oracle patch rather than mere token overlap. Complementing\nthis, a difficulty-aware curriculum progressively trains the model, starting\nwith simple fixes and advancing to complex, multi-hunk coordinated edits.\n  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly\ncrafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all\nbaselines, surpassing the best-performing baselines by 34.52% on BigVul and\n31.52% on PrimeVul\\textsubscript{AVR} in terms of CodeBLEU, respectively.\nComprehensive ablation studies further confirm that each component of our\nframework contributes to its final performance."}
{"id": "2510.01003", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers."}
{"id": "2510.01024", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01024", "abs": "https://arxiv.org/abs/2510.01024", "authors": ["Elvis Júnior", "Alan Valejo", "Jorge Valverde-Rebaza", "Vânia de Oliveira Neves"], "title": "GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation", "comment": "Preprint of a paper published at the 39th Brazilian Symposium on\n  Software Engineering (SBES 2025). Please cite the published version:\n  https://sol.sbc.org.br/index.php/sbes/article/view/37006", "summary": "Software testing is essential to ensure system quality, but it remains\ntime-consuming and error-prone when performed manually. Although recent\nadvances in Large Language Models (LLMs) have enabled automated test\ngeneration, most existing solutions focus on unit testing and do not address\nthe challenges of end-to-end (E2E) testing, which validates complete\napplication workflows from user input to final system response. This paper\nintroduces GenIA-E2ETest, which leverages generative AI to generate executable\nE2E test scripts from natural language descriptions automatically. We evaluated\nthe approach on two web applications, assessing completeness, correctness,\nadaptation effort, and robustness. Results were encouraging: the scripts\nachieved an average of 77% for both element metrics, 82% for precision of\nexecution, 85% for execution recall, required minimal manual adjustments\n(average manual modification rate of 10%), and showed consistent performance in\ntypical web scenarios. Although some sensitivity to context-dependent\nnavigation and dynamic content was observed, the findings suggest that\nGenIA-E2ETest is a practical and effective solution to accelerate E2E test\nautomation from natural language, reducing manual effort and broadening access\nto automated testing."}
{"id": "2510.01077", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01077", "abs": "https://arxiv.org/abs/2510.01077", "authors": ["Daniele Bifolco", "Guido Annicchiarico", "Pierluigi Barbiero", "Massimiliano Di Penta", "Fiorella Zampetti"], "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code", "comment": "Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2025), November 16-20 2025, Seoul, South\n  Korea", "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw"}
{"id": "2510.01096", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01096", "abs": "https://arxiv.org/abs/2510.01096", "authors": ["Nathan Wintersgill", "Trevor Stalnaker", "Daniel Otten", "Laura A. Heymann", "Oscar Chaparro", "Massimiliano Di Penta", "Daniel M. German", "Denys Poshyvanyk"], "title": "Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools", "comment": null, "summary": "Most modern software products incorporate open-source components, requiring\ndevelopment teams to maintain compliance with each component's licenses.\nNoncompliance can lead to significant financial, legal, and reputational\nrepercussions. While some organizations may seek advice from legal\npractitioners to assist with licensing tasks, developers still play a key role\nin such a process. To this end, it is essential to understand how developers\napproach license compliance tasks, the challenges they encounter, and the tools\nthat they use. This work studies these aspects of software licensing practices\nthrough a study - conducted by a joint team of software engineering and legal\nresearchers - consisting of a survey with 58 software developers and seven\nfollow-up interviews. The study resulted in 15 key findings regarding the\ncurrent state of practice. We discuss the implications of our findings and\noffer directions for future research as well as actionable recommendations for\nlicensing tools."}
{"id": "2510.01182", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01182", "abs": "https://arxiv.org/abs/2510.01182", "authors": ["Shuqing Li", "Chenran Zhang", "Binchang Li", "Cuiyun Gao", "Michael R. Lyu"], "title": "When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems", "comment": null, "summary": "Multi-user Extended Reality (XR) systems enable transformative shared\nexperiences but introduce unique software defects that compromise user\nexperience. Understanding software defects in multi-user XR systems is crucial\nfor enhancing system reliability, yet remains underexplored. To fill the gap,\nthis paper presents the first large-scale empirical study of multi-user XR\ndefects, analyzing 2,649 real-world bug reports from diverse sources, including\ndeveloper forums, GitHub repositories, and app reviews on mainstream XR app\nstores. Through rigorous qualitative analysis using iterative open coding, we\ndevelop a comprehensive taxonomy that classifies multi-user XR bugs along three\ndimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.\nOur findings reveal that synchronization inconsistencies and avatar-related\nanomalies are the most prevalent symptoms, while network/synchronization logic\ndefects and session management flaws emerge as dominant root causes.\nCritically, over 34% of analyzed bugs lead to severe consequences that\nfundamentally break the shared experience, including system crashes, persistent\ndisconnections, and complete interaction breakdowns, etc. We also identify\nconcerning privacy and health implications unique to multi-user XR contexts.\nBased on our findings of defect analysis, we provide actionable recommendations\nfor developers, platform vendors, and researchers. Our results demonstrate that\nmulti-user XR systems face distinct challenges at the intersection of\ndistributed systems, real-time 3D interaction, and immersive experiences,\nnecessitating specialized approaches to testing, debugging, and quality\nassurance."}
