<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: GenAI adoption increases developer burnout by raising job demands, but job resources and positive perceptions of GenAI can mitigate these negative effects.


<details>
  <summary>Details</summary>
Motivation: To investigate how Generative AI adoption affects developers' well-being and burnout, moving beyond just productivity gains to understand the psychological impacts.

Method: Mixed-methods study using JD-R model: surveyed 442 developers, applied PLS-SEM and regression analysis, complemented with qualitative analysis of open-ended responses.

Result: GenAI adoption increases burnout through heightened job demands, but job resources and positive perceptions of GenAI help reduce these negative effects.

Conclusion: GenAI adoption presents both challenges (increased job demands leading to burnout) and opportunities (when supported by adequate resources and positive perceptions).

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [2] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs.jar is the first dataset dedicated to real-world hot fixes, containing 679 manually validated hot fixes from Apache projects, with 110 reproducible cases.


<details>
  <summary>Details</summary>
Motivation: No existing evaluation benchmark focuses specifically on hot fixes, which are urgent, unplanned changes deployed to address time-critical issues in production systems.

Method: Mined 10 active Apache projects with over 190K commits and 150K issue reports, identified 746 potential hot fixes, manually validated them, and packaged reproducible cases with buggy/fixed versions, test suites, and metadata.

Result: Created HotBugs.jar dataset with 679 confirmed hot fixes (110 reproducible), adopted as official challenge dataset for SBSE Conference Challenge Track.

Conclusion: This benchmark enables study and evaluation of tools for rapid debugging, automated repair, and production-grade resilience, driving research in this essential area forward.

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [3] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure is a system that uses LLMs to automatically transpile C code to Rust, employing prompt engineering for idiomatic code generation and differential symbolic testing to verify semantic equivalence between original C and transpiled Rust code.


<details>
  <summary>Details</summary>
Motivation: To leverage Rust's memory safety benefits for existing C codebases by automating the transpilation process while ensuring the generated Rust code is both safe and semantically equivalent to the original C code.

Method: Uses Large Language Models with prompt engineering techniques to generate Rust code from C, followed by differential symbolic testing to verify semantic similarity between the original C and transpiled Rust functions.

Result: Successfully generated compilable Rust functions for 89.8% of C functions, with 69.9% of those producing equivalent symbolic return values in both C and Rust versions.

Conclusion: RustAssure demonstrates effective automated transpilation from C to Rust using LLMs, with high success rates for generating compilable code and verifying semantic equivalence through symbolic testing.

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [4] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: APPFORGE is a benchmark for evaluating LLMs' ability to build complete Android apps from scratch, testing system-level reasoning across app states, lifecycle management, and asynchronous operations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on isolated function-level code generation, but real-world software development requires reasoning over entire systems with multiple components, states, and framework constraints.

Method: Created a benchmark of 101 Android app problems from real-world apps, using a multi-agent system to summarize functionalities and synthesize test cases, with expert verification and automated evaluation framework.

Result: Evaluation of 12 flagship LLMs showed poor performance, with the best model (GPT-5) achieving only 18.8% functionally correct applications.

Conclusion: Current LLMs have fundamental limitations in handling complex, multi-component software engineering challenges, highlighting the gap between function-level coding and system-level development.

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [5] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: FLEX is a self-adaptive fuzzing framework for MLIR that uses neural networks and feedback-driven augmentation to generate diverse, semantically valid test cases, significantly outperforming existing fuzzers in bug detection and code coverage.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzing approaches for MLIR struggle to generate diverse and semantically valid test cases, making it difficult to expose subtle bugs in MLIR's complex and evolving codebase.

Method: FLEX leverages neural networks for program generation, perturbed sampling strategy for diversity, and a feedback-driven augmentation loop that iteratively improves the model using both crashing and non-crashing test cases.

Result: In 30-day testing, FLEX discovered 80 previously unknown bugs (including multiple new root causes and parser bugs). In 24-hour comparisons, it detected 53 bugs (3.5x more than best baseline) and achieved 28.2% code coverage (42% better than next-best tool).

Conclusion: FLEX effectively addresses MLIR fuzzing challenges through neural program generation and adaptive learning, with ablation studies confirming the importance of perturbed generation and diversity augmentation.

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [6] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut extracts compiler fuzzing mutators from historical bug reports to improve bug detection in compilers like GCC and LLVM.


<details>
  <summary>Details</summary>
Motivation: Compiler bugs have significant negative impacts, and existing mutational fuzzers don't leverage historical bug reports as a source of mutators, missing potential bug patterns.

Method: Automated mining of mutators from 1760 GCC and LLVM bug reports, then retrofitting these mutators into existing mutational compiler fuzzers.

Result: 587 mutators mined from bug reports found 28 new bugs in GCC and 37 in LLVM, with 60 confirmed or fixed - outperforming state-of-the-art fuzzers.

Conclusion: Bug histories contain rich information that compiler fuzzers should leverage, as demonstrated by IssueMut's effectiveness in finding new bugs.

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [7] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: This study analyzes 180 automotive SoC vulnerabilities in AUTOSAR-aligned architectures, identifying 16 root causes, 56 affected modules, and examining mitigation delays to provide security insights for automotive CPS platforms.


<details>
  <summary>Details</summary>
Motivation: There is a surge in SoC-related vulnerabilities in automotive systems, but systematic analysis of root causes and impacts within AUTOSAR-aligned architectures is lacking, particularly for real-time, safety-critical environments.

Method: Analyzed 180 publicly reported automotive SoC vulnerabilities mapped to a representative SoC software architecture model aligned with AUTOSAR principles for layered abstraction and service orientation.

Result: Identified 16 root causes and 56 affected software modules, examined mitigation delays across CWE categories and architectural layers, uncovered dominant vulnerability patterns and critical modules with prolonged patch delays.

Conclusion: Provides actionable insights for securing automotive CPS platforms, including guides for improved detection, prioritization, and localization strategies for SoC software architectures in vehicle platforms.

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [8] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: An AI-powered bug tracking framework that uses LLMs to automate bug reporting, reproduction, classification, and patching, reducing time-to-fix and human overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional bug tracking systems are slow, require significant manual coordination between non-technical users and technical teams, and have long response times that delay fixes and cause user frustration.

Method: Proposes an AI-powered framework using large language models (LLMs) to automate bug tracking processes. Users report in natural language, AI agents refine reports, attempt reproduction, classify issues, resolve invalid ones with no-code fixes, localize valid bugs, and generate candidate patches with human oversight.

Result: The framework accelerates response times, improves collaboration between stakeholders, and strengthens software maintenance practices by integrating automation into each phase of bug tracking.

Conclusion: AI-powered automation can transform bug tracking into a more efficient, user-centric process by reducing manual overhead and speeding up the entire bug resolution lifecycle.

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [9] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: A technique for composing whitespace-insensitive language modules to construct whitespace-sensitive languages through pre-processing, evaluated by reconstructing a simplified Python version.


<details>
  <summary>Details</summary>
Motivation: The gap between whitespace-sensitive and whitespace-insensitive languages inhibits reusability of modular language components, forcing developers to build whitespace-sensitive languages from scratch.

Method: Pre-processing language artifacts before parsing to enable the use of modular, whitespace-insensitive language modules for constructing whitespace-sensitive languages.

Result: Successfully reconstructed a simplified version of Python using the approach, demonstrating feasibility.

Conclusion: The solution increases reusability of existing language components, reducing development time and improving software language quality.

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>
