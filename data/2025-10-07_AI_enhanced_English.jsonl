{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%.", "AI": {"tldr": "This paper presents Arodnap, a system that improves resource leak repair by handling resource wrappers through specification inference, program transformation, field containment analysis, and new repair patterns.", "motivation": "Prior resource leak repair approaches were limited to hard-coded library resource types and couldn't handle resource wrappers that store resources in fields, limiting their effectiveness in real-world code.", "method": "The approach integrates resource management specification inference, program transformation for easier analysis, field containment analysis for reasoning about resource lifetimes, and introduces new repair patterns for non-final fields.", "result": "Arodnap fixes 68% of resource leak warnings in the NJR benchmark suite, significantly improving over prior work that fixed only 41%.", "conclusion": "The integrated approach with specification inference, program transformation, and enhanced field analysis enables more effective resource leak repair in real-world code with resource wrappers."}}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature.", "AI": {"tldr": "ALMAS is an Autonomous LLM-based Multi-Agent Software Engineering framework that integrates with agile software development teams to perform end-to-end tasks across the software development life-cycle.", "motivation": "Current LLM systems in software development focus mainly on code implementation, testing, and maintenance, but software development is multifaceted and requires consideration of multiple SDLC stages.", "method": "Proposes ALMAS framework with agents aligned to agile roles, designed to work modularly with human developers and their development environment.", "result": "Demonstrated through published works and a use case where ALMAS successfully generated an application and added a new feature seamlessly.", "conclusion": "ALMAS represents a vision for autonomous LLM-based multi-agent systems that can effectively integrate with agile software development teams across the entire SDLC."}}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks.", "AI": {"tldr": "Proposes predicting relative code comprehensibility (which of two snippets is easier to understand) instead of absolute comprehensibility, achieving much better performance than traditional approaches.", "motivation": "Existing code comprehensibility metrics and ML models show poor correlation with actual human understanding due to inherent noise in human comprehensibility data.", "method": "Train ML models to predict relative comprehensibility between two code snippets rather than absolute comprehensibility scores, using a dataset of 150 Java snippets with 12.5k human measurements.", "result": "Relative comprehensibility models achieved 137.8% and 74.7% average improvements for snippet-wise and developer-wise prediction, while absolute models improved by at most 33.4% and often underperformed.", "conclusion": "Relative comprehensibility prediction is more effective than absolute prediction and supports practical applicability for software engineering tasks like refactoring assessment."}}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods.", "AI": {"tldr": "A framework of LLM agents that automatically updates Java library dependencies by localizing outdated usages and applying fixes using migration documentation, achieving 71.4% precision with fewer tokens than state-of-the-art methods.", "motivation": "As codebases expand, library dependencies become outdated, requiring updates for innovation and security. Manual updates are time-consuming due to breaking changes, necessitating automated solutions.", "method": "Uses a multi-agent framework (Summary Agent, Control Agent, Code Agent) combined with migration documentation to automatically localize and update library usages in Java codebases.", "result": "Achieved 71.4% precision in code updates and used fewer tokens than state-of-the-art methods across three synthetic code repositories with major upgrade changes.", "conclusion": "The LLM agent framework provides an efficient and effective automated solution for library dependency updates, outperforming existing methods in both token efficiency and precision."}}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin L\u00e4ufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries.", "AI": {"tldr": "AgentHub proposes a research agenda for building comprehensive infrastructure for LLM-based agent sharing, addressing current fragmentation in discovery, evaluation, and governance compared to mature software ecosystems.", "motivation": "Current infrastructure for LLM-based agents is fragmented, focusing narrowly on distribution, naming, or protocols, while broader software engineering requirements are needed to improve open-source distribution and reuse.", "method": "Proposes AgentHub as a research agenda that frames key challenges including capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.", "result": "Charts a community-wide agenda for building reliable and scalable agent ecosystems, though specific implementation results are not detailed as this is a vision paper.", "conclusion": "Envisions a future where agents can be shared, trusted, and composed as seamlessly as today's software libraries through comprehensive infrastructure development."}}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "AI": {"tldr": "Refine is a patch refinement framework that transforms Draft Patches from LLM-based APR systems into correct patches by addressing context ambiguity, diversifying candidates, and aggregating partial fixes through LLM-powered code review.", "motivation": "Current LLM-based APR techniques struggle to produce correct fixes due to limited code context understanding and over-reliance on incomplete test suites, often generating Draft Patches that are partially correct but not fully functional.", "method": "Refine addresses three challenges: disambiguating vague issue and code context, diversifying patch candidates through test-time scaling, and aggregating partial fixes via an LLM-powered code review process. It's implemented as a general refinement module for APR systems.", "result": "On SWE-Bench Lite, Refine achieves state-of-the-art results (51.67%) among workflow-based approaches, boosting AutoCodeRover's performance by 14.67%. On SWE-Bench Verified, it improves resolution rate by 12.2%, with average 14% improvement across multiple APR systems.", "conclusion": "Refine demonstrates the effectiveness of refinement as a missing component in APR pipelines and the potential of agentic collaboration in closing the gap between near-correct and correct patches."}}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG.", "AI": {"tldr": "Proposes a method for automatically generating high-level test cases from requirement documents using LLMs without RAG, by first generating test design techniques and then test cases for each technique.", "motivation": "Manual generation of high-level test cases is labor-intensive, and current LLM approaches using RAG require custom knowledge systems for each application, which is not scalable.", "method": "Two-step approach: 1) Input requirement document to LLM to generate test design techniques, 2) Generate high-level test cases for each technique. Uses semantic similarity for evaluation.", "result": "Achieved macro-recall of 0.81 on Bluetooth dataset and 0.37 on Mozilla dataset, showing feasibility for practical application.", "conclusion": "The proposed method is feasible for practical high-level test case generation without RAG, providing a more generalized approach across different requirement documents."}}
{"id": "2510.03712", "categories": ["cs.SE", "68M15, 90B25, 68T05, 90C29", "C.4; C.2.4; D.2.5; D.4.5"], "pdf": "https://arxiv.org/pdf/2510.03712", "abs": "https://arxiv.org/abs/2510.03712", "authors": ["Jahidul Arafat", "Kh. M. Moniruzzaman", "Shamim Hossain", "Fariha Tasmin", "Kamrujjaman", "Ahsan Habib Tareq"], "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems", "comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios", "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.", "AI": {"tldr": "A framework for detecting and preventing latent risks in distributed systems caused by aggressive optimizations, using mathematical modeling, perturbation testing, and risk-aware optimization.", "motivation": "Modern distributed systems use aggressive optimizations that create hidden vulnerabilities, where high performance masks catastrophic fragility when optimizations fail. Current reliability engineering focuses on reactive incident response rather than proactive detection of optimization-induced vulnerabilities.", "method": "Integrated framework with three systems: HYDRA (optimization-aware perturbation testing with 6 strategies), RAVEN (continuous production monitoring), and APEX (risk-aware optimization). Introduces Latent Risk Index (LRI) for predictive risk assessment.", "result": "Strong statistical validation: 89.7% risk discovery rate, 92.9% precision and 93.8% recall across 1,748 scenarios, 96.6% baseline performance with 59.2% risk reduction. Production deployment showed 69.1% MTTR reduction, 78.6% incident severity reduction, 81 prevented incidents with 3.2-month ROI.", "conclusion": "Transforms reliability engineering from reactive incident management to proactive risk-aware optimization, providing comprehensive latent risk detection and prevention."}}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs .", "AI": {"tldr": "APIDA-Chat is an open-source pipeline that generates realistic API dialogue data using a teacher-student approach, enabling efficient fine-tuning of smaller models for niche API assistance without exposing proprietary code.", "motivation": "Large language models struggle with niche or proprietary APIs due to lack of training data, and existing solutions risk exposing sensitive code to external services.", "method": "Two-phase approach: Phase I uses a teacher LLM with dialogue planner to create gold dialogues; Phase II fine-tunes a smaller student model (Llama 3.2 3B) that can generate new dialogues independently without external dependencies.", "result": "Fine-tuned student model improves BLEU from 0.38 to 0.50 and BERTScore from 0.88 to 0.91, running entirely on a single consumer GPU.", "conclusion": "APIDA-Chat provides a modular, cost-effective solution for generating API dialogue training data while maintaining code privacy, serving as a baseline for future research."}}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me.", "AI": {"tldr": "Code4MeV2 is an open-source code completion plugin for JetBrains IDEs that addresses the lack of accessible user interaction data from AI code completion tools, featuring modular data collection for research purposes.", "motivation": "Proprietary AI code completion tools limit academic research by keeping user interaction data inaccessible, making reproducible studies and large-scale analysis difficult.", "method": "Developed a client-server plugin with inline code completion and context-aware chat assistant, featuring a modular data collection framework for fine-grained telemetry control.", "result": "Achieves industry-comparable performance with 200ms average latency, validated through expert evaluation and user study with positive feedback from researchers and users.", "conclusion": "Code4MeV2 successfully provides an open-source alternative for studying human-AI interaction in code completion, inviting community adoption and contribution."}}
{"id": "2510.03802", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03802", "abs": "https://arxiv.org/abs/2510.03802", "authors": ["Gilberto Recupito", "Vincenzo De Martino", "Dario Di Nucci", "Fabio Palomba"], "title": "A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt", "comment": "Accepted at the International Workshop of Software Quality Assurance\n  for Artificial Intelligence 2025 (SQA4AI), Montr\\'eal, Canada", "summary": "The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized\nsoftware development, driving innovation across various domains. However, these\nsystems also introduce unique challenges, particularly in maintaining software\nquality and performance. Among these challenges, Self-Admitted Technical Debt\n(SATD) has emerged as a growing concern, significantly impacting the\nmaintainability and overall quality of ML and DL-enabled systems. Despite its\ncritical implications, the lifecycle of DL-specific SATD, how developers\nintroduce, acknowledge, and address it over time-remains underexplored. This\nstudy presents a preliminary analysis of the persistence and lifecycle of\nDL-specific SATD in DL-enabled systems. The purpose of this project is to\nuncover the patterns of SATD introduction, recognition, and durability during\nthe development life cycle, providing information on how to manage these\nissues. Using mining software repository techniques, we examined 40 ML\nprojects, focusing on 185 DL-specific SATD instances. The analysis tracked the\nintroduction and persistence of SATD instances through project commit histories\nto assess their lifecycle and developer actions. The findings indicate that\nDL-specific SATD is predominantly introduced during the early and middle stages\nof project development. Training and Hardware phases showed the longest SATD\ndurations, highlighting critical areas where debt accumulates and persists.\nAdditionally, developers introduce DL-specific SATD more frequently during\nfeature implementation and bug fixes. This study emphasizes the need for\ntargeted DL-specific SATD management strategies in DL-enabled systems to\nmitigate its impact. By understanding the temporal characteristics and\nevolution of DL-specific SATD, developers can prioritize interventions at\ncritical stages to improve the maintainability and quality of the system.", "AI": {"tldr": "This study analyzes the lifecycle of DL-specific Self-Admitted Technical Debt (SATD) in deep learning systems, finding it's introduced early/mid-development, persists longest in training and hardware phases, and occurs frequently during feature implementation and bug fixes.", "motivation": "DL-enabled systems introduce unique challenges to software quality, with SATD emerging as a critical concern affecting maintainability. The lifecycle of DL-specific SATD remains underexplored despite its significant impact.", "method": "Used mining software repository techniques to examine 40 ML projects, focusing on 185 DL-specific SATD instances. Tracked SATD introduction and persistence through commit histories to assess lifecycle and developer actions.", "result": "DL-specific SATD is predominantly introduced during early and middle development stages. Training and Hardware phases showed longest SATD durations. Developers introduce SATD more frequently during feature implementation and bug fixes.", "conclusion": "Targeted DL-specific SATD management strategies are needed in DL-enabled systems. Understanding temporal characteristics and evolution of SATD helps prioritize interventions at critical stages to improve system maintainability and quality."}}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "Jos\u00e9 Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Fr\u00f6mmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide.", "AI": {"tldr": "Smart Paste is an IDE feature that uses deep learning to suggest post-paste code edits, deployed at Google with 45% acceptance rate accounting for over 1% of all code written company-wide.", "motivation": "Code pasting is 4x more common than manual typing at Google, and pasted code frequently requires follow-up edits ranging from simple formatting to complex style adjustments and cross-language translations.", "method": "Iteratively developed and scaled an IDE feature using deep learning to predict and suggest post-paste edits, focusing on user experience, system integration, and model capabilities.", "result": "Deployed successfully at Google with overwhelmingly positive feedback, achieving 45% acceptance rate for suggestions, which accounts for over 1% of all code written company-wide at enterprise scale.", "conclusion": "Smart Paste demonstrates a holistic approach to AI feature development covering UX, system integration, and model capabilities, serving as a guide for AI practitioners on scaling such features in development environments."}}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts.", "AI": {"tldr": "Proposes a theoretical framework for standardizing empirical evaluation of LLM-based code generation to address lack of comparability and reproducibility in current studies.", "motivation": "Current empirical evaluation of LLM-based code generation lacks standardization, with studies varying widely in goals, tasks, and metrics, limiting comparability and reproducibility.", "method": "Developed a theoretical framework grounded in prior experience and comparative analysis of recent studies, organizing evaluation around core components like problem sources, quality attributes, and metrics.", "result": "Created a framework that supports structured and systematic experimentation, demonstrated through representative case mappings, with identified opportunities for refinement.", "conclusion": "The framework provides a foundation for standardizing LLM evaluation in software engineering contexts, with plans to evolve it into a more robust and mature tool."}}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches.", "AI": {"tldr": "ACToR is an adversarial LLM agent-based system that translates C code to memory-safe Rust by pitting a generator agent against a discriminator agent, achieving over 90% test pass rate on real-world command line utilities averaging 485 LoC.", "motivation": "Existing C to Rust translation approaches fail on larger codebases (>500 LoC) due to complex program analyses that break frequently, creating a need for more robust translation methods.", "method": "Uses adversarial approach inspired by GANs: generator agent synthesizes/refines Rust translation to pass tests, while discriminator agent finds new failing tests for iterative improvement.", "result": "Successfully translated all 63 real-world command line utilities (avg 485 LoC) with over 90% test pass rate, improving correctness by up to 18.9% compared to non-adversarial baselines.", "conclusion": "ACToR is the first system to reliably translate C programs at this scale, demonstrating the effectiveness of adversarial LLM agent collaboration for automated code translation."}}
{"id": "2510.03890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03890", "abs": "https://arxiv.org/abs/2510.03890", "authors": ["Jose Garcia-Alonso", "Enrique Moguel", "Jaime Alvarado-Valiente", "Javier Romero-Alvarez", "\u00c1lvaro M. Aparicio-Morales", "Juan M. Murillo", "Francisco Javier Cavero", "Adri\u00e1n Romero-Flores", "Alfonso E. Marquez-Chamorro", "Jos\u00e9 Antonio Parejo", "Antonio Ruiz-Cort\u00e9s", "Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Rethinking Services in the Quantum Age: The SOQ Paradigm", "comment": "39 pages, 5 figures, 6 tables", "summary": "Quantum computing is rapidly progressing from theoretical promise to\npractical implementation, offering significant computational advantages for\ntasks in optimization, simulation, cryptography, and machine learning. However,\nits integration into real-world software systems remains constrained by\nhardware fragility, platform heterogeneity, and the absence of robust software\nengineering practices. This paper introduces Service-Oriented Quantum (SOQ), a\nnovel paradigm that reimagines quantum software systems through the lens of\nclassical service-oriented computing. Unlike prior approaches such as Quantum\nService-Oriented Computing (QSOC), which treat quantum capabilities as\nauxiliary components within classical systems, SOQ positions quantum services\nas autonomous, composable, and interoperable entities. We define the\nfoundational principles of SOQ, propose a layered technology stack to support\nits realization, and identify the key research and engineering challenges that\nmust be addressed, including interoperability, hybridity, pricing models,\nservice abstractions, and workforce development. This approach is of vital\nimportance for the advancement of quantum technology because it enables the\nscalable, modular, and interoperable integration of quantum computing into\nreal-world software systems independently and without relying on a dedicated\nclassical environment to manage quantum processing.", "AI": {"tldr": "Service-Oriented Quantum (SOQ) is a new paradigm that treats quantum services as autonomous, composable entities rather than auxiliary components, enabling scalable integration of quantum computing into real-world software systems.", "motivation": "Quantum computing faces integration challenges due to hardware fragility, platform heterogeneity, and lack of robust software engineering practices, limiting its practical implementation in real-world systems.", "method": "SOQ reimagines quantum software through classical service-oriented computing principles, defining foundational principles and proposing a layered technology stack to support autonomous quantum services.", "result": "The paper establishes SOQ as a framework that enables modular, interoperable quantum service integration without relying on dedicated classical environments to manage quantum processing.", "conclusion": "SOQ is vital for advancing quantum technology by providing scalable, modular integration of quantum computing into real-world software systems independently of classical management environments."}}
{"id": "2510.03894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03894", "abs": "https://arxiv.org/abs/2510.03894", "authors": ["Antonios Saravanos"], "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "comment": null, "summary": "The waterfall model, one of the earliest software development methodologies,\nhas played a foundational role in shaping contemporary software engineering\npractices. This paper provides a historical and critical overview of the model,\ntracing its conceptual origins in software engineering, its formalization by\nRoyce, and its evolution through decades of industry adoption and critique.\nAlthough often criticized for its rigidity, shortcomings, and high failure\nrates, the waterfall model persists in specific domains. Its principles\ncontinue to influence contemporary hybrid development frameworks that combine\ntraditional and agile methods. Drawing on a range of scholarly sources, this\nstudy synthesizes key developments in the perception and application of the\nwaterfall model. The analysis highlights how the model has shifted from a\nstandalone framework to a component within modern hybrid methodologies. By\nrevisiting its origins, assessing its present utility, and examining its role\nin contemporary development practices, this paper argues that the waterfall\nmodel remains relevant, not as a relic of the past but as part of context-aware\ndevelopment strategies. The paper contends that the model's enduring relevance\nlies in its adaptability. By recognizing both its limitations and its\nstrengths, and by understanding its integration within hybrid approaches,\npractitioners can make more informed decisions about methodology selection and\nprocess design in diverse development environments.", "AI": {"tldr": "The waterfall model remains relevant as a component in modern hybrid development frameworks, not as a standalone methodology but through its adaptability and integration with agile methods.", "motivation": "To provide a historical and critical overview of the waterfall model's evolution, assess its present utility, and examine its role in contemporary development practices.", "method": "Drawing on scholarly sources to synthesize key developments in the perception and application of the waterfall model, tracing its origins, formalization, and evolution through industry adoption.", "result": "The waterfall model has shifted from a standalone framework to a component within modern hybrid methodologies, maintaining relevance through adaptability and integration with agile approaches.", "conclusion": "The waterfall model's enduring relevance lies in its adaptability and integration within hybrid approaches, allowing practitioners to make informed methodology decisions in diverse development environments."}}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively.", "AI": {"tldr": "MACOG is a multi-agent LLM architecture for Infrastructure-as-Code generation that decomposes the task into specialized agents working together via a shared-blackboard orchestrator to produce syntactically valid, policy-compliant Terraform configurations.", "motivation": "Traditional monolithic LLM approaches for IaC generation often result in syntactic errors, policy violations, and unscalable designs, requiring a more systematic approach.", "method": "Multi-agent architecture with specialized agents (Architect, Provider Harmonizer, Engineer, Reviewer, Security Prover, Cost and Capacity Planner, DevOps, Memory Curator) interacting via shared-blackboard and finite-state orchestrator, incorporating Terraform Plan for validation and Open Policy Agent for policy enforcement.", "result": "MACOG achieves top enhancement on IaC-Eval benchmark: GPT-5 improves from 54.90 (RAG) to 74.02 and Gemini-2.5 Pro from 43.56 to 60.13, with gains on BLEU, CodeBERTScore, and LLM-judge metrics. Ablations show constrained decoding and deploy feedback are critical.", "conclusion": "The multi-agent approach effectively addresses limitations of monolithic LLM generation for IaC, producing higher quality, policy-compliant infrastructure code through specialized decomposition and orchestration."}}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality.", "AI": {"tldr": "This paper explores using instruction strategies based on Martin Fowler's refactoring guidelines to enhance LLMs' ability to perform diverse automated code refactoring tasks, showing improved performance across 61 refactoring types.", "motivation": "Developers often neglect refactoring due to time, effort, and lack of immediate rewards. Existing automated tools are limited in supporting diverse refactoring types, creating a need for more comprehensive automated solutions.", "method": "Leveraged state-of-the-art LLMs (GPT-mini and DeepSeek-V3) with instruction strategies based on Fowler's refactoring guidelines. Designed multiple instruction strategies encoding motivations, procedural steps, and transformation objectives for 61 refactoring types. Evaluated on benchmark examples and real-world GitHub code.", "result": "Instruction designs based on Fowler's guidelines enabled LLMs to successfully perform all benchmark refactoring types and preserve program semantics in real-world settings. Rule-based instructions performed better in specific scenarios, and goal-focused instructions yielded greater code quality improvements than fixed transformation types.", "conclusion": "Instruction strategies inspired by human best-practice guidelines significantly enhance LLMs' refactoring capabilities. The approach successfully automates diverse refactoring tasks while preserving program semantics, with goal-focused instructions showing particular promise for code quality improvement."}}
{"id": "2510.03920", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.03920", "abs": "https://arxiv.org/abs/2510.03920", "authors": ["Ravi Kalluri"], "title": "Why Does the Engineering Manager Still Exist in Agile Software Development?", "comment": "12 pages, 3 figures, 2 tables", "summary": "Although Agile methodologies emphasize decentralized decision-making and team\nautonomy, engineering managers continue to be employed in Agile software\norganizations. This apparent paradox suggests that traditional managerial\nfunctions persist despite the theoretical displacement of managerial hierarchy\nin Agile. This paper explores the persistence of engineering managers through a\nmultidimensional framework encompassing historical context, theoretical\ntensions, organizational realities, empirical evidence, evolving managerial\nroles, and practical implications. A systematic literature review underpins our\nmultifaceted analysis, supplemented by illustrative case studies. We conclude\nby proposing a conceptual model that reconciles Agile principles with\nmanagerial necessity, offering guidance for practitioners, researchers, and\ntool designers. Implications for leadership development, tool integration, and\nfuture research are discussed.", "AI": {"tldr": "Engineering managers persist in Agile organizations despite Agile's emphasis on decentralization, suggesting traditional managerial functions continue to exist alongside Agile principles.", "motivation": "To understand why engineering managers continue to be employed in Agile software organizations despite Agile methodologies theoretically displacing managerial hierarchy through decentralized decision-making and team autonomy.", "method": "Conducted a systematic literature review supplemented by illustrative case studies, using a multidimensional framework that examines historical context, theoretical tensions, organizational realities, empirical evidence, evolving roles, and practical implications.", "result": "Developed a conceptual model that reconciles Agile principles with managerial necessity, providing guidance for practitioners, researchers, and tool designers.", "conclusion": "Traditional managerial functions persist in Agile environments, and the paper offers a framework to integrate Agile principles with managerial roles, with implications for leadership development, tool integration, and future research."}}
{"id": "2510.04078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04078", "abs": "https://arxiv.org/abs/2510.04078", "authors": ["Han Hu", "Wei Minn", "Yonghui Liu", "Jiakun Liu", "Ferdian Thung", "Terry Yue Zhuo", "Lwin Khin Shar", "Debin Gao", "David Lo"], "title": "Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework", "comment": null, "summary": "The permission mechanism in the Android Framework is integral to safeguarding\nthe privacy of users by managing users' and processes' access to sensitive\nresources and operations. As such, developers need to be equipped with an\nin-depth understanding of API permissions to build robust Android apps.\nUnfortunately, the official API documentation by Android chronically suffers\nfrom imprecision and incompleteness, causing developers to spend significant\neffort to accurately discern necessary permissions. This potentially leads to\nincorrect permission declarations in Android app development, potentially\nresulting in security violations and app failures. Recent efforts in improving\npermission specification primarily leverage static and dynamic code analyses to\nuncover API-permission mappings within the Android framework. Yet, these\nmethodologies encounter substantial shortcomings, including poor adaptability\nto Android SDK and Framework updates, restricted code coverage, and a\npropensity to overlook essential API-permission mappings in intricate\ncodebases. This paper introduces a pioneering approach utilizing large language\nmodels (LLMs) for a systematic examination of API-permission mappings. In\naddition to employing LLMs, we integrate a dual-role prompting strategy and an\nAPI-driven code generation approach into our mapping discovery pipeline,\nresulting in the development of the corresponding tool, \\tool{}. We formulate\nthree research questions to evaluate the efficacy of \\tool{} against\nstate-of-the-art baselines, assess the completeness of official SDK\ndocumentation, and analyze the evolution of permission-required APIs across\ndifferent SDK releases. Our experimental results reveal that \\tool{} identifies\n2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and\n10 respectively, substantially outprforming existing baselines.", "AI": {"tldr": "This paper introduces a novel approach using large language models (LLMs) to systematically discover API-permission mappings in Android, addressing limitations of existing methods and improving documentation completeness.", "motivation": "Android's official API documentation suffers from imprecision and incompleteness regarding permissions, causing developers to spend significant effort and potentially leading to security violations and app failures. Existing static/dynamic analysis methods have poor adaptability to updates, limited code coverage, and miss complex mappings.", "method": "The approach utilizes large language models (LLMs) with a dual-role prompting strategy and API-driven code generation to systematically examine API-permission mappings. A corresponding tool called \\tool{} was developed.", "result": "The tool identified 2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and 10 respectively, substantially outperforming existing baselines.", "conclusion": "The LLM-based approach successfully addresses the limitations of traditional methods for discovering API-permission mappings in Android, providing more comprehensive and adaptable results across different SDK versions."}}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment.", "AI": {"tldr": "GA4GC is a framework that optimizes coding agents by balancing runtime efficiency (greener agent) and code performance (greener code) through Pareto-optimal hyperparameters and prompt templates.", "motivation": "LLM-powered coding agents face sustainability and scalability issues in industrial use, with high token consumption (over 100k tokens per run) and environmental costs that may outweigh optimization benefits.", "method": "Systematic optimization of coding agent runtime and code performance trade-offs by discovering Pareto-optimal agent hyperparameters and prompt templates.", "result": "Evaluation on SWE-Perf benchmark shows up to 135x hypervolume improvement, reducing agent runtime by 37.7% while improving correctness. Temperature identified as the most critical hyperparameter.", "conclusion": "Provides actionable strategies to balance agent sustainability with code optimization effectiveness in industrial deployment, establishing a framework for greener coding agents."}}
{"id": "2510.04143", "categories": ["cs.SE", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.04143", "abs": "https://arxiv.org/abs/2510.04143", "authors": ["Konstantinos Kitsios", "Francesco Sovrano", "Earl T. Barr", "Alberto Bacchelli"], "title": "Detecting Semantic Clones of Unseen Functionality", "comment": "13 pages, 3 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Semantic code clone detection is the task of detecting whether two snippets\nof code implement the same functionality (e.g., Sort Array). Recently, many\nneural models achieved near-perfect performance on this task. These models seek\nto make inferences based on their training data. Consequently, they better\ndetect clones similar to those they have seen during training and may struggle\nto detect those they have not. Developers seeking clones are, of course,\ninterested in both types of clones. We confirm this claim through a literature\nreview, identifying three practical clone detection tasks in which the model's\ngoal is to detect clones of a functionality even if it was trained on clones of\ndifferent functionalities. In light of this finding, we re-evaluate six\nstate-of-the-art models, including both task-specific models and generative\nLLMs, on the task of detecting clones of unseen functionality. Our experiments\nreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs\nperform on par with task-specific models without explicit training for clone\ndetection, but generalize better to unseen functionalities, where F1 drops up\nto 5% (average 3%) instead. We propose and evaluate the use of contrastive\nlearning to improve the performance of existing models on clones of unseen\nfunctionality. We draw inspiration from the computer vision and natural\nlanguage processing fields where contrastive learning excels at measuring\nsimilarity between two objects, even if they come from classes unseen during\ntraining. We replace the final classifier of the task-specific models with a\ncontrastive classifier, while for the generative LLMs we propose contrastive\nin-context learning, guiding the LLMs to focus on the differences between\nclones and non-clones. The F1 on clones of unseen functionality is improved by\nup to 26% (average 9%) for task-specific models and up to 5% (average 3%) for\nLLMs.", "AI": {"tldr": "Semantic code clone detection models struggle with unseen functionalities. While current models achieve near-perfect performance on seen clones, they drop up to 48% F1 on unseen functionalities. LLMs generalize better with only 5% F1 drop. Contrastive learning improves performance by up to 26% for task-specific models and 5% for LLMs.", "motivation": "Current neural models for semantic code clone detection perform well on clones similar to their training data but struggle with unseen functionalities, which developers also need to detect. The paper identifies three practical scenarios where models must detect clones of functionalities not seen during training.", "method": "Re-evaluated six state-of-the-art models on detecting clones of unseen functionality. Proposed contrastive learning approaches: replacing final classifier with contrastive classifier for task-specific models, and contrastive in-context learning for LLMs to focus on differences between clones and non-clones.", "result": "Task-specific models showed F1 drop of up to 48% (average 31%) on unseen functionalities. LLMs performed similarly to task-specific models but generalized better with only 5% F1 drop (average 3%). Contrastive learning improved F1 by up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for LLMs.", "conclusion": "Current clone detection models have poor generalization to unseen functionalities. LLMs show better generalization without explicit training. Contrastive learning effectively improves performance on unseen functionalities for both task-specific models and LLMs, addressing the generalization gap in semantic code clone detection."}}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages.", "AI": {"tldr": "This paper introduces a unified deep learning model for syntax highlighting that supports multiple programming languages, reducing deployment complexity and improving generalization through novel normalization techniques and few-shot learning.", "motivation": "Current syntax highlighting models are language-specific, requiring multiple models for multi-language environments, which increases system complexity and operational costs. They also depend on large datasets from slow brute-force generators.", "method": "Developed a unified model capable of highlighting up to six programming languages using a novel normalization technique for better generalization. Employed few-shot learning to reduce dependency on large datasets from brute-force generators.", "result": "The unified model reduces deployment complexity by a factor of six and improves performance on unseen languages. Few-shot learning experiments show that small oracle samples can replace large datasets.", "conclusion": "The proposed innovations enable efficient, scalable, and cost-effective syntax highlighting across diverse programming languages, addressing key challenges in multi-language development environments."}}
{"id": "2510.04274", "categories": ["cs.SE", "D.2; I.2; J.6; K.3; K.7"], "pdf": "https://arxiv.org/pdf/2510.04274", "abs": "https://arxiv.org/abs/2510.04274", "authors": ["Damjan Fujs", "Damjan Vavpoti\u010d", "Toma\u017e Hovelja", "Marko Po\u017eenel"], "title": "Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience", "comment": "5 pages, 1 figure, 2 tables, presented at IARIA CYBER 2025", "summary": "This study investigates how access to Large Language Models (LLMs) and\nvarying levels of professional software development experience affect the\nprioritization of cybersecurity requirements for web applications. Twenty-three\npostgraduate students participated in a research study to prioritize security\nrequirements (SRs) using the MoSCoW method and subsequently rated their\nproposed solutions against multiple evaluation criteria. We divided\nparticipants into two groups (one with and the other without access to LLM\nsupport during the task). Results showed no significant differences related to\nLLM use, suggesting that access to LLMs did not noticeably influence how\nparticipants evaluated cybersecurity solutions. However, statistically\nsignificant differences emerged between experience groups for certain criteria,\nsuch as estimated cost to develop a feature, perceived impact on user\nexperience, and risk assessment related to non-implementation of the proposed\nfeature. Participants with more professional experience tended to provide\nhigher ratings for user experience impact and lower risk estimates.", "AI": {"tldr": "Study examines how LLM access and software development experience affect cybersecurity requirement prioritization. No significant LLM impact found, but experience differences emerged in cost estimation, user experience impact, and risk assessment.", "motivation": "To understand how access to Large Language Models and varying levels of professional software development experience influence the prioritization of cybersecurity requirements for web applications.", "method": "23 postgraduate students participated in prioritizing security requirements using MoSCoW method. Divided into two groups (with/without LLM support). Rated solutions against multiple evaluation criteria.", "result": "No significant differences related to LLM use. Statistically significant differences between experience groups for cost estimation, user experience impact, and risk assessment. More experienced participants gave higher UX impact ratings and lower risk estimates.", "conclusion": "LLM access doesn't significantly influence cybersecurity requirement evaluation, but professional experience affects how developers assess cost, user experience impact, and risk factors."}}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.", "AI": {"tldr": "A competition organized by JetBrains and Mistral AI at ASE 2025 focused on optimizing context collection for code completion in Python and Kotlin, using real-world datasets and evaluating submissions with neural models and chrF metric.", "motivation": "The need for systematic evaluation of AI methods in software engineering, particularly their ability to leverage information from entire projects in large code bases.", "method": "Organized a competition where participants developed efficient context collection mechanisms for fill-in-the-middle code completions using a dataset of real-world Python and Kotlin code from permissively licensed open-source projects.", "result": "19 teams submitted Python solutions and 8 teams submitted Kotlin solutions in the public phase; 6 teams competed in the private phase with 5 submitting papers.", "conclusion": "The competition successfully addressed the challenge of optimizing context collection for code completion across multiple programming languages using state-of-the-art neural models."}}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "AI": {"tldr": "MacroBench is a code-first benchmark that evaluates LLMs' ability to synthesize reusable browser automation programs from natural language goals using Python with Selenium across 681 tasks on seven self-hosted sites.", "motivation": "To assess whether LLMs can effectively create reusable browser automation programs from natural language instructions by reading HTML/DOM and generating Python code with Selenium.", "method": "Uses seven self-hosted sites (Airbnb-like, TikTok-like, Reddit-like, Instagram-like, Facebook-like, Discord-like, Threads-like) with 681 tasks, validated through static checks, sandboxed execution, DOM assertions, database snapshots, and safety testing.", "result": "GPT-4o-Mini achieved 96.8% success, GPT-4.1 95.3%, Gemini-2.5-Pro 89.0%, DeepSeek-V3.1 83.4%. Models handle simple tasks at 91.7% but fail completely on complex workflows (0.0%). None meet production-quality coding standards despite functional completion.", "conclusion": "LLMs show stratified performance in web automation synthesis, with high success on simple tasks but complete failure on complex workflows, and none achieve production-quality coding practices. The benchmark enables reproducible assessment of macro synthesis for web automation."}}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko M\u00e4kitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development.", "AI": {"tldr": "AI can enhance Requirement Engineering by automating tasks and improving collaboration, but raises ethical concerns and requires trustworthy solutions.", "motivation": "To address persistent challenges in Requirement Engineering like ambiguity and evolving requirements, and explore AI's potential to streamline the process while managing new concerns.", "method": "Explores how AI can enhance traditional RE practices through automation of labor-intensive tasks, requirement prioritization support, and facilitating stakeholder-AI collaboration.", "result": "Identifies opportunities for AI in RE but also highlights challenges including ethical issues, biases, and lack of transparency that need to be addressed.", "conclusion": "Calls for ethical AI practices, enhanced academia-industry collaboration, and development of trustworthy, practical AI solutions adaptable to fast-paced software development."}}
{"id": "2510.04437", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04437", "abs": "https://arxiv.org/abs/2510.04437", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "comment": null, "summary": "Against the backdrop of deepening digital and intelligent transformation in\nhuman resource management, traditional recruitment models struggle to fully\nmeet enterprises' growing demand for precise talent acquisition due to limited\nefficiency, high costs, and information asymmetry. As a vital tool for\noptimizing recruitment processes, reducing labor and time costs, and enhancing\ncore competitiveness, intelligent recruitment management systems become an\nindispensable component of modern organizational talent strategies.Compared\nwith the labor intensive tasks of resume screening, candidate position\nmatching, and interview coordination in traditional manual recruitment,\nintelligent recruitment systems significantly enhance the efficiency and\naccuracy of the hiring process through automation and data driven approaches.\nThese systems enable rapid parsing of massive resume volumes, intelligent\nmatching of candidates to positions, and automated scheduling of interview\nprocesses.", "AI": {"tldr": "Intelligent recruitment systems use automation and data-driven approaches to overcome limitations of traditional manual recruitment, improving efficiency and accuracy in talent acquisition.", "motivation": "Traditional recruitment models face challenges with limited efficiency, high costs, and information asymmetry, failing to meet enterprises' growing demand for precise talent acquisition in the digital transformation era.", "method": "Intelligent recruitment systems employ automation and data-driven approaches for rapid resume parsing, intelligent candidate-position matching, and automated interview scheduling.", "result": "These systems significantly enhance the efficiency and accuracy of the hiring process compared to labor-intensive traditional manual recruitment methods.", "conclusion": "Intelligent recruitment management systems are indispensable tools for optimizing recruitment processes, reducing costs, and enhancing core competitiveness in modern organizational talent strategies."}}
{"id": "2510.04468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04468", "abs": "https://arxiv.org/abs/2510.04468", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improving IR-based Bug Localization with Semantics-Driven Query Reduction", "comment": "56 pages, 16 figures, 11 tables", "summary": "Despite decades of research, software bug localization remains challenging\ndue to heterogeneous content and inherent ambiguities in bug reports. Existing\nmethods such as Information Retrieval (IR)-based approaches often attempt to\nmatch source documents to bug reports, overlooking the context and semantics of\nthe source code. On the other hand, Large Language Models (LLM) (e.g.,\nTransformer models) show promising results in understanding both texts and\ncode. However, they have not been yet adapted well to localize software bugs\nagainst bug reports. They could be also data or resource-intensive. To bridge\nthis gap, we propose, IQLoc, a novel bug localization approach that capitalizes\non the strengths of both IR and LLM-based approaches. In particular, we\nleverage the program semantics understanding of transformer-based models to\nreason about the suspiciousness of code and reformulate queries during bug\nlocalization using Information Retrieval. To evaluate IQLoc, we refine the\nBench4BL benchmark dataset and extend it by incorporating ~30% more recent bug\nreports, resulting in a benchmark containing ~7.5K bug reports. We evaluated\nIQLoc using three performance metrics and compare it against four baseline\ntechniques. Experimental results demonstrate its superiority, achieving up to\n58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in\nHIT@K for the test bug reports with random and time-wise splits, respectively.\nMoreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,\n72.73% for those that include code elements, and 65.38% for those containing\nonly descriptions in natural language. By integrating program semantic\nunderstanding into Information Retrieval, IQLoc mitigates several longstanding\nchallenges of traditional IR-based approaches in bug localization.", "AI": {"tldr": "IQLoc is a novel bug localization approach that combines IR and LLM methods to improve software bug localization by leveraging program semantics understanding and query reformulation.", "motivation": "Existing bug localization methods have limitations - IR-based approaches overlook code context and semantics, while LLMs are not well-adapted for bug localization and can be resource-intensive.", "method": "IQLoc integrates transformer-based models' program semantics understanding with Information Retrieval, using LLMs to reason about code suspiciousness and reformulate queries during bug localization.", "result": "IQLoc achieves superior performance with up to 58.52-60.59% MAP, 61.49-64.58% MRR, and 69.88-100.90% HIT@K. It particularly improves MAP by 91.67% for bug reports with stack traces, 72.73% for those with code elements, and 65.38% for natural language descriptions.", "conclusion": "By integrating program semantic understanding into Information Retrieval, IQLoc effectively mitigates longstanding challenges of traditional IR-based approaches in bug localization."}}
{"id": "2510.04469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04469", "abs": "https://arxiv.org/abs/2510.04469", "authors": ["Wenqi Yan", "Toby Murray", "Benjamin Rubinstein", "Van-Thuan Pham"], "title": "DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing", "comment": null, "summary": "We present DynamiQ, a full-fledged and optimized successor to AFLTeam that\nsupports dynamic and adaptive parallel fuzzing. Unlike most existing approaches\nthat treat individual seeds as tasks, DynamiQ leverages structural information\nfrom the program's call graph to define tasks and continuously refines task\nallocation using runtime feedback. This design significantly reduces redundant\nexploration and enhances fuzzing efficiency at scale. Built on top of the\nstate-of-the-art LibAFL framework, DynamiQ incorporates several practical\noptimizations in both task allocation and task-aware fuzzing. Evaluated on 12\nreal-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ\noutperforms state-of-the-art parallel fuzzers in both code coverage and\nvulnerability discovery, uncovering 9 previously unknown bugs in widely used\nand extensively fuzzed open-source software.", "AI": {"tldr": "DynamiQ is an optimized parallel fuzzing system that uses program call graph structure for task allocation and runtime feedback to reduce redundancy and improve efficiency, outperforming state-of-the-art fuzzers in coverage and bug discovery.", "motivation": "To address the limitations of existing parallel fuzzing approaches that treat individual seeds as tasks, leading to redundant exploration and reduced efficiency at scale.", "method": "Leverages structural information from program call graphs to define tasks, continuously refines task allocation using runtime feedback, and incorporates practical optimizations in both task allocation and task-aware fuzzing built on the LibAFL framework.", "result": "Outperforms state-of-the-art parallel fuzzers in code coverage and vulnerability discovery, uncovering 9 previously unknown bugs in widely used open-source software during 25,000 CPU hours of evaluation on 12 real-world targets.", "conclusion": "DynamiQ demonstrates that structural task definition and adaptive allocation significantly enhance parallel fuzzing efficiency and effectiveness compared to traditional seed-based approaches."}}
{"id": "2510.04495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04495", "abs": "https://arxiv.org/abs/2510.04495", "authors": ["Napasorn Tevarut", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem", "comment": "Accepted in PROFES 2025", "summary": "Trivial packages, small modules with low functionality, are common in the npm\necosystem and can pose security risks despite their simplicity. This paper\nrefines existing definitions and introduce data-only packages that contain no\nexecutable logic. A rule-based static analysis method is developed to detect\ntrivial and data-only packages and evaluate their prevalence and associated\nrisks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are\ntrivial, with vulnerability levels comparable to non-trivial ones, and\ndata-only packages, though rare, also contain risks. The proposed detection\ntool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale\nanalysis to reduce security exposure. This findings suggest that trivial and\ndata-only packages warrant greater attention in dependency management to reduce\npotential technical debt and security exposure.", "AI": {"tldr": "This paper refines definitions of trivial and data-only packages in the npm ecosystem, develops a static analysis method to detect them, and finds that 17.92% of packages are trivial with comparable vulnerability levels to non-trivial packages.", "motivation": "Trivial packages (small modules with low functionality) are common in npm and pose security risks despite their simplicity, requiring better detection and understanding of their prevalence and risks.", "method": "Developed a rule-based static analysis method to detect trivial and data-only packages, and evaluated their prevalence and associated risks in the 2025 npm ecosystem.", "result": "17.92% of packages are trivial with vulnerability levels comparable to non-trivial ones; data-only packages are rare but also contain risks; detection tool achieves 94% accuracy (macro-F1 0.87).", "conclusion": "Trivial and data-only packages warrant greater attention in dependency management to reduce potential technical debt and security exposure."}}
{"id": "2510.04519", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04519", "abs": "https://arxiv.org/abs/2510.04519", "authors": ["Heiko Koziolek", "Thilo Braun", "Virendra Ashiwal", "Sofia Linsbauer", "Marthe Ahlgreen Hansen", "Karoline Grotterud"], "title": "Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation", "comment": "12 pages, 9 figures", "summary": "Distributed control systems (DCS) manage the automation for many industrial\nproduction processes (e.g., power plants, chemical refineries, steel mills).\nProgramming the software for such systems remains a largely manual and tedious\nprocess, incurring costs of millions of dollars for extensive facilities. Large\nlanguage models (LLMs) have been found helpful in generating DCS control logic,\nresulting in commercial copilot tools. Today, these tools are focused on\ntextual notations, they provide limited automation, and have not been tested on\nlarge datasets with realistic test cases. We introduce Spec2Control, a highly\nautomated LLM workflow to generate graphical control logic directly from\nnatural language user requirements. Experiments using an open dataset with 10\ncontrol narratives and 65 complex test cases demonstrate that Spec2Control can\nsuccessfully identify control strategies, can generate 98.6% of correct control\nstrategy connections autonomously, and can save between 94-96% of human labor.\nSpec2Control is being integrated into commercial ABB engineering tools, but is\nalso available as an open-source variant for independent validation.", "AI": {"tldr": "Spec2Control is an automated LLM workflow that generates graphical control logic directly from natural language requirements for distributed control systems, achieving 98.6% accuracy in control strategy connections and saving 94-96% of human labor.", "motivation": "Programming distributed control systems (DCS) for industrial processes is manual, tedious, and costly (millions of dollars). Existing LLM-based tools are limited to textual notations, provide limited automation, and lack testing on large datasets with realistic cases.", "method": "Spec2Control uses a highly automated LLM workflow to generate graphical control logic directly from natural language user requirements, tested on an open dataset with 10 control narratives and 65 complex test cases.", "result": "Spec2Control successfully identifies control strategies, generates 98.6% of correct control strategy connections autonomously, and saves between 94-96% of human labor.", "conclusion": "Spec2Control demonstrates high automation and accuracy in generating DCS control logic from natural language, with commercial integration into ABB engineering tools and an open-source variant available for validation."}}
{"id": "2510.04603", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04603", "abs": "https://arxiv.org/abs/2510.04603", "authors": ["Johan Lin\u00e5ker", "Sachiko Muto"], "title": "Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes", "comment": "In submission", "summary": "Context: Open Source Software (OSS) is a vital public good, included across\nmost of modern software stacks, significantly impacting GDP and national tech\ngrowth, while supporting interoperability, sovereignty, and transparency.\nHowever, systematic measurement of governmental OSS adoption remain limited.\n  Research Aim: This study contributes to digital government maturity indexes\nby analyzing policies and support actions leveraging OSS for software reuse and\ncollaborative development across 16 digitally mature countries, and proposing\npotential indicators for said indexes. It examines OSS policy formation, stated\ngoals, key actors, and support mechanisms.\n  Methodology: A qualitative approach is used combining desk research of policy\ndocuments with semi-structured interviews of government representatives,\nproducing detailed country reports. These are cross-analyzed, focusing on OSS\npolicy promotion, rationale, and implementation support.\n  Results: Policies facilitating OSS reuse are widespread, targeting both\ninbound acquisition and outbound sharing, and are predominantly governed by\ncentral public sector organizations. Policy goals include interoperability,\ndigital sovereignty, transparency, and cost efficiency, with security framed\nboth as a risk and strength. Implementation is supported by diverse Open Source\nProgram Offices (OSPOs) at multiple government levels, which foster capacity\nbuilding, resource pooling, and sustainable project governance. Indicators are\nsynthesized and proposed across 14 areas covering policy incentives and design,\nand implementation and support.\n  Conclusions: OSS is a strategic enabler for public sector digital\ntransformation. Clear policy frameworks, coupled with institutional support\nsuch as OSPOs, are essential. International digital maturity frameworks should\nexpand OSS indicators to better guide and assess government adoption and\nimpact.", "AI": {"tldr": "This study analyzes OSS adoption in 16 digitally mature countries, examining policies, goals, actors, and support mechanisms to propose indicators for digital government maturity indexes.", "motivation": "Systematic measurement of governmental OSS adoption remains limited despite OSS being a vital public good that impacts GDP, national tech growth, interoperability, sovereignty, and transparency.", "method": "Qualitative approach combining desk research of policy documents with semi-structured interviews of government representatives, producing detailed country reports that are cross-analyzed.", "result": "Policies facilitating OSS reuse are widespread, governed by central public sector organizations, with goals including interoperability, digital sovereignty, transparency, and cost efficiency. Implementation is supported by diverse Open Source Program Offices (OSPOs) at multiple government levels.", "conclusion": "OSS is a strategic enabler for public sector digital transformation. Clear policy frameworks and institutional support like OSPOs are essential, and international digital maturity frameworks should expand OSS indicators to better guide government adoption."}}
{"id": "2510.04605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04605", "abs": "https://arxiv.org/abs/2510.04605", "authors": ["Jingyao Zhang", "Tianlin Li", "Xiaoyu Zhang", "Qiang Hu", "Bin Shi"], "title": "Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) are widely used in software\nengineering (SE) but face limitations in processing code structure information\nand suffer from high inference latency. Diffusion LLMs (DLLMs) offer a\npromising alternative with global bidirectional encoding and decoupled\ngeneration steps. This work presents the first comprehensive evaluation of\nDLLMs across the software development lifecycle, including code generation,\ndefect detection, and program repair. On a large-scale benchmark of 52,937\ntasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy\nimprovement achieving a 113% gain on cross-file repair, while maintaining\nsuperior efficiency and reduced latency. Our results establish DLLMs as a\nsuperior paradigm for SE tasks.", "AI": {"tldr": "Diffusion LLMs outperform autoregressive LLMs in software engineering tasks with 30% average accuracy improvement and 113% gain on cross-file repair, while maintaining better efficiency.", "motivation": "Autoregressive LLMs have limitations in processing code structure and suffer from high inference latency, while Diffusion LLMs offer global bidirectional encoding and decoupled generation steps.", "method": "First comprehensive evaluation of Diffusion LLMs across software development lifecycle including code generation, defect detection, and program repair on a large-scale benchmark of 52,937 tasks.", "result": "7B-parameter DLLMs outperform AR-LLMs with 30% average accuracy improvement and achieve 113% gain on cross-file repair while maintaining superior efficiency and reduced latency.", "conclusion": "Diffusion LLMs establish as a superior paradigm for software engineering tasks compared to autoregressive LLMs."}}
{"id": "2510.04611", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.04611", "abs": "https://arxiv.org/abs/2510.04611", "authors": ["Pawel Weichbroth", "Maciej Lotysz", "Michal Wrobel"], "title": "A survey on the impact of emotions on the productivity among software developers", "comment": "29 pages, 5 tables, 96 references", "summary": "The time pressure associated with software development, among other factors,\noften leads to a diminished emotional state among developers. However, whether\nemotions affect perceived productivity remains an open question. This study\naims to determine the strength and direction of the relationship between\nemotional state and perceived productivity among software developers. We\nemployed a two-stage approach. First, a survey was conducted with a pool of\nnine experts to validate the measurement model. Second, a survey was\nadministered to a pool of 88 software developers to empirically test the\nformulated hypothesis by using Partial Least Squares, as the data analysis\nmethod. The results of the path analysis clearly confirm the formulated\nhypothesis, showing that the emotional state of a software developer has a\nstrong positive, and significant impact (beta = 0.893, p < 0.001) on perceived\nproductivity among software developers. The findings highlight the importance\nof managing and improving developers emotional well-being to enhance\nproductivity in software development environments. Additionally, interventions\naimed at reducing burnout, stress, and other negative factors could have a\nconsiderable impact on their performance outcomes.", "AI": {"tldr": "Emotional state of software developers has strong positive impact on perceived productivity (beta=0.893, p<0.001), highlighting importance of emotional well-being for productivity.", "motivation": "Time pressure in software development often leads to diminished emotional state, but whether emotions affect perceived productivity was unknown.", "method": "Two-stage survey approach: expert validation with 9 experts, then empirical testing with 88 developers using Partial Least Squares analysis.", "result": "Path analysis confirmed hypothesis - emotional state has strong positive significant impact on perceived productivity (beta=0.893, p<0.001).", "conclusion": "Managing developers' emotional well-being is crucial for productivity; interventions to reduce burnout and stress can significantly impact performance."}}
{"id": "2510.04689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04689", "abs": "https://arxiv.org/abs/2510.04689", "authors": ["Chengwei Liu", "Wenbo Guo", "Yuxin Zhang", "Limin Wang", "Sen Chen", "Lei Bu", "Yang Liu"], "title": "Evolaris: A Roadmap to Self-Evolving Software Intelligence Management", "comment": null, "summary": "In recent years, the landscape of software threats has become significantly\nmore dynamic and distributed. Security vulnerabilities are no longer discovered\nand shared only through formal channels such as public vulnerability databases\nor vendor advisories. Increasingly, criti- cal threat information emerges\ninformally through blogs, social media, developer forums, open source\nrepositories, and even underground com- munities. To this end, capturing such\nintelligence in a timely manner is essential for maintaining situational\nawareness and enabling prompt security responses. However, this remains a\ncomplex challenge due to the fragmented nature of data sources and the\ntechnical difficulty of collecting, parsing, mapping, and validating\ninformation at scale. To ad- dress this, we propose Evolaris, a self-evolving\nsoftware intelligence sys- tem built on a multi-agent framework. Evolaris is\ndesigned to support a full-stack workflow, where agents operate independently\nbut coordinate through shared context to perform tasks such as information\ndiscovery, reasoning, gap completion, validation, and risk detection. This\narchi- tecture enables the platform to learn from new inputs, refine its\ninternal knowledge, and adapt to emerging threat patterns over time, which\ncould continuously improve the precision, timeliness, and scalability of\nsoftware threat analysis, and offers a sustainable foundation for proactive\nsecu- rity decision-making and strengthens the broader ecosystem of security\nthreat understanding.", "AI": {"tldr": "Evolaris is a self-evolving software intelligence system that uses a multi-agent framework to automatically collect, analyze, and validate threat information from diverse informal sources like blogs, social media, and forums to improve security threat analysis.", "motivation": "Traditional security vulnerability discovery through formal channels is insufficient as critical threat information increasingly emerges from informal sources like blogs, social media, and developer forums, creating challenges for timely threat intelligence collection and validation.", "method": "A multi-agent framework where independent agents coordinate through shared context to perform information discovery, reasoning, gap completion, validation, and risk detection, enabling the system to learn from new inputs and adapt to emerging threats.", "result": "The system continuously improves precision, timeliness, and scalability of software threat analysis by learning from new inputs and refining internal knowledge over time.", "conclusion": "Evolaris provides a sustainable foundation for proactive security decision-making and strengthens the broader ecosystem of security threat understanding through its self-evolving, adaptive architecture."}}
{"id": "2510.04711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04711", "abs": "https://arxiv.org/abs/2510.04711", "authors": ["Aoyang Fang", "Songhan Zhang", "Yifan Yang", "Haotong Wu", "Junjielong Xu", "Xuyang Wang", "Rui Wang", "Manyi Wang", "Qisheng Lu", "Pinjia He"], "title": "An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures", "comment": "Our project is available on https://operationspai.github.io/", "summary": "While cloud-native microservice architectures have transformed software\ndevelopment, their complexity makes Root Cause Analysis (RCA) both crucial and\nchallenging. Although many data-driven RCA models have been proposed, we find\nthat existing benchmarks are often oversimplified and fail to capture\nreal-world conditions. Our preliminary study shows that simple rule-based\nmethods can match or even outperform state-of-the-art (SOTA) models on four\nwidely used benchmarks, suggesting performance overestimation due to benchmark\nsimplicity. To address this, we systematically analyze popular RCA benchmarks\nand identify key limitations in fault injection, call graph design, and\ntelemetry patterns. Based on these insights, we develop an automated framework\nto generate more realistic benchmarks, yielding a dataset of 1,430 validated\nfailure cases from 9,152 injections, covering 25 fault types under dynamic\nworkloads with hierarchical ground-truth labels and verified SLI impact.\nRe-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy\n(average 0.21, best 0.37) and significantly longer execution times. Our\nanalysis highlights three common failure patterns: scalability issues,\nobservability blind spots, and modeling bottlenecks.", "AI": {"tldr": "Existing RCA benchmarks are oversimplified, allowing simple rule-based methods to match SOTA models. A new realistic benchmark framework reveals that SOTA models perform poorly (average Top@1 accuracy 0.21) due to scalability, observability, and modeling issues.", "motivation": "Current microservice RCA benchmarks are too simple and don't reflect real-world conditions, leading to overestimated performance of data-driven models.", "method": "Systematic analysis of RCA benchmarks, development of automated framework to generate realistic benchmarks with 1,430 validated failure cases from 9,152 injections covering 25 fault types under dynamic workloads.", "result": "Re-evaluation of 11 SOTA models shows low Top@1 accuracy (average 0.21, best 0.37) and significantly longer execution times. Identified three failure patterns: scalability issues, observability blind spots, and modeling bottlenecks.", "conclusion": "Existing RCA benchmarks overestimate model performance. More realistic benchmarks are needed to properly evaluate RCA methods, as current SOTA models struggle with real-world complexity."}}
{"id": "2510.04760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature.", "AI": {"tldr": "This paper develops a story point based agile effort estimation model using LASSO and Elastic Net regression techniques, with LASSO achieving superior predictive performance.", "motivation": "Software development effort estimation is critical for project success, and researchers continue to study agile effort estimation methods to improve accuracy.", "method": "Used LASSO and Elastic Net regression techniques on 21 software projects from six firms, trained with default parameters and tuned grid search with 5-fold cross-validation.", "result": "LASSO regression achieved excellent performance: PRED(8%) and PRED(25%) of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593, MdMER of 0.063, and MSE of 0.0007.", "conclusion": "LASSO regression outperformed Elastic Net and showed superior predictive capability for agile story point effort estimation."}}
{"id": "2510.04791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04791", "abs": "https://arxiv.org/abs/2510.04791", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Simone Paolo Ponzetto", "Alexander Maedche", "Christian Bartelt"], "title": "GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes", "comment": null, "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.", "AI": {"tldr": "GUISpector is a multi-modal LLM-based framework that automatically verifies natural language requirements in GUI prototypes, provides actionable feedback, and integrates with LLM-driven development workflows.", "motivation": "Existing GUI testing approaches struggle with modern interface complexity and lack actionable feedback for integration with automated development agents, creating a need for better NL requirements verification.", "method": "Uses a multi-modal LLM agent to interpret NL requirements, autonomously plan and execute verification trajectories across GUI applications, and extract detailed NL feedback for developers.", "result": "Evaluated on 150 requirements across diverse GUI applications, effectively detecting requirement satisfaction and violations, demonstrating seamless integration with LLM-driven development workflows.", "conclusion": "GUISpector successfully addresses the gap in GUI requirements verification by providing automated NL requirement validation and actionable feedback for iterative GUI refinement and LLM-based code generation."}}
{"id": "2510.04796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04796", "abs": "https://arxiv.org/abs/2510.04796", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms", "comment": null, "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.", "AI": {"tldr": "RevMine is a tool that uses LLMs to automate code review data collection and analysis, reducing manual scripting efforts for researchers.", "motivation": "Current code review research requires time-consuming manual scripting for data extraction from platforms like GitHub and GitLab, creating barriers for empirical studies.", "method": "RevMine uses large language models to guide users through authentication, endpoint discovery, and natural language-driven data collection, supporting both quantitative and qualitative analysis.", "result": "The tool streamlines the entire code review mining pipeline and reduces the need for manual scripting.", "conclusion": "RevMine democratizes code review mining by lowering barriers to entry, enabling broader empirical software engineering studies."}}
{"id": "2510.04835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04835", "abs": "https://arxiv.org/abs/2510.04835", "authors": ["Wentao Gao", "Renata Borovica-Gajic", "Sang Kil Cha", "Tian Qiu", "Van-Thuan Pham"], "title": "InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface", "comment": null, "summary": "Fuzzing is a highly effective automated testing method for uncovering\nsoftware vulnerabilities. Despite advances in fuzzing techniques, such as\ncoverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus\ncaused by fuzz blockers, limiting their ability to find deeper vulnerabilities.\nHuman expertise can address these challenges, but analyzing fuzzing results to\nguide this support remains labor-intensive. To tackle this, we introduce\nInsightQL, the first human-assisting framework for fuzz blocker analysis.\nPowered by a unified database and an intuitive parameterized query interface,\nInsightQL aids developers in systematically extracting insights and efficiently\nunblocking fuzz blockers. Our experiments on 14 popular real-world libraries\nfrom the FuzzBench benchmark demonstrate the effectiveness of InsightQL,\nleading to the unblocking of many fuzz blockers and considerable improvements\nin code coverage (up to 13.90%).", "AI": {"tldr": "InsightQL is a human-assisting framework for analyzing fuzz blockers in fuzzing, using a unified database and query interface to help developers systematically extract insights and improve code coverage.", "motivation": "Fuzzing often hits coverage plateaus due to fuzz blockers, limiting vulnerability discovery. Human expertise can help but analyzing results is labor-intensive.", "method": "Developed InsightQL framework with unified database and parameterized query interface to assist developers in fuzz blocker analysis.", "result": "Experiments on 14 real-world libraries from FuzzBench showed unblocking of many fuzz blockers and code coverage improvements up to 13.90%.", "conclusion": "InsightQL effectively helps developers overcome fuzz blockers and significantly improves fuzzing coverage."}}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.", "AI": {"tldr": "FreshBrew is a benchmark for evaluating AI agents on Java code migrations, focusing on semantic preservation and avoiding reward hacking using high test coverage projects.", "motivation": "Traditional code migrations rely on rule-based systems and human intervention, but AI-driven frameworks using LLMs offer a promising alternative that needs systematic evaluation.", "method": "Created FreshBrew benchmark with 228 repositories to evaluate state-of-the-art LLMs on project-level Java migrations, comparing against rule-based tools with focus on test coverage for reliable assessment.", "result": "Gemini 2.5 Flash successfully migrated 52.3% of projects to JDK 17, revealing critical strengths and limitations of current agentic approaches in real-world Java modernization tasks.", "conclusion": "FreshBrew provides foundation for evaluating trustworthy code-migration systems and will facilitate rigorous, reproducible evaluation to catalyze progress in AI-driven codebase modernization."}}
{"id": "2510.04905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04905", "abs": "https://arxiv.org/abs/2510.04905", "authors": ["Yicheng Tao", "Yao Qin", "Yepang Liu"], "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches", "comment": null, "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.", "AI": {"tldr": "A comprehensive survey of Retrieval-Augmented Code Generation (RACG) focusing on repository-level approaches, categorizing existing work across multiple dimensions and analyzing current limitations and future directions.", "motivation": "Real-world software development requires reasoning across entire repositories, but existing function-level and file-level code generation approaches are insufficient for capturing long-range dependencies and ensuring global semantic consistency in repository-level code generation (RLCG).", "method": "The paper provides a systematic review and categorization of RACG research along dimensions including generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols.", "result": "The survey establishes a unified analytical framework for understanding RACG, summarizes widely used datasets and benchmarks, and identifies current limitations in the field.", "conclusion": "The work aims to inspire continued progress in AI-powered software engineering by providing comprehensive insights into repository-level code generation challenges and opportunities."}}
{"id": "2510.04964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04964", "abs": "https://arxiv.org/abs/2510.04964", "authors": ["Kelechi G. Kalu", "James C. Davis"], "title": "Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain", "comment": "8 Pages, 3 Figures", "summary": "Software signing provides a formal mechanism for provenance by ensuring\nartifact integrity and verifying producer identity. It also imposes tooling and\noperational costs to implement in practice. In an era of centralized registries\nsuch as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask\nwhether hardening registry security controls obviates the need for end-to-end\nartifact signing. In this work, we posit that the core guarantees of signing,\nprovenance, integrity, and accountability are not automatically carried across\ndifferent software distribution boundaries. These boundaries include mirrors,\ncorporate proxies, re-hosting, and air-gapped transfers, where registry\nsecurity controls alone cannot provide sufficient assurance. We synthesize\nhistorical practice and present a trust model for modern distribution modes to\nidentify when signing is necessary to extend trust beyond registry control.\nTreating signing as a baseline layer of defense strengthens software supply\nchain assurance even when registries are secure.", "AI": {"tldr": "Software signing remains necessary despite secure registries because its core guarantees (provenance, integrity, accountability) don't automatically carry across distribution boundaries like mirrors, proxies, and air-gapped transfers.", "motivation": "To determine if hardened registry security controls eliminate the need for end-to-end artifact signing in modern software distribution.", "method": "Synthesize historical practice and present a trust model for modern distribution modes to identify when signing is necessary beyond registry control.", "result": "Found that signing is essential to extend trust beyond registry boundaries across various distribution scenarios including mirrors, corporate proxies, re-hosting, and air-gapped transfers.", "conclusion": "Treating signing as a baseline layer of defense strengthens software supply chain assurance even when registries are secure, as registry controls alone cannot provide sufficient assurance across distribution boundaries."}}
{"id": "2510.04982", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04982", "abs": "https://arxiv.org/abs/2510.04982", "authors": ["Aakash Ahmad", "Muhammad Waseem", "Bakheet Aljedaani", "Mahdi Fahmideh", "Peng Liang", "Feras Awaysheh"], "title": "Quantum Computing as a Service - a Software Engineering Perspective", "comment": "37 pages, 10 images, 5 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS.", "AI": {"tldr": "This paper proposes a process-centric and architecture-driven approach for Quantum Computing as a Service (QCaaS) using a two-phase research method combining systematic mapping study and architecture-based development to identify quantum service development lifecycle phases and integrate them into a reference architecture.", "motivation": "Quantum computing is emerging as disruptive technology, and QCaaS is viewed as a service-oriented solution to offer quantum computing resources as utility computing to organizations without quantum computers. The research aims to provide a software engineering perspective on enabling QCaaS.", "method": "Two-phase research method: (1) systematic mapping study to identify quantum service development lifecycle phases through analysis of 41 peer-reviewed studies, and (2) architecture-based development to integrate these phases into a reference architecture supporting QCaaS.", "result": "Identified a 4-phased development lifecycle with quantum significant requirements (QSRs), various modeling notations, catalogue of patterns, programming languages, and deployment platforms that can be integrated in a layered reference architecture for engineering QCaaS.", "conclusion": "The research successfully developed a reference architecture that supports QCaaS by integrating identified quantum service development lifecycle phases, providing a systematic approach for conception, modeling, assembly, and deployment of quantum services."}}
{"id": "2510.04997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.", "AI": {"tldr": "LLMs can significantly accelerate software fault analysis, reducing processing time from weeks to about 2 hours for 3,829 faults, though full automation remains challenging.", "motivation": "Traditional fault analysis is labor-intensive and time-consuming, creating bottlenecks for large-scale studies in complex software systems and slowing empirical research.", "method": "Decomposed fault study into three phases (research objective, data preparation, fault analysis) and evaluated LLMs on 3,829 software faults from a high-quality empirical study.", "result": "LLMs substantially improved efficiency with average processing time of about two hours compared to weeks of manual effort typically required.", "conclusion": "LLMs show great potential for advancing empirical fault studies, but open challenges remain for achieving fully automated, end-to-end software fault analysis."}}
