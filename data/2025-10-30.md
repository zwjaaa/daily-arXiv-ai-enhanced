<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: RepoAlign-Bench is the first benchmark for repository-level code retrieval under change requests, and ReflectCode is a dual-tower architecture that improves retrieval performance through LLM-guided reflection.


<details>
  <summary>Details</summary>
Motivation: Modern codebases are complex and require retrieval systems that can interpret cross-component change intents, which conventional function-level search cannot handle. Retrieving contextually relevant code for change requests remains underexplored.

Method: ReflectCode uses an adversarial reflection augmented dual-tower architecture with disentangled code_encoder and doc_encoder components. It dynamically integrates syntactic patterns, function dependencies, and semantic expansion intents through LLM-guided reflection.

Result: ReflectCode achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over state-of-the-art baselines.

Conclusion: The approach establishes a new direction for context-aware code retrieval by shifting from function-centric matching to holistic repository-level reasoning.

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [2] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Compiler.next is a search-based compiler that transforms human-written intents into working software by dynamically optimizing cognitive architectures and finding optimal trade-offs between objectives like accuracy, cost, and latency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current AI-assisted software engineering tools, including cognitive overload, inefficient tool integration, and narrow AI copilot capabilities, by enabling seamless evolution of AI-native software systems.

Method: Proposes a search-based compiler that takes human intents and automatically generates software through dynamic optimization of cognitive architectures, including prompts, foundation model configurations, and system parameters.

Result: The paper outlines Compiler.next's architecture and positions it as a cornerstone for democratizing software development by lowering technical barriers for non-experts and enabling scalable, adaptable AI-powered software.

Conclusion: Provides a roadmap for addressing challenges in intent compilation and lays groundwork for fully automated, search-driven software development to foster faster innovation and more efficient AI-driven systems.

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [3] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: Proposes a Domain Specific Language (LSL) for scripting LLM interactions to improve reliability, robustness, and trustworthiness of AI applications by enabling output control, structured interactions, and integration with verification tools.


<details>
  <summary>Details</summary>
Motivation: Current LLMs produce unreliable and hallucinated content, making them unsuitable for automated workflows. Software Engineering tools can provide formal verification but current approaches are fragmented.

Method: Develop an LLM Scripting Language (LSL) as a Domain Specific Language to program LLM interactions, control outputs, enforce structure, and integrate with verification and validation tools.

Result: Vision presented for LSL framework that would make LLM interactions programmable and decoupled from training/implementation, addressing reliability issues.

Conclusion: A specialized scripting language for LLMs is needed to overcome current limitations and enable more reliable, verifiable AI applications through structured programming of interactions.

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [4] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct is a framework that extends AI-assisted verification from single functions to complex data structure modules in Verus, using LLMs with syntax guidance and repair mechanisms to achieve high verification success rates.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing AI-assisted verification tools that only handle single functions, and to overcome LLMs' difficulties with Verus' annotation syntax and verification semantics.

Method: Uses a planner module to generate abstractions, type invariants, specifications, and proof code. Embeds syntax guidance in prompts and includes a repair stage to automatically correct annotation errors.

Result: Successfully verified 128 out of 129 functions (99.2%) across eleven Rust data structure modules, succeeding on ten out of eleven modules.

Conclusion: VeriStruct represents an important advancement toward automatic AI-assisted formal verification by effectively extending verification capabilities to complex data structure modules.

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [5] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: The paper proposes a Human-AI RE Synergy Model (HARE-SM) that integrates AI technologies with human oversight to address challenges in Requirements Engineering while ensuring ethical AI use through transparency and bias mitigation.


<details>
  <summary>Details</summary>
Motivation: Traditional Requirements Engineering is labor-intensive and error-prone, while AI-powered approaches offer transformative solutions but face challenges like algorithmic bias, lack of explainability, and ethical concerns.

Method: The study introduces HARE-SM framework with multi-phase research methodology including preparing RE datasets, fine-tuning AI models, and designing collaborative human-AI workflows.

Result: Preliminary study presents conceptual framework and early-stage prototype implementation, establishing research agenda for applying intelligent data science to RE data in collaborative environments.

Conclusion: The HARE-SM model provides a foundation for integrating AI-driven analysis with human oversight to improve requirements elicitation, analysis, and validation while addressing ethical AI concerns.

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [6] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: BeTaL is a framework that uses LLMs to automatically design dynamic benchmarks by parameterizing benchmark templates and optimizing for target properties like difficulty, achieving 2-4x better alignment with desired difficulty levels compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Current static benchmarks quickly become saturated as LLMs evolve, while dynamic benchmarks are expensive to create and update manually. There's a need for automated, cost-efficient dynamic benchmark design.

Method: BeTaL parameterizes key design choices in base benchmark templates and uses LLMs to reason through the parameter space to achieve target properties like difficulty and realism in a cost-efficient manner.

Result: BeTaL created two new benchmarks and extended τ-bench, achieving average deviations from target difficulty ranging from 5.3% to 13.2% - a 2-4x improvement over baselines.

Conclusion: BeTaL successfully automates dynamic benchmark design, producing benchmarks that closely match desired difficulty levels and addressing the limitations of both static and manually-created dynamic benchmarks.

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [7] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: A framework using code property graphs and graph transformations to improve plagiarism detection in programming education against refactoring-based obfuscation attacks.


<details>
  <summary>Details</summary>
Motivation: Current plagiarism detection systems struggle against sophisticated obfuscation techniques, especially refactoring-based attacks that preserve program behavior while changing structure.

Method: Leverages code property graphs and graph transformations to enhance state-of-the-art detectors, creating an extensible framework to counteract refactoring-based obfuscation.

Result: Comprehensive evaluation on real-world student submissions shows significant improvement in detecting plagiarized code, tested against both algorithmic and AI-based obfuscation attacks.

Conclusion: The proposed framework effectively enhances plagiarism detection capabilities against advanced obfuscation techniques in programming education.

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [8] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: Adapt is a proof refinement framework that uses LLM-guided decision-making to dynamically select refinement strategies for theorem proving, outperforming existing methods by proving 16.63-18.58% more theorems.


<details>
  <summary>Details</summary>
Motivation: Formal verification via theorem proving requires significant manual effort, and while LLMs show potential, they often produce incorrect proofs initially and existing refinement approaches use fixed strategies that can't adapt to specific proof issues.

Method: Adapt leverages an LLM-guided decision-maker to dynamically select suitable refinement strategies based on the proof assistant state and context of incorrect proofs, enabling adaptive proof correction.

Result: Adapt significantly outperforms four existing methods on two benchmarks, proving 16.63% and 18.58% more theorems respectively, and shows strong generalizability across five different LLMs.

Conclusion: The dynamic strategy selection approach in Adapt effectively addresses limitations of fixed refinement strategies in LLM-based theorem proving, demonstrating substantial performance improvements and generalizability across different language models.

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [9] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix is a method that automatically detects and repairs REST API misuses in client programs by identifying non-conforming code, integrating API specifications into prompts, and using LLMs to generate corrected code.


<details>
  <summary>Details</summary>
Motivation: Developers often discover REST API specification violations only during testing, and error messages lack sufficient detail for effective diagnosis, making debugging a trial-and-error process.

Method: dcFix identifies non-conforming code fragments, integrates them with relevant API specifications into prompts, and leverages Large Language Models to produce corrected code.

Result: Evaluation shows dcFix accurately detects misuse and outperforms baseline approaches where LLM prompts omit indications of code fragments non-conforming to REST API specifications.

Conclusion: dcFix provides an effective automated solution for detecting and repairing REST API misuses in client programs, improving over manual debugging approaches.

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [10] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: KUMIC is a framework for multi-intent code comment generation that uses Chain-of-Thought and retrieval mechanisms to help LLMs generate intent-specific comments by establishing proper relationships between code, intents, and comments.


<details>
  <summary>Details</summary>
Motivation: Generic code summaries are insufficient for diverse practitioner needs - developers want implementation insights while users need usage instructions, highlighting the need for multi-intent comment generation. Current LLM approaches struggle with constructing correct relationships among intents, code, and comments with limited examples.

Method: KUMIC uses in-context learning with Chain-of-Thought (CoT) to optimize knowledge utilization. It retrieves similar demonstration examples with high code-comment consistency, then uses CoT to guide LLMs to focus on statements that help derive intent-specific comments, constructing a mapping knowledge chain from code to intent-specific statements to comments.

Result: KUMIC outperforms state-of-the-art baselines by significant margins: 14.49% in BLEU, 22.41% in METEOR, 20.72% in ROUGE-L, and 12.94% in SBERT scores.

Conclusion: KUMIC effectively addresses the multi-intent comment generation challenge by leveraging CoT and retrieval mechanisms to help LLMs generate high-quality intent-specific comments through proper reasoning steps and knowledge mapping.

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [11] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: TECS/Rust-OE is a memory-safe component-based development framework that uses call flows to optimize performance while maintaining reusability, addressing performance issues in the original TECS/Rust framework.


<details>
  <summary>Details</summary>
Motivation: Embedded systems are becoming larger and more complex, requiring reliable programming languages. The existing TECS/Rust framework suffered from performance degradation due to excessive exclusive controls for thread safety.

Method: Proposes TECS/Rust-OE framework that uses call flows and leverages real-time OS exclusive control mechanisms. Rust code is automatically generated from component descriptions to optimize performance without compromising reusability.

Result: Evaluations show reduced overhead from optimized exclusion control and high reusability of the generated code.

Conclusion: TECS/Rust-OE successfully addresses the performance limitations of TECS/Rust while maintaining memory safety and reusability in component-based embedded system development.

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [12] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: TECS/Rust is a Rust-based framework for embedded systems that replaces C in component-based development to provide memory safety while maintaining performance and CBD flexibility.


<details>
  <summary>Details</summary>
Motivation: As embedded systems grow more complex, component-based development using C faces memory-related vulnerabilities. Rust's compile-time memory safety features offer a solution to these issues.

Method: Proposes TECS/Rust framework that leverages Rust's lifetime and borrowing features, automates Rust code generation for CBD components, and supports integration with real-time operating systems.

Result: Evaluation shows generated code accounts for large percentage of actual code, with minimal execution time difference compared to non-framework code, indicating negligible overhead.

Conclusion: TECS/Rust successfully provides memory safety for embedded CBD while maintaining performance and flexibility, with minimal runtime overhead.

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [13] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: Combining Property-based Testing (PBT) and Example-based Testing (EBT) improves bug detection in LLM-generated code from 68.75% to 81.25%, showing complementary strengths in finding edge cases.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly generate code, ensuring code quality is crucial. Traditional EBT often misses edge cases, prompting investigation of PBT's effectiveness for detecting boundary conditions and extreme scenarios.

Method: Analyzed 16 HumanEval problems where standard solutions failed on extended tests, generating both PBT and EBT test codes using Claude-4-sonnet and comparing their bug detection capabilities.

Result: Individual PBT and EBT methods each achieved 68.75% bug detection rate, but combining both approaches increased detection to 81.25%. PBT excels at finding performance issues and edge cases through input space exploration, while EBT is better at specific boundary conditions.

Conclusion: A hybrid approach combining PBT and EBT can significantly improve the reliability of LLM-generated code, providing valuable guidance for test generation strategies in LLM-based code development.

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [14] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus is an AI-assisted system that uses modular decomposition and recomposition to automate formal verification, improving verification success rates from 68% to 86% compared to baseline approaches.


<details>
  <summary>Details</summary>
Motivation: Formal verification is critical for reliable software but requires specialized expertise, making it expensive. AI systems can recognize patterns but struggle to integrate effectively into verification workflows.

Method: Decomposes complex program logic into smaller verifiable components, then recomposes them. Uses structured decomposition of lemmas and allows natural language guidance when automated tools fail.

Result: Verifies 86% of tasks vs 68% baseline. More significant gains with complex specifications (30% to 69%) and with proof outlines for complex programs (25% to 87%).

Conclusion: Modular restructuring combined with AI guidance substantially improves verification effectiveness, making formal verification more accessible and efficient.

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [15] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: This paper analyzes developer challenges in building AI agents by studying Stack Overflow discussions, identifying 77 technical issues across 7 major areas, and tracking their evolution from 2021-2025.


<details>
  <summary>Details</summary>
Motivation: AI agents are rapidly gaining popularity but developers face persistent and underexplored challenges when building, deploying, and maintaining these systems.

Method: Studied developer discussions on Stack Overflow using tag expansion/filtering, applied LDA-MALLET for topic modeling, and manually validated themes. Analyzed 60M questions/answers from 30M users.

Result: Identified 7 major areas with 77 distinct technical challenges related to runtime integration, dependency management, orchestration complexity, and evaluation reliability. Quantified topic popularity/difficulty and tracked evolution from 2021-2025.

Conclusion: Provides concrete guidance for practitioners, researchers, and educators on agent reliability and developer support based on the identified challenges and their patterns.

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [16] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: Analysis of reproducibility in LLM-centric studies from ICSE 2024 and ASE 2024 conferences, finding only 5 out of 86 studies were suitable for reproduction attempts, with none fully reproducible.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in research raises concerns about reproducibility. Understanding current reproducibility challenges is crucial for improving empirical research quality and reliability.

Method: Analyzed 86 LLM-centric studies from ICSE 2024 and ASE 2024, focusing on 18 that provided research artifacts and used OpenAI models. Attempted to replicate these 18 studies.

Result: Only 5 out of 18 studies were fit for reproduction. None were fully reproducible - 2 appeared partially reproducible, 3 not reproducible at all.

Conclusion: Current LLM research faces significant reproducibility issues, highlighting the need for stricter artifact evaluation and more robust study designs to ensure research value.

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [17] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL is an energy-aware fuzzing framework that incorporates power consumption into fuzzing heuristics to reduce environmental impact while maintaining coverage.


<details>
  <summary>Details</summary>
Motivation: Traditional grey-box fuzzing approaches like AFL++ focus on coverage maximization without considering energy costs, leading to substantial computational resources and carbon footprints in continuous fuzzing campaigns.

Method: GreenAFL introduces two key modifications: energy-aware corpus minimization that considers power consumption when reducing initial corpora, and energy-guided heuristics that direct mutation towards high-coverage, low-energy inputs.

Result: Results from ablation studies show that highest coverage and lowest energy usage are achieved when at least one of the energy-aware modifications is used compared to vanilla AFL++.

Conclusion: GreenAFL successfully reduces the environmental impact of automated testing while maintaining coverage by incorporating energy awareness into fuzzing workflows.

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [18] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: LOCALIZE is a low-code framework for reproducible radio localization experiments that uses configuration-first approach with versioned artifacts and standardized pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the gap in tools that offer low coding effort, reproducibility by default, and built-in extensibility for machine learning in radio-based localization services.

Method: A configuration-first framework where experiments are declared in human-readable config files, with workflow orchestrator running standardized pipelines from data prep to reporting, and all artifacts are versioned.

Result: Reduces authoring effort while maintaining comparable runtime and memory behavior to Jupyter notebooks, with bounded orchestration overhead when scaling data from 1x to 10x.

Conclusion: The framework makes reproducible machine-learning-based localization experimentation practical, accessible, and extensible.

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [19] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench is a new benchmark that provides process-level trajectory assessment for LLM-based agents in software environment configuration, going beyond end-to-end success rates to analyze fine-grained capabilities like error diagnosis and repair.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks only assess end-to-end build/test success, which obscures where and why agents succeed or fail in environment configuration tasks. There's a need for process-level analysis to understand agent capabilities during environment setup.

Method: Automatically construct task instances by injecting realistic README errors and validate in Docker for scalable evaluation. The benchmark assesses planning, perception-driven error diagnosis, feedback-driven repair, and action execution during environment configuration.

Result: Evaluations show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. The benchmark enables capability assessments beyond aggregate success rates.

Conclusion: Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>
