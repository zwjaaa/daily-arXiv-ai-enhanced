<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert G체ltekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: This paper explores using locally hosted LLMs with retrieval-augmented generation to support cybersecurity risk assessment in the forestry domain, addressing data protection concerns while assisting cybersecurity experts.


<details>
  <summary>Details</summary>
Motivation: The need arises from the shortage of cybersecurity experts in software teams, leading to high workloads and requiring software engineers to conduct cybersecurity activities themselves. There's a need for tools to support vulnerability and threat evaluation during risk assessment.

Method: Design science study involving 12 experts through interviews, interactive sessions, and surveys within a large-scale project, using locally hosted LLMs with retrieval-augmented generation.

Result: LLMs can assist cybersecurity experts by generating initial risk assessments, identifying threats, and providing redundancy checks. Experts were willing to use LLMs in specific evaluation and assistance roles despite trust concerns.

Conclusion: LLM-based agents can support risk assessment processes in cyber-physical systems within safety-critical domains, though human oversight remains essential for accuracy and compliance.

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [2] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: A Git-based submission system for higher education improves assignment tracking, collaboration, and efficiency with 85% instructor preference and 84% student preference over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in traditional assignment submission methods in higher education by leveraging distributed version control workflows.

Method: Iterative software development and user-centered design methodologies, integrated within a real-world university environment with usability testing and student feedback.

Result: 85% of instructors found the system easier to use, 84% of students preferred it over traditional methods, 38% reduction in submission/review time, and 48% reduction in storage requirements.

Conclusion: The Git-based system provides practical insights for integrating distributed version control into education, enhancing instructor oversight and student engagement despite initial adoption challenges.

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [3] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo M체ller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: Systematic literature review shows current MDE research insufficiently supports vision-related accessibility, with limited modeling techniques, empirical validation, and developer expertise.


<details>
  <summary>Details</summary>
Motivation: Software applications often pose barriers for users with visual impairments, and MDE offers systematic methods to integrate accessibility concerns while reducing manual effort.

Method: Conducted systematic literature review of 447 papers, with 30 primary studies meeting inclusion criteria, analyzing how MDE addresses accessibility for vision impairments.

Result: About two-thirds reference WCAG guidelines but with project-specific adaptations and limited end-user validation. Studies model UI structures, interaction, user capabilities, but few specify concrete modeling techniques or demonstrate fully functional systems.

Conclusion: Current MDE research insufficiently supports vision-related accessibility. Research agenda proposed to more effectively embed support for vision impairments in MDE processes.

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [4] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: The paper presents a context collection approach for code completion that uses file-level and chunk-based retrieval strategies to improve LLM performance, achieving 6% improvement over file-retrieval and 16% over no-context baseline.


<details>
  <summary>Details</summary>
Motivation: Context is crucial for code completion quality, but LLMs have limited context length and are sensitive to noisy/irrelevant context in large repositories, making effective context collection challenging.

Method: Developed retrieval strategies at file and chunk levels using static analysis, focusing on context size, file ordering, and hybrid approaches for context collection pipelines.

Result: Chunk-based retrieval with static analysis achieved 6% improvement over best file-retrieval strategy and 16% improvement over no-context baseline for Python in the competition's initial phase.

Conclusion: Retrieval granularity, ordering, and hybrid strategies are critical for developing effective context collection pipelines in real-world development scenarios.

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [5] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika M채ntyl채*

Main category: cs.SE

TL;DR: AiSysRev is an LLM-based screening tool that helps automate systematic review paper screening, showing LLMs can significantly reduce screening burden while still requiring human intervention for boundary cases.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews in software engineering are laborious, especially during the screening phase where large numbers of papers must be assessed against inclusion/exclusion criteria. LLMs can help expedite this process.

Method: Developed AiSysRev - a web application in Docker container that accepts CSV files with paper titles/abstracts, allows users to specify criteria, uses multiple LLMs via OpenRouter, and supports zero-shot/few-shot screening with manual review interfaces.

Result: Trial study with 137 papers showed papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. LLMs perform well on easy cases but struggle with boundary cases requiring human intervention.

Conclusion: LLMs cannot fully replace human judgment in systematic reviews but can significantly reduce the burden of assessing large volumes of scientific literature, especially useful for Rapid Reviews.

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [6] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: Study of how 11 companies create LLM chatbot policies for software development


<details>
  <summary>Details</summary>
Motivation: Risks of adopting LLM chatbots in software organizations require clear policies for safe integration

Method: Examined how 11 companies create LLM chatbot policies and factors influencing them

Result: Analysis of policy creation processes and influencing factors across multiple organizations

Conclusion: Findings aim to help managers safely integrate chatbots into development workflows

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [7] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: This study examines validity threats in software mining tool pipelines and evaluates tool agreement on data, outcomes, and conclusions for the same research questions.


<details>
  <summary>Details</summary>
Motivation: To understand limitations and agreement of software mining tools commonly used by researchers and practitioners, as their constraints are often not well understood despite widespread use.

Method: Conducted a lightweight literature review to select three studies from high-ranked venues, then formally replicated them with four independent mining tools to quantitatively and qualitatively compare extracted data, analysis results, and conclusions.

Result: Found that technical details in tool design and implementation accumulate in complex mining pipelines, causing substantial differences in baseline data, derivatives, statistical analysis results, and sometimes conclusions.

Conclusion: Users must carefully choose tools and evaluate limitations to assess validity scope. Reusing tools is recommended. Researchers and tool authors should promote reusability through reproduction packages and comparative studies.

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [8] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: This study analyzes proposal rejection patterns in Go programming language projects, identifying key decline reasons and demonstrating that LLMs can predict proposal outcomes early in the review process.


<details>
  <summary>Details</summary>
Motivation: Proposal processes in OSS projects are resource-intensive and often frustrate contributors when proposals are declined without clear feedback, yet the reasons behind rejections remain poorly understood.

Method: Mixed-method empirical study on 1,091 Go project proposals, including quantitative analysis of outcomes, qualitative coding to create a taxonomy of decline reasons, and evaluation of LLMs for outcome prediction.

Result: Proposals are more often declined than accepted (resolution takes over a month), only 14.7% of declined proposals are resubmitted, nine key decline reasons identified (duplication, limited use cases, violation of principles), and GPT models can predict decline decisions early (F1=0.71 with partial comments).

Conclusion: The findings reveal inefficiencies in proposal processes and highlight opportunities to improve contributor experience and reviewer workload through early triage and structured guidance based on past decline patterns.

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [9] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: CRAI-MCF is a new framework that transforms static AI model documentation into actionable, value-aligned documentation to address challenges in LLM discovery and adoption.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of specialized LLMs creates challenges in model discovery due to inconsistent documentation. Existing frameworks like Model Cards are static, qualitative, and lack quantitative comparison mechanisms, leading to model underutilization.

Method: Grounded in Value Sensitive Design, the framework was developed through empirical analysis of 240 open-source projects, distilling 217 parameters into an eight-module, value-aligned architecture with quantitative sufficiency criteria.

Result: CRAI-MCF enables rigorous cross-model comparison under a unified scheme and balances technical, ethical, and operational dimensions for comprehensive model assessment.

Conclusion: The framework empowers practitioners to efficiently assess, select, and adopt LLMs with greater confidence and operational integrity by providing actionable, human-aligned documentation.

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [10] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Development of AI Bill of Materials (AIBOM) specification as an extension to SPDX standard for AI components, validated through regulatory alignment, industry use cases, interviews, and case studies.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how open standards are created for fast-evolving domains like AI-powered systems, and to provide a standardized way to capture AI components such as datasets and training artifacts.

Method: Action Research approach with global multi-stakeholder collaboration involving 90+ contributors, using structured AR cycles and four validation methods: regulatory alignment, industry use case mapping, practitioner interviews, and industrial case study.

Result: Successfully developed and validated the AIBOM specification that extends SPDX standard for AI components, with demonstrated alignment with major regulations and practical industry applications.

Conclusion: The paper provides both a validated AIBOM specification and valuable insights into the standardization process that can guide future software engineering standardization efforts in fast-evolving domains.

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [11] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Secure-Instruct is a framework that automatically generates vulnerable and secure code examples to instruction-tune LLMs, improving both security and functional correctness of generated code.


<details>
  <summary>Details</summary>
Motivation: Current approaches like SafeCoder suffer from limited and imbalanced datasets, reducing effectiveness and generalizability for secure code generation in LLMs.

Method: Secure-Instruct automatically synthesizes high-quality vulnerable and secure code examples, generates fine-tuning instructions, and instruction-tunes LLMs to align task description with secure code generation abilities.

Result: On CWEBench, Secure-Instruct gives 14.3% average increase in secure ratio over pretrained models and outperforms SafeCoder by 7.6%. On CWEval, it achieves 14% increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained models.

Conclusion: Secure-Instruct significantly improves secure code generation in LLMs while also enhancing functional correctness, demonstrating superior performance over existing approaches.

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>
