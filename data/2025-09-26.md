<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM introduces two fine-tuned LLMs that generate expert OpenACC directives for data-parallel loops, achieving 87% correct directive type and 50% exact pragma generation on test data.


<details>
  <summary>Details</summary>
Motivation: GPU programming complexity remains high despite directive-based standards like OpenACC, requiring significant expertise for effective directive usage.

Method: Fine-tuned large language models on a supervised dataset of 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories.

Result: Fine-tuned models generate valid pragmas with correct directive type for 87% of loops and exact pragmas for 50% of cases, significantly outperforming base LLMs.

Conclusion: ACCeLLiuM lowers GPU programming barriers by automating directive generation and establishes a reproducible benchmark for LLM-powered OpenACC pragma generation.

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [2] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: This paper presents a systematic review of software security visualization techniques, categorizing them into four types (graph-based, notation-based, matrix-based, metaphor-based) and analyzing over 60 recent research papers to identify key issues, advancements, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based and numerical methods for analyzing security concerns are becoming ineffective as software systems grow more complex and threat landscapes evolve, necessitating better visualization techniques to transform complex security data into digestible formats.

Method: Systematic literature review of over 60 recent research papers in software security visualization, creating a comprehensive taxonomy and categorizing techniques into four main types.

Result: Identified two main areas: extensive software development visualization (focusing on software architecture depiction) and operational/cybersecurity visualization. The review highlights key issues, recent advancements, and the need for adaptive visualization techniques.

Conclusion: There is a critical need for innovative visualization techniques that can adapt to evolving security landscapes, with practical implications for enhancing threat detection, improving security response strategies, and guiding future research directions.

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [3] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct enables ReAct agents to efficiently handle large MCP tool sets by addressing tool selection challenges when dealing with hundreds/thousands of tools that exceed LLM memory limits.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches struggle with extensive tool sets where loading all tools simultaneously is computationally infeasible, limiting the scalability of AI agents in diverse environments.

Method: Proposes five distinct architectures that progressively refine tool selection, culminating in a search-and-load mechanism for intelligent tool selection with minimal computational overhead.

Result: Experimental results show up to 50% reduction in tool loading while maintaining task completion accuracy.

Conclusion: The approach advances towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments by solving the tool selection scalability problem.

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [4] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: The paper proposes a knowledge graph-based framework to address discrimination in software systems by formalizing fairness requirements specification and verification, moving beyond traditional focus on algorithmic flaws and biased data.


<details>
  <summary>Details</summary>
Motivation: Discrimination in software systems is often attributed to algorithmic design flaws or biased data, but the paper argues that the root cause is actually the lack of well-specified fairness requirements and verification mechanisms. Experts' implicit knowledge about fairness makes it difficult to specify precise requirements.

Method: The authors propose developing a knowledge graph-based framework for fairness, inspired by successful applications in security engineering. Knowledge graphs can formalize domain knowledge to assist in requirements specification and verification.

Result: The paper presents a research roadmap discussing the challenges and research questions involved in creating this framework, but does not yet present empirical results from implementation.

Conclusion: A knowledge graph-based approach can provide formal mechanisms for specifying and verifying fairness requirements, addressing a critical gap in current fairness engineering practices that focuses too narrowly on algorithms and data.

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [5] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: Online-Optimized RAG addresses embedding misalignment in retrieval-augmented generation systems by continuously adapting retrieval embeddings using live interactions and minimal feedback, improving tool selection accuracy and task success.


<details>
  <summary>Details</summary>
Motivation: Embedding misalignment in RAG systems due to imperfect embedding models or noisy descriptions leads to incorrect retrieval and task failure, which needs to be addressed for robust tool use.

Method: A deployment-time framework that applies lightweight online gradient updates to adapt retrieval embeddings from live interactions using minimal feedback (e.g., task success), requiring no changes to the underlying LLM and supporting various retrieval scenarios.

Result: The method consistently improves tool selection accuracy and end-task success across diverse tool-use and document-retrieval scenarios with negligible per-query latency.

Conclusion: Online-Optimized RAG provides a simple, practical path to robust, self-improving RAG systems that can handle embedding misalignment through continuous online optimization.

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [6] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula is a domain-specific language for legal contracts with enforceable properties. The paper presents a methodology to verify Stipula contracts by translating them into Java code with JML specifications and using KeY for automatic verification.


<details>
  <summary>Details</summary>
Motivation: To ensure the correctness of legal contracts modeled in Stipula, especially those involving asset transfers and obligations, by providing formal verification methods.

Method: Translate Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications, and use the KeY deductive verification tool for automatic verification of partial and total correctness.

Result: The translation and verification process is fully automatic for a large subset of Stipula contracts (those with disjoint cycles), demonstrating that general-purpose deductive verification tools can be effectively used in this context.

Conclusion: The approach successfully verifies Stipula contracts, showing that translation-based verification using tools like KeY is feasible and effective for ensuring contract correctness.

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [7] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI is a tool-based approach for detecting AI-specific code smells in AI-based systems using a declarative DSL and static analysis, achieving high precision and recall on large codebases.


<details>
  <summary>Details</summary>
Motivation: AI-based systems introduce new software issues that existing detection tools miss, particularly AI-specific code smells that can lead to problems like unreproducibility, silent failures, or poor model generalization.

Method: Combines a high-level declarative Domain-Specific Language (DSL) for rule specification with an extensible static analysis tool that interprets and detects these rules for AI-based systems. Specified 22 AI-specific code smells.

Result: Evaluated on 826 AI-based systems (20M lines of code), achieving precision of 88.66% and recall of 88.89%, outperforming existing detection tools. Demonstrated efficiency and extensibility with SUS score of 81.7/100.

Conclusion: SpecDetect4AI effectively supports specification and detection of AI-specific code smells through dedicated rules and can analyze large AI-based systems efficiently.

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [8] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: First large-scale empirical study of LLM-specific technical debt (SATD) in software integrations, revealing prompt design as the primary source of debt across major LLM APIs.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly embedded in software via APIs, but these integrations create their own form of technical debt that needs systematic study and understanding.

Method: Analyzed 93,142 Python files across major LLM APIs to identify SATD origins, prevalence, and mitigation strategies, with focus on OpenAI and LangChain integrations.

Result: 54.49% of SATD instances stem from OpenAI integrations; prompt design is the primary source (6.61% of debt), with instruction-based prompts (38.60%) and few-shot prompts (18.13%) being most vulnerable due to instruction clarity and example quality dependencies.

Conclusion: The study provides the first comprehensive analysis of LLM-specific technical debt, releases a SATD dataset for reproducibility, and offers practical guidance for managing technical debt in LLM-powered systems.

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [9] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: AI-Python chatbot for programming education that combines static code analysis, dynamic execution tracing, and LLMs to help students debug errors and learn programming concepts, achieving 85% error resolution success rate.


<details>
  <summary>Details</summary>
Motivation: Traditional IDEs and static analyzers lack interactive guidance, while AI code assistants like GitHub Copilot focus on code completion rather than learning. This research aims to bridge the gap by creating an educational AI tool that prioritizes learning over mere code generation.

Method: Hybrid architecture using CodeLlama for code embedding, GPT-4 for natural language interactions, and Docker-based sandboxing for secure execution. Combines static code analysis with dynamic execution tracing to provide practical advice.

Result: 85% error resolution success rate (outperforming pylint's 62% and GPT-4's 73%), 59.3% reduction in debugging time, 34% improvement in coding proficiency, with particular gains in recursion and exception handling. Positive qualitative feedback from 120 students.

Conclusion: The chatbot successfully demonstrates how AI can augment programming education by balancing technical innovation with pedagogical empathy, fostering deeper conceptual understanding and prioritizing educational equity over code completion.

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [10] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc is a novel framework that enhances method-level fault localization by integrating LLMs with retrieval-augmented generation (RAG), outperforming state-of-the-art baselines on Defects4J benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based fault localization methods struggle with complex systems due to lack of project-specific knowledge and difficulty navigating large projects.

Method: Three-component framework: LLM Functionality Extraction generates natural language descriptions of failing behavior, Semantic Dense Retrieval embeds descriptions and methods in shared semantic space, and LLM Re-ranking reorders methods based on contextual relevance.

Result: FaR-Loc outperforms SoapFL and AutoFL by 14.6% and 9.1% in Top-1 accuracy, and 19.2% and 22.1% in Top-5 accuracy respectively. Pre-trained code embedding models with structure improve performance by up to 49.0% in Top-1 accuracy.

Conclusion: The framework effectively addresses limitations of existing LLM-based fault localization methods and shows significant improvements without requiring re-training.

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [11] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: A novel programming language topic classification workflow using multi-label SVM with sliding window and voting strategy to localize language concepts in source code, achieving high accuracy on IBM Project CodeNet dataset.


<details>
  <summary>Details</summary>
Motivation: As software systems grow in scale and complexity, understanding programming language topic distribution in source code is crucial for technical decisions, onboarding, tooling, and education.

Method: Combines multi-label Support Vector Machine (SVM) with sliding window and voting strategy for fine-grained localization of core language concepts like operator overloading, virtual functions, inheritance, and templates.

Result: Trained on IBM Project CodeNet dataset, the model achieves average F1 score of 0.90 across topics and 0.75 in code-topic highlight.

Conclusion: Provides empirical insights and a reusable pipeline for researchers and practitioners in code analysis and data-driven software engineering.

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [12] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: This study examines engagement patterns in hybrid meetings among software professionals, finding comparable engagement levels between onsite and remote participants, though remote participants show lower engagement in long meetings.


<details>
  <summary>Details</summary>
Motivation: The shift to hybrid work post-COVID-19 has created new challenges in communication and collaboration, with remote participation potentially leading to isolation and decreased engagement among team members.

Method: Researchers studied professionals from three software companies using multimodal measurements including self-reported questionnaires and physiological biometric devices during hybrid meetings over several weeks.

Result: Regression analyses showed comparable engagement levels between onsite and remote participants, but remote participants had lower engagement in long meetings. Active roles correlated with higher engagement, while larger meetings and afternoon sessions were associated with lower engagement.

Conclusion: The findings provide insights into engagement factors in hybrid meetings and offer improvement recommendations relevant for software teams and other knowledge-intensive organizations facing hybrid collaboration challenges.

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [13] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: Current synthetic data generation for code LLMs faces a verification ceiling where data quality is limited by synthetic verifiers. The paper shows that rigid 100% pass criteria are too restrictive, and that relaxed verification thresholds combined with diverse test suites can improve model performance by 2-4 points in pass@1.


<details>
  <summary>Details</summary>
Motivation: To address the verification bottleneck in synthetic data generation for code LLMs, where the capabilities of synthetic verifiers fundamentally constrain training data quality and diversity.

Method: Systematic study of verification design strategies: (i) analyzing test complexity vs quantity, (ii) exploring relaxed pass thresholds and LLM-based soft verification, (iii) comparing formally correct vs incorrect solutions with human evaluation.

Result: Richer test suites improve code generation (+3 pass@1), relaxed thresholds recover valuable training data (2-4 point improvement), and retaining diverse correct solutions yields consistent generalization gains.

Conclusion: Verification is necessary but too rigid in current practice. By combining calibrated verification with diverse problem-solution pairs, we can break the verification ceiling and develop stronger code generation models.

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [14] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge introduces pseudo-code as an intermediate modality to bridge the semantic gap between natural language queries and programming language code, improving code retrieval accuracy and robustness to code style variations.


<details>
  <summary>Details</summary>
Motivation: Existing PLM-based code search methods struggle with the fundamental semantic gap between human intent and machine execution logic, and have limited robustness to diverse code styles.

Method: A two-stage framework: 1) Use LLM to synthesize pseudo-code for explicit NL-pseudo-code alignment, 2) Generate stylistically diverse but logically equivalent code implementations with pseudo-code alignment to enhance robustness.

Result: PseudoBridge consistently outperforms baselines across 10 PLMs and 6 programming languages, achieving significant gains in retrieval accuracy and generalization, especially in zero-shot domain transfer scenarios.

Conclusion: The explicit logical alignment via pseudo-code is effective, making PseudoBridge a robust and generalizable solution for code retrieval.

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [15] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: CodeHinter is an AI-assisted debugging tool that combines traditional debugging tools with LLM-based techniques to help novice programmers fix semantic errors while promoting active engagement in the debugging process.


<details>
  <summary>Details</summary>
Motivation: Current AI-assisted debugging tools foster over-reliance on AI and don't actively engage students in the debugging process. The goal is to design an intuitive debugging assistant that helps novices while promoting their active participation.

Method: CodeHinter combines traditional debugging tools with LLM-based techniques. The tool was tested with undergraduate students in its second design iteration, focusing on features like error localization.

Result: Students found the tool highly effective in resolving semantic errors and significantly easier to use than the first version. Error localization was identified as the most valuable feature.

Conclusion: AI-assisted debugging tools should be personalized based on user profiles to optimize interactions with students, rather than fostering passive reliance on AI.

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [16] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: This paper analyzes quantum software engineering challenges by classifying Stack Overflow questions using transformer models, achieving 95% accuracy with BERT-based approaches and providing model interpretability through SHAP analysis.


<details>
  <summary>Details</summary>
Motivation: Quantum developers face challenges in optimizing quantum computing and need better ways to categorize and understand QSE challenges discussed on platforms like Stack Overflow, where current tags focus on technical aspects rather than developer challenges.

Method: Extracted 2829 quantum-related questions from Q&A platforms, conducted content analysis and grounded theory to identify common challenges (Tooling, Theoretical, Learning, Conceptual, Errors, API Usage), validated annotations with ChatGPT, and compared fine-tuned transformer models (BERT, DistilBERT, RoBERTa) against traditional D&ML classifiers (FNN, CNN, LSTM).

Result: Transformer-based models achieved 95% average accuracy, outperforming D&ML classifiers (89%, 86%, 84% for FNN, CNN, LSTM respectively) by 6% without data augmentation. SHAP analysis provided model interpretability for linguistic feature importance.

Conclusion: The approach successfully classifies quantum developer challenges and can help vendors and forums better organize discussions. However, empirical evaluation with actual developers and vendors is needed for validation.

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [17] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR is a chain-of-thought fine-tuning approach that enhances LLMs' code review capabilities by training them to analyze multiple dimensions simultaneously using long COT techniques with Maximum Entropy modeling.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code review methods are limited by vague fine-tuning information and cannot analyze multiple dimensions simultaneously like human reviewers, leading to suboptimal performance.

Method: Proposes MelcotCR - combines chain-of-thought fine-tuning with Maximum Entropy modeling and pre-defined reasoning pathways to handle long COT prompts effectively, addressing context loss and reasoning logic issues.

Result: A 14B Qwen2.5 model fine-tuned with MelcotCR surpasses state-of-the-art methods in code issue detection accuracy and performs comparably to the 671B DeepSeek-R1 model.

Conclusion: MelcotCR enables smaller LLMs to achieve human-level multi-dimensional code review analysis through structured reasoning pathways, making high-quality automated code review more accessible.

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [18] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: This paper introduces an automated approach using BERTopic with seed words and LLM validation to classify citizen contributions on digital platforms like Brasil Participativo, enabling governments to process large volumes of input efficiently.


<details>
  <summary>Details</summary>
Motivation: Governments face challenges in utilizing citizen engagement on digital platforms due to the sheer volume of contributions, which makes manual classification infeasible, requires expert involvement, and needs alignment with official taxonomies.

Method: The approach combines BERTopic with seed words and automatic validation by large language models to generate coherent and institutionally aligned topics with minimal human effort.

Result: Initial results show that the generated topics are coherent and institutionally aligned, successfully transforming large volumes of citizen input into actionable data.

Conclusion: This methodology enables governments to effectively process citizen contributions for public policy purposes, overcoming scalability challenges through automated topic modeling and validation.

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>
