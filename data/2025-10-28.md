<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: First extended evaluation of Software Engineering Agents (SWE-Agents) on embodied task controller generation, showing how different information access levels affect performance in Minigrid environment tasks.


<details>
  <summary>Details</summary>
Motivation: SWE-Agents have proven effective for traditional software engineering tasks, but their performance for embodied tasks requiring information discovery remains unexplored.

Method: Adapted Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks from Minigrid environment, comparing performance across different information access conditions (with/without environment source code, varying interactive exploration capabilities).

Result: Quantified how different information access levels affect SWE-Agent performance for embodied tasks and analyzed the relative importance of static code analysis versus dynamic exploration.

Conclusion: Establishes controller generation for embodied tasks as crucial evaluation domain for SWE-Agents and provides baseline results for future research in efficient reasoning systems.

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [2] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: ToM-SWE introduces a dual-agent architecture with a software engineering agent and a theory-of-mind agent that models user mental states to improve coding agent performance on underspecified tasks.


<details>
  <summary>Details</summary>
Motivation: Current coding agents struggle to infer and track user intent when instructions are underspecified or context-dependent, limiting their effectiveness in real-world software engineering scenarios.

Method: A dual-agent architecture pairing a primary software engineering agent with a lightweight theory-of-mind partner agent that infers user goals, constraints, and preferences from instructions and interaction history, maintaining persistent memory of the user.

Result: ToM-SWE achieved 59.7% task success rate on stateful SWE benchmark vs 18.1% for OpenHands, and in a 3-week study with professional developers, participants found it useful 86% of the time.

Conclusion: Stateful user modeling through theory-of-mind agents significantly improves coding agent performance and user satisfaction in practical software engineering tasks.

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [3] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: RAG-enhanced GPT models can provide more comprehensive developer assistance than human developers in OSS projects, though they tend to be less concise.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of Retrieval-Augmented Generation (RAG) in assisting developers within the Mozilla Firefox project and reduce the burden on core maintainers.

Method: Empirical analysis comparing responses from human developers, standard GPT, and RAG-enhanced GPT using real queries from Mozilla's developer chat rooms, assessed by Mozilla experts on helpfulness, comprehensiveness, and conciseness.

Result: RAG-assisted responses were more comprehensive than human developers (62.50% vs 54.17%) and almost as helpful (75.00% vs 79.17%), but less concise and often verbose.

Conclusion: RAG-based tools show potential for enhancing developer assistance in OSS projects while reducing maintainer workload, though future improvements should focus on making responses more concise.

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [4] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: ArchISMiner is a framework that automatically identifies architecture-related posts and extracts architectural issue-solution pairs from Stack Overflow and other developer forums using ML/DL models and language models.


<details>
  <summary>Details</summary>
Motivation: Finding architectural knowledge in Stack Overflow is challenging due to large volume of unstructured content and fragmented discussions, requiring manual effort that is time-consuming and error-prone.

Method: Two-component framework: ArchPI uses ML/DL models, PLMs, and LLMs to identify Architecture-Related Posts; ArchISPE uses indirect supervised approach with BERT embeddings and TextCNN features to extract architectural issue-solution pairs.

Result: ArchPI achieved F1-score of 0.960 in ARP detection; ArchISPE outperformed baselines with F1-scores of 0.883 for issues and 0.894 for solutions. Applied to three additional forums, creating dataset of 18K+ architectural issue-solution pairs.

Conclusion: ArchISMiner helps architects and developers identify relevant architectural knowledge from developer communities more accurately and efficiently.

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [5] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: FeaGPT is the first framework enabling complete geometry-mesh-simulation workflows through conversational interfaces, automating FEA from engineering specifications to validated results without manual intervention.


<details>
  <summary>Details</summary>
Motivation: To democratize access to advanced computational engineering tools by enabling natural language control of complex FEA workflows, overcoming limitations of existing tools that only automate individual components.

Method: Implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline that interprets engineering intent, generates physics-aware adaptive meshes, configures complete FEA simulations with boundary condition inference, and performs multi-objective analysis through closed-loop iteration.

Result: Successfully transforms natural language specifications into validated CalculiX simulations for industrial turbocharger cases (7-blade compressor and 12-blade turbine at 110,000 rpm), producing physically realistic results. Additional validation with 432 NACA airfoil configurations confirms scalability for parametric design exploration.

Conclusion: Natural language interfaces can effectively democratize access to advanced computational engineering tools while preserving analytical rigor, demonstrating complete end-to-end automation capability for FEA workflows.

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [6] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: Systematic review of 33 studies shows GenAI supports design ideation, artifact creation, and decision support in agile enterprise architecture, but faces risks like bias, incorrect outputs, and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To systematically understand how Generative AI is reshaping enterprise architecture work in agile software organizations, as evidence on its effects remains scattered.

Method: Systematic literature review following Kitchenham and PRISMA protocols, analyzing 1,697 records to identify 33 relevant studies across various architect roles.

Result: GenAI consistently supports design ideation, rapid artifact creation, and architectural decision support. Identified risks include opacity/bias, incorrect outputs, privacy/compliance issues, and social loafing. Emerging skills include prompt engineering and model evaluation.

Conclusion: The review provides a mapping of GenAI use cases and risks, implications for capability building and governance, and establishes a research agenda for human-AI collaboration in architecture to enable responsible adoption.

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [7] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: LSPRAG is a framework that uses Language Server Protocol to provide precise context for LLM-based unit test generation, achieving significant coverage improvements across multiple programming languages.


<details>
  <summary>Details</summary>
Motivation: Existing unit test generation approaches struggle with cross-language generalization and real-time operation, while current LLM solutions need precise context but rely on imprecise similarity searches or costly static analysis pipelines.

Method: LSPRAG leverages off-the-shelf Language Server Protocol back-ends to supply LLMs with precise symbol definitions and references in real time, enabling language-aware context retrieval with minimal per-language engineering.

Result: LSPRAG increased line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python compared to the best baseline performance.

Conclusion: LSPRAG provides an effective language-agnostic solution for real-time unit test generation by reusing mature LSP servers, significantly outperforming existing approaches across multiple programming languages.

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [8] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: FAME framework combines formal verification and runtime monitoring to detect silent AI failures in safety-critical systems, achieving 93.5% detection rate in autonomous vehicle testing.


<details>
  <summary>Details</summary>
Motivation: Address silent failures in AI systems where confident but incorrect outputs pose safety risks in critical applications like autonomous vehicles.

Method: Integrates offline formal synthesis with online runtime monitoring to create verifiable safety nets around opaque AI components.

Result: Successfully detected 93.5% of critical safety violations in autonomous vehicle perception systems that would otherwise be silent failures.

Conclusion: Provides a practical, certifiable pathway for trustworthy AI deployment, shifting from probabilistic performance to provable safety in next-generation systems.

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [9] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: This study analyzes Self-Admitted Technical Debt (SATD) in test code, revealing it has different characteristics than production code SATD and isn't directly linked to test smells. The research developed machine learning models to classify SATD types, with CodeBERT performing best.


<details>
  <summary>Details</summary>
Motivation: Previous SATD research focused mainly on production code, overlooking test code SATD which has different characteristics and doesn't fit existing categories. There was a gap in understanding SATD distribution, types, and impact on test quality in test code.

Method: Empirical study of 17,766 SATD comments (2,779 from test code) from 50 repositories. Analyzed distribution, types, and relationship with test quality. Developed machine learning models including CodeBERT to automatically classify SATD comments by type.

Result: SATD widely exists in test code but isn't directly associated with test smells. Comprehensive categories of SATD types in test code were identified. CodeBERT-based model outperformed other ML models in recall and F1-score, though performance varied across different SATD types.

Conclusion: SATD in test code has distinct characteristics from production code SATD and requires different management approaches. Machine learning models, particularly CodeBERT, can effectively classify SATD types for better management, though performance depends on the specific SATD type.

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [10] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: Ten practical rules for using AI coding tools in scientific computing that balance AI capabilities with scientific rigor, focusing on problem preparation, context management, testing, and code quality.


<details>
  <summary>Details</summary>
Motivation: AI coding tools can accelerate software development but raise concerns about code quality and scientific validity in research contexts, requiring guidelines to ensure methodological rigor.

Method: Proposes ten practical rules organized around four themes: problem preparation and understanding, managing context and interaction, testing and validation, and code quality assurance with iterative improvement.

Result: A framework that helps researchers leverage AI's transformative potential while maintaining human agency, robust validation procedures, and domain expertise for methodologically sound research.

Conclusion: These rules enable researchers to harness AI for faster software development while ensuring code meets standards of reliability, reproducibility, and scientific validity required for research integrity.

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [11] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: This paper explores using Large Language Models (LLMs) to complement ISTQB certification framework for software testing education, including dataset creation, prompt optimization, systematic evaluation, and integration recommendations.


<details>
  <summary>Details</summary>
Motivation: Software testing education needs continuous improvement to reflect current field state. ISTQB certification is globally recognized but rarely combined with recent generative AI advances. LLMs' potential for ISTQB-based learning remains unexplored.

Method: Created comprehensive ISTQB-aligned dataset (28 exams, 1,145 questions over 10+ years), developed domain-optimized prompts, systematically evaluated state-of-the-art LLMs on ISTQB tasks.

Result: Developed enhanced LLM precision and explanation quality through optimized prompts. Found LLMs show promise in supporting ISTQB certification preparation and software testing education.

Conclusion: LLMs have significant potential to complement ISTQB framework in higher education. The research provides foundation for broader LLM integration in software engineering education and actionable recommendations for implementation.

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [12] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: This paper explores using LLMs to generate better code comments by leveraging design documents as context, addressing the issue of useless comments from novice coders that hinder code maintenance.


<details>
  <summary>Details</summary>
Motivation: Novice coders create many codebases with useless comments due to lack of standards, which increases maintenance time. LLMs could potentially generate better comments to address this problem.

Method: The study focuses on using design documents as context for LLMs to generate more useful comments, since maintainers often rely on design documents when comments are insufficient.

Result: The paper investigates the feasibility of this approach but does not present specific experimental results in the abstract.

Conclusion: Using design documents as context for LLMs shows promise for generating more useful code comments that could improve code maintenance efficiency.

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [13] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: First large-scale study of self-admitted technical debt (SATD) in test code, analyzing 50,000 comments from 1,000 Java projects to identify 615 SATD instances and create a taxonomy of 15 categories, while finding current detection tools and LLMs perform poorly.


<details>
  <summary>Details</summary>
Motivation: While SATD in source code has been extensively studied, its presence and impact in test code has received no focused attention, leaving a significant gap in understanding how technical debt manifests in testing contexts.

Method: Manually analyzed 50,000 comments randomly sampled from 1.6 million comments across 1,000 open-source Java projects, identified 615 SATD comments, classified them into 15 categories, and evaluated existing SATD detection tools and LLMs for automatic detection.

Result: Identified 615 SATD comments and built a taxonomy of 15 distinct categories of test code SATD. Existing tools showed moderate recall (MAT performed best), while both open-source and proprietary LLMs exhibited poor detection accuracy primarily due to low precision.

Conclusion: Current approaches and LLMs cannot reliably detect SATD in test code. This work provides foundational understanding of test code SATD types and detection limitations, laying groundwork for future research on test code-specific technical debt.

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [14] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: The study reveals that discrimination in software engineering extends beyond gender and race to include age, political views, disabilities, and neurodivergence, with gender and age discrimination being most prevalent.


<details>
  <summary>Details</summary>
Motivation: To highlight the multifaceted nature of discrimination in software engineering beyond commonly discussed gender and racial disparities, using survey data to uncover less visible forms of bias.

Method: Secondary analysis of 800 open-ended responses from the State of the Developer Nation 2025 survey (8,717 total participants), examining patterns of perceived discrimination across multiple identity facets.

Result: Age- and gender-related discrimination were most frequently reported, with political/religious discrimination also significant. Women and non-binary individuals reported higher rates of discrimination (35%) and mental health challenges (62%), often with intersectional factors. Caregiving discrimination affected all genders.

Conclusion: Discrimination in software development is multifaceted, requiring researchers to consider diverse identity facets beyond age and gender in study design to better understand workplace challenges.

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [15] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: AutoCrashFL is an LLM agent for fault localization that uses only crashdumps and source code access, achieving 30% top-1 accuracy on industrial crashes compared to 17% baseline.


<details>
  <summary>Details</summary>
Motivation: Traditional fault localization techniques require expensive dynamic analyses like coverage profiling, which are impractical for large industrial software with millions of lines of code.

Method: Proposes AutoCrashFL, an LLM agent that localizes crashes using only crashdump information and source code repository access, without requiring expensive dynamic analysis.

Result: AutoCrashFL identified 30% of crashes at top position vs 17% baseline on SAP HANA (35M+ LOC), showing better performance on complex bugs and providing confidence indicators.

Conclusion: LLM agents like AutoCrashFL demonstrate practical deployment potential for fault localization at industrial scale, offering effective crash analysis without expensive dynamic instrumentation.

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [16] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: DynaCausal is a dynamic causality-aware framework for root cause analysis in microservice systems that addresses challenges in modeling fault propagation, noise interference, and causal attribution through multi-modal dynamic signal fusion and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing RCA approaches fail to capture dynamic behaviors and shifting service relationships in cloud-native microservices, with limitations in modeling cascading fault propagation, vulnerability to noise, and over-reliance on service deviation intensity.

Method: Unifies multi-modal dynamic signals to capture time-varying spatio-temporal dependencies through interaction-aware representation learning, introduces dynamic contrastive mechanism to disentangle true fault indicators from noise, and adopts causal-prioritized pairwise ranking objective.

Result: Achieves average AC@1 of 0.63 with absolute gains from 0.25 to 0.46, consistently surpassing state-of-the-art methods on public benchmarks.

Conclusion: DynaCausal delivers both accurate and interpretable diagnoses in highly dynamic microservice environments by explicitly optimizing causal attribution.

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [17] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: This paper investigates confidence calibration for AI code generation models in IDEs, finding that general post-hoc calibration doesn't improve reliability, while personalized calibration shows limited effectiveness. The study also reveals developer preference for color-coded reliability indicators over numerical ones.


<details>
  <summary>Details</summary>
Motivation: Large language models in IDEs are revolutionizing software engineering but pose challenges for AI-generated code reliability. Post-hoc calibration aims to align model confidences with acceptability, but evidence at scale is limited.

Method: Developed a scalable calibration framework for open-source models using any dataset. Analyzed 24+ million developer interactions across multiple languages. Conducted multi-phase design study with 3 expert designers and 153 developers using scenario-based design, interviews, and surveys.

Result: General post-hoc calibration (Platt-scaling) doesn't improve reliability of model confidence signals on average. Personalized calibration to individual users can be effective but depends heavily on user interaction data volume. Developers prefer non-numerical, color-coded reliability indicators in IDE workflows.

Conclusion: Standard calibration methods don't reliably improve code model confidence signals, while effective personalization requires substantial user data. Visual, non-numerical reliability indicators are preferred by developers for in-IDE code generation workflows.

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [18] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.SE

TL;DR: An LLM-based multi-agent system automates C4 software architecture model creation by simulating expert dialogues, with hybrid evaluation combining deterministic checks and LLM-as-a-Judge scoring.


<details>
  <summary>Details</summary>
Motivation: Manual creation of C4 software architecture models is time-consuming despite being fundamental to software system design, creating a need for automation.

Method: Multi-agent LLM system with role-specific experts analyzing requirements to generate Context, Container, and Component views, evaluated through hybrid framework with deterministic checks and LLM-as-a-Judge scoring.

Result: The workflow enables fast C4 model creation with high compilation success and semantic fidelity across five canonical system briefs, with different LLMs showing varied architectural design strengths.

Conclusion: This study advances automated software architecture design and evaluation methods, demonstrating effective LLM-based automation for C4 model generation.

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [19] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: The paper studies dependency pinning in software ecosystems, finding that over 60% of Maven consumers use stale dependencies that miss security fixes. It introduces Pin-Freshener, a tool that uses crowdsourced tests from peer projects to provide additional safety signals for dependency upgrades.


<details>
  <summary>Details</summary>
Motivation: Dependency pinning ensures reproducible builds but risks using outdated versions with bugs and security vulnerabilities. The research aims to understand the prevalence and consequences of stale dependencies and provide a solution to encourage safer upgrades.

Method: Defined stale vs fresh pins based on dependency outdatedness. Conducted empirical study on Maven libraries. Developed Pin-Freshener prototype that leverages crowdsourced tests from peer projects to provide additional safety signals for dependency upgrades.

Result: Found over 60% of Maven consumers use stale pins, some over a year old. 10% of dependency upgrades would reduce security vulnerabilities. Pin-Freshener shows 1-5 additional test suites can provide 35-100% more coverage. Can provide safety signals for over 3,000 consumers to perform security-reducing upgrades.

Conclusion: Pin-Freshener offers practical confidence to developers by providing additional safety signals beyond their own test suites, representing an improvement over current dependency management practices and helping reduce security vulnerabilities.

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [20] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: CodeAD is a framework that automatically generates lightweight Python rule functions for log-based anomaly detection using LLMs, achieving better performance and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing log-based anomaly detection methods suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, while rule-based systems require manual effort and are hard to scale.

Method: Uses hierarchical clustering and anchor-grounded sampling to create contrastive log windows, then employs an agentic workflow to iteratively generate, test, repair, and refine Python rules using LLMs.

Result: Achieves 3.6% F1 score improvement over state-of-the-art baselines, processes datasets 4x faster, with total LLM cost under $4 per dataset.

Conclusion: CodeAD provides a practical, scalable solution for interpretable, efficient, and automated log-based anomaly detection in real-world environments.

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [21] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: TALM is a tree-structured multi-agent framework with long-term memory that improves code generation through dynamic task decomposition, localized reasoning, and experience reuse.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent frameworks for code generation suffer from rigid workflows and high reasoning recovery costs, limiting their effectiveness in complex context management and multi-step reasoning tasks.

Method: TALM uses an extensible tree-based collaboration structure with parent-child relationships, divide-and-conquer strategy, localized re-reasoning, and a long-term memory module for semantic querying and experience integration.

Result: Experimental results on HumanEval, BigCodeBench, and ClassEval benchmarks show TALM delivers strong reasoning performance and high token efficiency in complex code generation tasks.

Conclusion: TALM demonstrates robustness and practical utility in agentic code generation by enabling flexible reasoning, efficient error correction, and implicit self-improvement through experience reuse.

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [22] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: Evaluation of five lightweight open-source LLMs on three RE tasks using user feedback, showing moderate-to-high performance in classification and specification generation.


<details>
  <summary>Details</summary>
Motivation: Online user feedback is valuable for RE but challenging to analyze due to volume and noise. LLMs show potential to automate this process and enable new tasks like requirements specification generation.

Method: Evaluated five lightweight open-source LLMs on three RE tasks: user request classification, NFR classification, and requirements specification generation. Used two feedback datasets for classification and human evaluation for specification quality.

Result: LLMs achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and moderately high specification quality (mean ~ 3/5).

Conclusion: Lightweight LLMs show promise for feedback-driven requirements development, though limitations exist. The study provides empirical evidence, replication package, and insights into LLM capabilities for RE.

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [23] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: Checkstyle+ combines traditional rule-based linter Checkstyle with LLM capabilities to detect nuanced code style violations that conventional static analysis misses.


<details>
  <summary>Details</summary>
Motivation: Traditional linters like Checkstyle use rigid rule-based mechanisms that miss semantically nuanced style rules requiring deeper code understanding.

Method: Hybrid approach augmenting Checkstyle with large language model capabilities to identify style violations beyond conventional rule-based analysis.

Result: Evaluated on 380 Java code files from 30,800 real-world programs, Checkstyle+ achieved superior performance over standard Checkstyle in detecting violations of semantically nuanced rules.

Conclusion: The hybrid LLM-augmented approach effectively addresses limitations of traditional rule-based linters for detecting nuanced code style violations.

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [24] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: LLMs can effectively generate test cases from natural language requirements for Alloy specifications, with GPT-5 showing strong performance in creating syntactically correct test cases that detect human-written specification errors.


<details>
  <summary>Details</summary>
Motivation: Validation of formal specifications is crucial but test case creation is burdensome and error-prone, causing users to skip this important validation task.

Method: Empirical evaluation using pre-trained LLMs (primarily GPT-5) to automatically generate test cases from natural language requirements for Alloy specification language domain models.

Result: GPT-5 effectively generates syntactically correct positive and negative test cases that satisfy (or violate) given requirements and can detect many wrong specifications written by humans.

Conclusion: LLMs, particularly GPT-5, show promise in automating test case generation for formal specification validation, potentially reducing the burden of manual test creation.

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [25] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael Sá Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: The paper presents NeuroCodeBench 2.0, a benchmark for verifying neural network implementations in C, and evaluates eight software verifiers showing they correctly solve only 11% of cases while producing 3% incorrect verdicts.


<details>
  <summary>Details</summary>
Motivation: Existing neural network verification techniques cannot certify the absence of faults at the software level, which is critical for safety-critical systems deployment.

Method: Constructed NeuroCodeBench 2.0 with 912 neural network verification examples in plain C, covering activation functions, common layers, and full networks up to 170K parameters, compatible with SV-COMP format.

Result: Evaluation of eight state-of-the-art software verifiers showed they correctly solve an average of 11% of benchmark cases while producing around 3% incorrect verdicts.

Conclusion: The benchmark has already had a positive impact on verifier development, but current automated verification tools have limited effectiveness for neural network code verification.

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [26] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: Proposes ML System Maps - causal maps that enable systematic attribution of distribution shifts in ML systems through layered views showing propagation paths between environment and system internals.


<details>
  <summary>Details</summary>
Motivation: Current ML system monitoring focuses on detecting distribution shifts but lacks systematic methods to identify root causes, relying on manual tracing to distinguish between software faults, data-quality issues, or natural changes.

Method: Develops ML System Maps - causal maps with layered views that explicitly show propagation paths between the environment and ML system's internals, enabling systematic root-cause analysis.

Result: The approach provides a framework for systematic attribution of distribution shifts in ML systems, moving beyond simple detection to understanding underlying causes.

Conclusion: ML System Maps offer a promising approach for comprehensive ML system monitoring and root-cause analysis, with a research agenda outlined for further development and evaluation.

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>
