<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: This paper examines whether Research Software Science (RSS) should be classified as metascience, finding that RSS aligns with metascience goals but is best understood as a distinct interdisciplinary domain that bridges technical, social, and cognitive aspects of research.


<details>
  <summary>Details</summary>
Motivation: As computational methods become central to research, ensuring software quality, reproducibility, and transparency is critical for scientific integrity. The classification of RSS affects recognition, funding, and integration into research improvement efforts.

Method: The authors define metascience and RSS, compare their principles and objectives, examine overlaps, and analyze arguments for and against classification. They consider both broad and narrow definitions of metascience.

Result: RSS advances core metascience goals, particularly in computational reproducibility, and bridges multiple aspects of research. Classification depends on whether one adopts a broad or narrow definition of metascience.

Conclusion: RSS is best understood as a distinct interdisciplinary domain that aligns with metascience. Recognizing this classification can strengthen RSS's role in improving research reliability, justify funding, and elevate software development in research institutions.

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [2] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: An agentic LLM framework for translating tax laws into executable code using metamorphic testing, achieving 45% pass rate with smaller models vs 9-15% with frontier models.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for legal translation but face reliability challenges due to ambiguity and hallucinations in legally critical settings like tax preparation.

Method: Multi-agent system using higher-order metamorphic relations for test generation, with LLM-driven role-based framework to automate code synthesis and metamorphic-testing agent to find counterexamples.

Result: Framework using GPT-4o-mini achieved 45% worst-case pass rate, significantly outperforming GPT-4o and Claude 3.5 (9-15%) on complex tax-code tasks.

Conclusion: Agentic LLM methodologies provide a path to robust, trustworthy legal-critical software from natural-language specifications.

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [3] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG transforms natural language descriptions into executable Apache Airflow DAGs using four generation approaches, with Hybrid method emerging as optimal (78.5% success rate) outperforming LLM-only and Direct methods.


<details>
  <summary>Details</summary>
Motivation: Developing reliable data enrichment pipelines demands significant engineering expertise, so the goal is to democratize data pipeline development through automated workflow generation.

Method: Four generation approaches (Direct, LLM-only, Hybrid, Template-based) evaluated across 260 experiments using thirteen LLMs and five case studies, measured using penalized scoring framework combining reliability with code quality (SAT), structural integrity (DST), and executability (PCT).

Result: Hybrid approach achieved 78.5% success rate with robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76), significantly outperforming LLM-only (66.2%) and Direct (29.2%) methods. Reliability is the primary differentiator, not intrinsic code quality. Hybrid method is over twice as efficient as Direct prompting per successful DAG.

Conclusion: A structured, hybrid approach is essential for balancing flexibility and reliability in automated workflow generation, offering a viable path to democratize data pipeline development.

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [4] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: LLMs can significantly enhance crash reports by adding fault locations, root-cause explanations, and repair suggestions, improving debugging accuracy from 10.6% to over 40%.


<details>
  <summary>Details</summary>
Motivation: Crash reports often lack sufficient diagnostic detail for efficient debugging, creating a need for automated enhancement to help developers resolve issues faster.

Method: Two strategies: Direct-LLM (single-shot approach using stack-trace context) and Agentic-LLM (iterative approach exploring repository for additional evidence), tested on 492 real-world crash reports.

Result: Top-1 problem-localization accuracy improved from 10.6% to 40.2-43.1%, with suggested fixes resembling developer patches (CodeBLEU ~56-57%). Agentic-LLM provided stronger explanations and more actionable guidance.

Conclusion: LLMs supplied with stack traces and repository code can produce substantially more useful crash reports for debugging, as confirmed by both automated metrics and user studies.

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [5] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot's code review feature performs poorly at detecting critical security vulnerabilities like SQL injection and XSS, instead focusing on low-severity issues like coding style errors.


<details>
  <summary>Details</summary>
Motivation: As AI-powered tools become integral to software development, it's critical to evaluate their effectiveness in supporting secure coding practices, particularly GitHub Copilot's new code review feature.

Method: Used a curated set of labeled vulnerable code samples from diverse open-source projects across multiple programming languages and domains to systematically assess Copilot's vulnerability detection capabilities.

Result: Copilot's code review frequently failed to detect critical vulnerabilities (SQL injection, XSS, insecure deserialization) and primarily provided feedback on low-severity issues like coding style and typographical errors.

Conclusion: There's a significant gap between perceived and actual capabilities of AI-assisted code review, highlighting the continued need for dedicated security tools and manual code audits to ensure software security.

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [6] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest is a regression testing framework for Jupyter notebooks that enables cell-level assertions and automated assertion generation for ML pipelines, improving reliability without adding developer burden.


<details>
  <summary>Details</summary>
Motivation: Notebooks are widely used for ML development but provide limited testing support, leading to subtle bugs and performance regressions that often go unnoticed during continuous development.

Method: Developed a framework with assertion APIs and JupyterLab plugin for writing cell-level assertions, plus automated assertion generation for key ML components (data processing, model building, evaluation). Uses statistical techniques to handle non-deterministic computations and minimize flakiness.

Result: Evaluated on 592 Kaggle notebooks, generating 21,163 assertions (35.75 per notebook) with mutation score of 0.57. Successfully caught regression bugs and was adopted in CI of a popular ML library. User study (17 participants) showed high usability ratings (4.3/5 intuitive, 4.24/5 useful).

Conclusion: NBTest effectively addresses testing gaps in ML notebooks by providing practical assertion capabilities and automated generation, improving reliability while maintaining developer productivity.

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [7] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE is a framework that evaluates code generation models' sensitivity to prompt phrasing variations with different emotions and personalities, revealing that performance and stability are largely decoupled objectives.


<details>
  <summary>Details</summary>
Motivation: Code generation models are widely used but their sensitivity to prompt phrasing variations (emotions, communication styles) remains under-examined, while most benchmarks only focus on peak performance.

Method: Creates semantically equivalent prompt variants using emotion and personality templates, evaluates stability with probability-aware continuous scoring or binary pass rates, and aggregates results into AUC-E metric for cross-model comparison.

Result: Across 14 models from Llama, Qwen, and DeepSeek families, performance and stability behave as largely decoupled optimization objectives, revealing architectural and scale-related patterns that challenge common assumptions about model robustness.

Conclusion: PromptSE enables practitioners to quantify performance-stability trade-offs for deployment and model selection, positioning prompt stability as a complementary evaluation dimension alongside performance and fairness for more trustworthy AI-assisted software development.

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [8] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: CodeEraser enables efficient erasure of sensitive memorized data from deployed Code Language Models using selective machine unlearning, eliminating need for full retraining while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: CLMs exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information. Existing solutions require costly full-model retraining.

Method: Developed CodeEraser - a gradient ascent-based unlearning approach that selectively removes sensitive memorized segments while preserving code structure and functionality. Used curated dataset of 50,000 high-risk samples.

Result: Extensive experiments on CodeParrot, CodeGen-Mono, and Qwen2.5-Coder show CodeEraser effectively erases targeted sensitive memorization while maintaining model performance.

Conclusion: Machine unlearning provides an efficient post-hoc solution for removing sensitive data from deployed CLMs without full retraining, addressing critical privacy vulnerabilities.

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [9] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: This paper analyzes reasoning patterns of large reasoning models (LRMs) in code generation, identifying 15 reasoning actions across 4 phases, comparing different models' approaches, and showing how specific reasoning actions correlate with code correctness.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in large reasoning models (LRMs) for code generation, there is limited systematic analysis of their reasoning patterns and how these patterns influence generated code quality.

Method: Prompted state-of-the-art LRMs with code generation tasks, applied open coding to manually annotate reasoning traces, and derived a taxonomy of 15 reasoning actions across four phases.

Result: Identified common human-like reasoning patterns, found model-specific differences (Qwen3 iterative vs DeepSeek-R1-7B linear), showed unit test creation and scaffold generation strongly support functional correctness, and demonstrated context-aware prompts improve code quality.

Conclusion: The study provides insights into LRM reasoning behaviors during code generation and offers practical implications for improving automatic code generation through better understanding of reasoning patterns.

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [10] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS is a spectrum-based failure attribution approach for multi-agent systems that identifies which agent actions cause failures through systematic trajectory replay and analysis.


<details>
  <summary>Details</summary>
Motivation: Failure attribution in multi-agent systems is labor-intensive and underexplored, making debugging and system improvement challenging despite the increasing use of LLM-powered MASs for complex problems.

Method: FAMAS uses systematic trajectory replay and abstraction followed by spectrum analysis. It estimates failure likelihood for each agent action by analyzing variations across repeated executions, using a novel suspiciousness formula that integrates agent behavior patterns and action activation patterns.

Result: FAMAS outperformed all 12 baseline methods in evaluations on the Who and When benchmark, demonstrating superior performance in failure attribution.

Conclusion: FAMAS provides an effective automated approach for failure attribution in multi-agent systems, addressing a critical gap in debugging and improving LLM-powered MASs.

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [11] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Autoscope introduces span-level sampling that reduces trace storage by 81.2% while maintaining 98.1% faulty span coverage and improving root cause analysis by 8.3%.


<details>
  <summary>Details</summary>
Motivation: Traditional trace sampling discards valuable normal traces needed for comparative analysis while dealing with storage burden from massive trace volumes in microservices.

Method: Trace Sampling 2.0 operating at span level with static analysis to extract execution logic, preserving critical spans while maintaining trace structure consistency.

Result: 81.2% reduction in trace size, 98.1% faulty span coverage, and 8.3% average improvement in root cause analysis effectiveness.

Conclusion: Autoscope significantly enhances observability and storage efficiency in microservices, providing a robust solution for performance monitoring that outperforms existing trace-level sampling methods.

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [12] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: Prompt-based large language models can match or exceed traditional supervised learning for requirements classification, reducing the need for large labeled datasets while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning for requirements classification requires large labeled datasets that are costly, slow to create, domain-dependent, and generalize poorly. This study explores whether prompt-based LLMs can reduce data dependency while maintaining accuracy.

Method: Benchmarked several LLMs with different prompting styles (zero-shot, few-shot, persona, and chain-of-thought) across multiple tasks on two English datasets (PROMISE and SecReq). Compared best LLM configurations against a strong fine-tuned transformer baseline.

Result: Prompt-based LLMs, especially with few-shot prompts, can match or exceed the baseline performance. Adding persona or persona plus chain-of-thought further improves results.

Conclusion: Prompt-based LLMs are a practical and scalable alternative that reduces dependence on large annotations and improves generalizability across different requirements classification tasks.

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [13] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: Systematic review finds only 3 out of 1,386 papers address ethical considerations of GenAI in software modeling education, revealing critical gap in ethical discourse and need for frameworks.


<details>
  <summary>Details</summary>
Motivation: GenAI is rapidly being adopted in software modeling education without clear ethical oversight or pedagogical guidelines, shaping core learning outcomes while ethical implications remain underexplored.

Method: Conducted systematic literature review across six major computer science digital libraries (ACM, IEEE, Scopus, ScienceDirect, SpringerLink, Web of Science) to identify studies discussing ethical aspects of GenAI in software modeling education.

Result: Only 3 papers out of 1,386 unique papers explicitly addressed ethical considerations, highlighting a critical absence of ethical discourse in this field.

Conclusion: There is an urgent need for structured ethical frameworks and responsible integration of GenAI in modeling curricula, with emerging research opportunities and challenges requiring immediate attention.

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [14] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: This paper analyzes failure modes in automated issue solving on SWE-Bench, develops a taxonomy of 25 failure subcategories, and proposes an Expert-Executor framework that solves 22.2% of previously intractable issues.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of automated issue solving tools only report aggregate success rates, which obscure the underlying causes of failure and make it difficult to diagnose model weaknesses or guide targeted improvements.

Method: The authors analyzed performance of three state-of-the-art tools on SWE-Bench-Verified, conducted manual analysis of 150 failed instances to develop a comprehensive failure taxonomy, and proposed a collaborative Expert-Executor framework with supervisory oversight.

Result: The analysis revealed distinct failure patterns between pipeline-based and agentic architectures, with agentic failures mainly stemming from flawed reasoning and cognitive deadlocks. The proposed Expert-Executor framework solved 22.2% of previously unsolvable issues for a leading single agent.

Conclusion: The findings demonstrate the value of diagnostic evaluation and collaborative agent design for building more robust automated issue solving systems, with the Expert-Executor framework effectively addressing reasoning flaws and cognitive deadlocks.

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [15] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: Classical software processes (Waterfall, V-Model, Agile) can be adapted as coordination scaffolds for LLM-based multi-agent systems, with each process offering different trade-offs in code quality, cost, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore how traditional software development processes can be repurposed to guide autonomous collaboration in LLM-based multi-agent systems and examine their impact on development outcomes.

Method: Executed 11 diverse software projects under three process models (Waterfall, V-Model, Agile) and four GPT variants, totaling 132 runs. Evaluated outputs using standardized metrics for size, cost, and quality including code smells and bug detection.

Result: Both process model and LLM choice significantly affected performance. Waterfall was most efficient, V-Model produced verbose code, and Agile achieved highest code quality but at higher computational cost.

Conclusion: Classical software processes can be effectively implemented in LLM-based MAS, but each involves trade-offs across quality, cost, and adaptability. Process selection should align with project goals prioritizing efficiency, robustness, or structured validation.

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [16] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought reasoning improves LLM accuracy but increases computational costs. SEER framework adaptively compresses CoT reasoning, reducing length by 42.1% while maintaining accuracy and eliminating infinite loops.


<details>
  <summary>Details</summary>
Motivation: CoT reasoning enhances LLM performance but comes with high computational costs (latency, memory, KV-cache demands) that are problematic for software engineering tasks requiring concise outputs. Longer reasoning doesn't always help and can cause truncation and accuracy drops.

Method: SEER (Self-Enhancing Efficient Reasoning) framework that combines Best-of-N sampling with task-aware adaptive filtering. It dynamically adjusts thresholds based on pre-inference outputs to compress CoT while preserving accuracy.

Result: SEER shortens CoT by 42.1% on average, improves accuracy by reducing truncation, eliminates most infinite loops, and maintains performance while reducing computational overhead across three software engineering tasks and one math task.

Conclusion: SEER provides a practical method to make CoT-enhanced LLMs more efficient and robust under resource constraints, challenging the assumption that longer reasoning is always better and demonstrating the need for adaptive CoT control.

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>
