{"id": "2509.15283", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "I.2.7; F.2.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2509.15283", "abs": "https://arxiv.org/abs/2509.15283", "authors": ["Kadin Matotek", "Heather Cassel", "Md Amiruzzaman", "Linh B. Ngo"], "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges", "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern\n  2025", "summary": "This study examines the performance of today's open-source, locally hosted\nlarge-language models (LLMs) in handling complex competitive programming tasks\nwith extended problem descriptions and contexts. Building on the original\nFramework for AI-driven Code Generation Evaluation (FACE), the authors retrofit\nthe pipeline to work entirely offline through the Ollama runtime, collapsing\nFACE's sprawling per-problem directory tree into a handful of consolidated JSON\nfiles, and adding robust checkpointing so multi-day runs can resume after\nfailures. The enhanced framework generates, submits, and records solutions for\nthe full Kattis corpus of 3,589 problems across eight code-oriented models\nranging from 6.7-9 billion parameters. The submission results show that the\noverall pass@1 accuracy is modest for the local models, with the best models\nperforming at approximately half the acceptance rate of the proprietary models,\nGemini 1.5 and ChatGPT-4. These findings expose a persistent gap between\nprivate, cost-controlled LLM deployments and state-of-the-art proprietary\nservices, yet also highlight the rapid progress of open models and the\npractical benefits of an evaluation workflow that organizations can replicate\non in-house hardware.", "AI": {"tldr": "Evaluation of open-source LLMs on competitive programming tasks using enhanced FACE framework shows local models perform at half the accuracy of proprietary models like GPT-4 and Gemini 1.5.", "motivation": "To assess the performance gap between open-source locally hosted LLMs and proprietary models in handling complex competitive programming tasks with extended contexts.", "method": "Enhanced the FACE framework to work offline via Ollama runtime, consolidated directory structure into JSON files, added checkpointing, and tested 8 code-oriented models (6.7-9B parameters) on 3,589 Kattis problems.", "result": "Open-source models achieved modest pass@1 accuracy, with best models performing at approximately 50% of proprietary models' acceptance rates.", "conclusion": "Significant gap exists between local and proprietary LLMs, but open-source models show rapid progress and the framework enables reproducible evaluation on in-house hardware."}}
{"id": "2509.15397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15397", "abs": "https://arxiv.org/abs/2509.15397", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "LoCaL: Countering Surface Bias in Code Evaluation Metrics", "comment": null, "summary": "With the increasing popularity of large language models (LLMs) and LLM-based\nagents, reliable and effective code evaluation metrics (CEMs) have become\ncrucial for progress across several software engineering tasks. While popular\nbenchmarks often provide test cases to assess the correctness of generated\ncode, crafting and executing test cases is expensive. Reference-based CEMs\nprovide a cheaper alternative by scoring a candidate program based on its\nfunctional similarity to a reference. Although prior research has focused on\nreporting the weak correlation between these CEMs and functional correctness,\nthe causes are only assumed, and plausible solutions remain unexplored. In this\nwork, we critically evaluate four state-of-the-art reference-based CEMs,\nrevealing their strong bias towards surface-level features rather than code\nfunctionality. Despite this surface bias, current evaluation datasets for these\nCEMs rarely include code pairs that are surface-similar yet functionally\ndissimilar, or functionally similar yet surface-dissimilar. To mitigate this\ngap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117\ncode pairs at both the method and program levels. Each pair is labeled with a\nfunctional similarity score and aims to target regions where CEMs are likely to\nperform poorly. The functional similarity scores are calculated through\ndifferential fuzzing, which eliminates the need for predefined test cases and,\nat the same time, improves the reliability of the scores by executing an order\nof magnitude more tests than prior work. We find that all four CEMs show\nsignificant performance degradation on LoCaL, compared to the baselines.\nFinally, based on our findings, we draw the implication that exposing CEMs to\nLoCaL-like data might facilitate the development of metrics that are robust to\nsurface bias.", "AI": {"tldr": "The paper critically evaluates reference-based code evaluation metrics (CEMs), revealing their surface-level bias, and introduces LoCaL benchmark with 3117 code pairs to test CEM robustness.", "motivation": "Current reference-based CEMs show weak correlation with functional correctness, but the causes are assumed and solutions unexplored. There's a need for better evaluation datasets that include challenging code pairs.", "method": "Proposes LoCaL benchmark with 3117 code pairs labeled with functional similarity scores calculated through differential fuzzing, which executes more tests than prior work without predefined test cases.", "result": "All four state-of-the-art CEMs show significant performance degradation on LoCaL compared to baselines, confirming their strong bias towards surface-level features rather than functionality.", "conclusion": "Exposing CEMs to LoCaL-like data can help develop metrics robust to surface bias, addressing the gap in current CEM evaluation practices."}}
{"id": "2509.15567", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15567", "abs": "https://arxiv.org/abs/2509.15567", "authors": ["Hongyu Kuang", "Ning Zhang", "Hui Gao", "Xin Zhou", "Wesley K. G. Assun\u00e7\u00e3o", "Xiaoxing Ma", "Dong Shao", "Guoping Rong", "He Zhang"], "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "comment": null, "summary": "Commit messages are valuable resources for describing why code changes are\ncommitted to repositories in version control systems (e.g., Git). They\neffectively help developers understand code changes and better perform software\nmaintenance tasks. Unfortunately, developers often neglect to write\nhigh-quality commit messages in practice. Therefore, a growing body of work is\nproposed to generate commit messages automatically. These works all\ndemonstrated that how to organize and represent code changes is vital in\ngenerating good commit messages, including the use of fine-grained graphs or\nembeddings to better represent code changes. In this study, we choose an\nalternative way to condense code changes before generation, i.e., proposing\nbrief yet concise text templates consisting of the following three parts: (1)\nsummarized code changes, (2) elicited comments, and (3) emphasized code\nidentifiers. Specifically, we first condense code changes by using our proposed\ntemplates with the help of a heuristic-based tool named ChangeScribe, and then\nfine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding\ncommit messages. Our proposed templates better utilize pre-trained language\nmodels, while being naturally brief and readable to complement generated commit\nmessages for developers. Our evaluation based on a widely used dataset showed\nthat our approach can outperform six baselines in terms of BLEU-Norm, METEOR,\nand ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,\nrespectively. The ablation study and human evaluation also provide further\ninsights into the effectiveness of our approach.", "AI": {"tldr": "This paper proposes a novel approach for automatic commit message generation by condensing code changes into structured text templates before generation, achieving significant improvements over existing methods.", "motivation": "Developers often neglect writing high-quality commit messages, making automated generation crucial for software maintenance. Existing methods focus on representing code changes but lack effective condensation strategies.", "method": "The approach uses three-part text templates (summarized changes, elicited comments, emphasized identifiers) created with ChangeScribe tool, then fine-tunes CodeLlama-7B on template-message pairs.", "result": "Outperformed six baselines with average improvements of 51.7% (BLEU-Norm), 78.7% (METEOR), and 62.5% (ROUGE-L). Ablation studies and human evaluation confirmed effectiveness.", "conclusion": "The proposed template-based condensation approach better utilizes pre-trained language models while maintaining brevity and readability, providing a complementary solution for commit message generation."}}
{"id": "2509.15777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15777", "abs": "https://arxiv.org/abs/2509.15777", "authors": ["Haoran Xu", "Zhi Chen", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches", "comment": null, "summary": "Open-source software vulnerability patch detection is a critical component\nfor maintaining software security and ensuring software supply chain integrity.\nTraditional manual detection methods face significant scalability challenges\nwhen processing large volumes of commit histories, while being prone to human\nerrors and omissions. Existing automated approaches, including heuristic-based\nmethods and pre-trained model solutions, suffer from limited accuracy, poor\ngeneralization capabilities, and inherent methodological constraints that\nhinder their practical deployment. To address these fundamental challenges,\nthis paper conducts a comprehensive empirical study of existing vulnerability\npatch detection methods, revealing four key insights that guide the design of\neffective solutions: the critical impact of search space reduction, the\nsuperiority of pre-trained semantic understanding over architectural\ncomplexity, the temporal limitations of web crawling approaches, and the\nadvantages of knowledge-driven methods. Based on these insights, we propose a\nnovel two-stage framework that combines version-driven candidate filtering with\nlarge language model-based multi-round dialogue voting to achieve accurate and\nefficient vulnerability patch identification. Extensive experiments on a\ndataset containing 750 real vulnerabilities demonstrate that our method\noutperforms current approaches.", "AI": {"tldr": "A novel two-stage framework combining version-driven filtering and LLM-based multi-round dialogue voting for accurate vulnerability patch detection in open-source software.", "motivation": "Traditional manual methods are unscalable and error-prone, while existing automated approaches suffer from limited accuracy, poor generalization, and practical deployment constraints.", "method": "Two-stage framework: 1) version-driven candidate filtering to reduce search space, 2) large language model-based multi-round dialogue voting for accurate patch identification.", "result": "Extensive experiments on 750 real vulnerabilities show the method outperforms current approaches.", "conclusion": "The proposed framework effectively addresses scalability and accuracy challenges in vulnerability patch detection through search space reduction and semantic understanding."}}
{"id": "2509.15893", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15893", "abs": "https://arxiv.org/abs/2509.15893", "authors": ["Andrea Bombarda", "Federico Conti", "Marcello Minervini", "Aurora Zanenga", "Claudio Menghi"], "title": "Failure Modes and Effects Analysis: An Experience from the E-Bike Domain", "comment": "12 pages", "summary": "Software failures can have catastrophic and costly consequences. Functional\nFailure Mode and Effects Analysis (FMEA) is a standard technique used within\nCyber-Physical Systems (CPS) to identify software failures and assess their\nconsequences. Simulation-driven approaches have recently been shown to be\neffective in supporting FMEA. However, industries need evidence of the\neffectiveness of these approaches to increase practical adoption. This\nindustrial paper presents our experience with using FMEA to analyze the safety\nof a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial\ntool that supports engineers with FMEA. We identified 13 realistic faults,\nmodeled them, and analyzed their effects. We sought expert feedback to analyze\nthe appropriateness of our models and the effectiveness of the faults in\ndetecting safety breaches. Our results reveal that for the faults we\nidentified, our models were accurate or contained minor imprecision that we\nsubsequently corrected. They also confirm that FMEA helps engineers improve\ntheir models. Specifically, the output provided by the simulation-driven\nsupport for 38.4% (5 out of 13) of the faults did not match the engineers'\nexpectations, helping them discover unexpected effects of the faults. We\npresent a thorough discussion of our results and ten lessons learned. Our\nfindings are useful for software engineers who work as Simulink engineers, use\nthe Simulink Fault Analyzer, or work as safety analysts.", "AI": {"tldr": "Industrial study on using Simulink Fault Analyzer for FMEA in e-Bike CPS, showing that simulation-driven FMEA helps identify unexpected fault effects and improves model accuracy.", "motivation": "Software failures in Cyber-Physical Systems can have catastrophic consequences, and industries need evidence of effectiveness for simulation-driven FMEA approaches to increase practical adoption.", "method": "Used Simulink Fault Analyzer to analyze safety of an e-Bike CPS, identified 13 realistic faults, modeled them, and analyzed their effects with expert feedback to assess model appropriateness and fault detection effectiveness.", "result": "Models were accurate or contained minor imprecision that were corrected; FMEA helped engineers improve models as 38.4% (5 out of 13) of faults revealed unexpected effects that didn't match engineers' expectations.", "conclusion": "FMEA with simulation support effectively helps engineers discover unexpected fault effects and improve their models, with findings useful for Simulink engineers, safety analysts, and users of Simulink Fault Analyzer."}}
{"id": "2509.15971", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15971", "abs": "https://arxiv.org/abs/2509.15971", "authors": ["Owen Truong", "Terrence Zhang", "Arnav Marchareddy", "Ryan Lee", "Jeffery Busold", "Michael Socas", "Eman Abdullah AlOmar"], "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines", "comment": null, "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.", "AI": {"tldr": "A VS Code extension called LeakageDetector that identifies and corrects Data Leakage issues in Jupyter Notebooks for ML code quality improvement.", "motivation": "To help ML engineers enhance code quality by detecting Data Leakage issues that cause misleading performance evaluations when test data information contaminates training data.", "method": "Developed a Visual Studio Code extension with detection capabilities for Overlap, Preprocessing, and Multi-test leakage types, plus two correction approaches: manual quick fixes and LLM-driven guidance for ML pipeline best practices.", "result": "Created LeakageDetector tool that can identify Data Leakage patterns and provide both conventional and AI-assisted correction mechanisms.", "conclusion": "The extension successfully addresses Data Leakage issues in ML development workflows, offering practical solutions for maintaining code quality and accurate model evaluation."}}
{"id": "2509.16081", "categories": ["cs.SE", "cs.MS", "G.1.3; D.2.11"], "pdf": "https://arxiv.org/pdf/2509.16081", "abs": "https://arxiv.org/abs/2509.16081", "authors": ["Marcel Koch", "Tobias Ribizel", "Pratik Nayak", "Fritz G\u00f6bel", "Gregor Olenik", "Terry Cojean"], "title": "Software Development Aspects of Integrating Linear Algebra Libraries", "comment": "16 pages, 2 figures", "summary": "Many scientific discoveries are made through, or aided by, the use of\nsimulation software. These sophisticated software applications are not built\nfrom the ground up, instead they rely on smaller parts for specific use cases,\nusually from domains unfamiliar to the application scientists. The software\nlibrary Ginkgo is one of these building blocks to handle sparse numerical\nlinear algebra on different platforms. By using Ginkgo, applications are able\nto ease the transition to modern systems, and speed up their simulations\nthrough faster numerical linear algebra routines. This paper discusses the\nchallenges and benefits for application software in adopting Ginkgo. It will\npresent examples from different domains, such as CFD, power grid simulation, as\nwell as electro-cardiophysiology. For these cases, the impact of the\nintegrations on the application code is discussed from a software engineering\nstandpoint, and in particular, the approaches taken by Ginkgo and the\napplications to enable sustainable software development are highlighted.", "AI": {"tldr": "Ginkgo is a sparse linear algebra library that helps scientific applications transition to modern systems and accelerate simulations by providing faster numerical routines. This paper examines the challenges and benefits of integrating Ginkgo into applications from various domains.", "motivation": "Scientific simulation software relies on specialized building blocks from unfamiliar domains. Ginkgo addresses the need for efficient sparse linear algebra routines that can run on different platforms, helping applications modernize and improve performance.", "method": "The paper presents case studies from different application domains (CFD, power grid simulation, electro-cardiophysiology) and analyzes the integration approaches from a software engineering perspective, highlighting sustainable development practices.", "result": "The integration of Ginkgo enables applications to transition to modern systems more easily and achieve faster simulation speeds through improved numerical linear algebra performance.", "conclusion": "Ginkgo provides a valuable building block for scientific applications, offering both performance benefits and sustainable software development approaches that facilitate easier adoption of modern computing platforms."}}
{"id": "2509.16140", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16140", "abs": "https://arxiv.org/abs/2509.16140", "authors": ["Avinash Patil"], "title": "When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes", "comment": "7 pages, 2 tables, 21 figures", "summary": "Efficient bug resolution is critical for maintaining software quality and\nuser satisfaction. However, specific bug reports experience unusually long\nresolution times, which may indicate underlying process inefficiencies or\ncomplex issues. This study presents a comprehensive analysis of bug resolution\nanomalies across seven prominent open-source repositories: Cassandra, Firefox,\nHadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods\nsuch as Z-score and Interquartile Range (IQR), we identify anomalies in bug\nresolution durations. To understand the thematic nature of these anomalies, we\napply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature\nextraction and KMeans clustering to group similar bug summaries. Our findings\nreveal consistent patterns across projects, with anomalies often clustering\naround test failures, enhancement requests, and user interface issues. This\napproach provides actionable insights for project maintainers to prioritize and\neffectively address long-standing bugs.", "AI": {"tldr": "Analysis of bug resolution anomalies in open-source projects using statistical methods and text mining to identify patterns in long-resolution bugs.", "motivation": "To understand why certain bug reports take unusually long to resolve and identify underlying process inefficiencies in software maintenance.", "method": "Used Z-score and IQR for anomaly detection in bug resolution times, combined with TF-IDF for textual feature extraction and KMeans clustering to group similar bug summaries across 7 open-source repositories.", "result": "Found consistent patterns where anomalies cluster around test failures, enhancement requests, and user interface issues across all studied projects.", "conclusion": "The approach provides actionable insights for project maintainers to prioritize and effectively address long-standing bugs, improving software quality and user satisfaction."}}
{"id": "2509.16187", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16187", "abs": "https://arxiv.org/abs/2509.16187", "authors": ["Ali Reza Ibrahimzada", "Brandon Paulsen", "Reyhaneh Jabbarvand", "Joey Dodds", "Daniel Kroening"], "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair", "comment": null, "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.", "AI": {"tldr": "MatchFixAgent is an LLM-based framework for validating and repairing code translations across multiple programming languages using a multi-agent architecture that combines semantic analysis and test execution.", "motivation": "Existing code translation validation methods struggle with generalization across programming languages due to high engineering overhead and reliance on inadequate test suites, leading to false equivalence claims and ineffective repairs.", "method": "Multi-agent architecture dividing equivalence validation into sub-tasks: semantic analysis, test writing/execution, translation repair, and final verdict decision based on semantic analyses and test results.", "result": "Achieves 99.2% verdict coverage on 2,219 translation pairs across 6 PL pairs, corrects prior work's errors 60.7% of the time, and repairs 50.6% of inequivalent translations (vs 18.5% for prior work).", "conclusion": "MatchFixAgent is significantly more adaptable to multiple programming language pairs than prior approaches while producing highly accurate validation results."}}
