{"id": "2510.05147", "categories": ["cs.SE", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05147", "abs": "https://arxiv.org/abs/2510.05147", "authors": ["Yu Zhu"], "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing", "comment": null, "summary": "Ensuring reliability in modern software systems requires rigorous\npre-production testing across highly heterogeneous and evolving environments.\nBecause exhaustive evaluation is infeasible, practitioners must decide how to\nallocate limited testing resources across configurations where failure\nprobabilities may drift over time. Existing combinatorial optimization\napproaches are static, ad hoc, and poorly suited to such non-stationary\nsettings. We introduce a novel reinforcement learning (RL) framework that\nrecasts configuration allocation as a sequential decision-making problem. Our\nmethod is the first to integrate Q-learning with a hybrid reward design that\nfuses simulated outcomes and real-time feedback, enabling both sample\nefficiency and robustness. In addition, we develop an adaptive online-offline\ntraining scheme that allows the agent to quickly track abrupt probability\nshifts while maintaining long-run stability. Extensive simulation studies\ndemonstrate that our approach consistently outperforms static and\noptimization-based baselines, approaching oracle performance. This work\nestablishes RL as a powerful new paradigm for adaptive configuration\nallocation, advancing beyond traditional methods and offering broad\napplicability to dynamic testing and resource scheduling domains.", "AI": {"tldr": "A reinforcement learning framework for adaptive configuration allocation in software testing that outperforms static methods by combining Q-learning with hybrid rewards and online-offline training.", "motivation": "Existing combinatorial optimization approaches are static and poorly suited for non-stationary testing environments where failure probabilities drift over time, requiring adaptive resource allocation.", "method": "Reinforcement learning framework with Q-learning, hybrid reward design (simulated outcomes + real-time feedback), and adaptive online-offline training scheme for tracking probability shifts.", "result": "Extensive simulations show consistent outperformance over static and optimization-based baselines, approaching oracle performance in dynamic testing scenarios.", "conclusion": "Establishes RL as a powerful new paradigm for adaptive configuration allocation, advancing beyond traditional methods with broad applicability to dynamic testing and resource scheduling."}}
{"id": "2510.05156", "categories": ["cs.SE", "cs.AI", "cs.CR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.05156", "abs": "https://arxiv.org/abs/2510.05156", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "comment": "22 pages", "summary": "The deployment of autonomous AI agents in sensitive domains, such as\nhealthcare, introduces critical risks to safety, security, and privacy. These\nagents may deviate from user objectives, violate data handling policies, or be\ncompromised by adversarial attacks. Mitigating these dangers necessitates a\nmechanism to formally guarantee that an agent's actions adhere to predefined\nsafety constraints, a challenge that existing systems do not fully address. We\nintroduce VeriGuard, a novel framework that provides formal safety guarantees\nfor LLM-based agents through a dual-stage architecture designed for robust and\nverifiable correctness. The initial offline stage involves a comprehensive\nvalidation process. It begins by clarifying user intent to establish precise\nsafety specifications. VeriGuard then synthesizes a behavioral policy and\nsubjects it to both testing and formal verification to prove its compliance\nwith these specifications. This iterative process refines the policy until it\nis deemed correct. Subsequently, the second stage provides online action\nmonitoring, where VeriGuard operates as a runtime monitor to validate each\nproposed agent action against the pre-verified policy before execution. This\nseparation of the exhaustive offline validation from the lightweight online\nmonitoring allows formal guarantees to be practically applied, providing a\nrobust safeguard that substantially improves the trustworthiness of LLM agents.", "AI": {"tldr": "VeriGuard is a framework providing formal safety guarantees for LLM-based agents through offline policy validation and online action monitoring.", "motivation": "Autonomous AI agents in sensitive domains like healthcare pose critical safety, security, and privacy risks, including deviation from objectives, policy violations, and adversarial attacks that existing systems don't fully address.", "method": "Dual-stage architecture: 1) Offline stage with user intent clarification, safety specification establishment, policy synthesis, and formal verification; 2) Online stage with runtime monitoring of actions against pre-verified policies.", "result": "The framework enables formal safety guarantees to be practically applied, separating exhaustive offline validation from lightweight online monitoring.", "conclusion": "VeriGuard substantially improves the trustworthiness of LLM agents by providing robust safeguards through verifiable correctness guarantees."}}
{"id": "2510.05365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05365", "abs": "https://arxiv.org/abs/2510.05365", "authors": ["Irtaza Sajid Qureshi", "Zhen Ming", "Jiang"], "title": "Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to automated software\ntesting, yet their ability to generalize beyond memorized patterns and reason\nabout natural language bug reports remains unclear. We present a systematic\nevaluation of LLM reasoning in test case generation, structured around the\ncognitive layers of Bloom's taxonomy: \\textit{Remember}, \\textit{Understand},\n\\textit{Apply}, \\textit{Analyze}, \\textit{Evaluate}, and \\textit{Create}, which\nprogressively assess higher levels of cognitive and reasoning capabilities.\nBuilding on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,\nGHRB, and mutated variants that introduce linguistic and semantic challenges.\nOur findings show that both models largely reproduce prior results with minor\ndeviations (\\textit{Remember}), exhibit partial robustness to linguistic\nrephrasings and translations while uncovering unique reproducible bugs\n(\\textit{Understand}), but suffer severe performance drops exceeding 60\\% under\nidentifier mutations (\\textit{Apply}). Conversely, providing near-identical\nfew-shot examples in an open-book setting improves success rates by up to three\ntimes, and component-level analysis reveals that structured technical elements,\nsuch as test code and method names, are far more impactful than narrative\ndescriptions for successful test generation (\\textit{Analyze}). These insights\nilluminate the cognitive processes underlying LLM-generated tests, suggest\nconcrete directions for improving performance, and establish a robust and\nrealistic evaluation paradigm for this task.", "AI": {"tldr": "LLMs show limited reasoning in software testing - they reproduce patterns but fail with semantic changes, though structured technical elements help more than narrative descriptions.", "motivation": "To systematically evaluate LLM reasoning capabilities in test case generation beyond memorization, using Bloom's taxonomy to assess cognitive layers.", "method": "Evaluated StarCoder and GPT-4o on Defects4J, GHRB, and mutated datasets with linguistic/semantic challenges, using LIBRO framework and Bloom's taxonomy cognitive layers.", "result": "Models reproduce prior results (Remember) but suffer 60%+ performance drops with identifier mutations (Apply). Few-shot examples triple success rates, and technical elements are more impactful than narratives.", "conclusion": "LLMs have limited reasoning in test generation, but structured evaluation reveals cognitive processes and suggests performance improvement directions."}}
{"id": "2510.05390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05390", "abs": "https://arxiv.org/abs/2510.05390", "authors": ["Felicity Anderson", "Julien Sindt", "Neil Chue Hong"], "title": "Who Do You Think You Are? Creating RSE Personas from GitHub Interactions", "comment": "36 pages. Invited extended paper of original poster at deRSE2025. To\n  be published in ECEASST", "summary": "We describe data-driven RSE personas: an approach combining software\nrepository mining and data-driven personas applied to research software (RS),\nan attempt to describe and identify common and rare patterns of Research\nSoftware Engineering (RSE) development. This allows individuals and RS project\nteams to understand their contributions, impact and repository dynamics - an\nimportant foundation for improving RSE. We evaluate the method on different\npatterns of collaborative interaction behaviours by contributors to mid-sized\npublic RS repositories (those with 10-300 committers) on GitHub. We demonstrate\nhow the RSE personas method successfully characterises a sample of 115,174\nrepository contributors across 1,284 RS repositories on GitHub, sampled from\n42,284 candidate software repository records queried from Zenodo. We identify,\nname and summarise seven distinct personas from low to high interactivity:\nEphemeral Contributor; Occasional Contributor; Project Organiser; Moderate\nContributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.\nThis demonstrates that large datasets can be analysed despite difficulties of\ncomparing software projects with different project management factors, research\ndomains and contributor backgrounds.", "AI": {"tldr": "The paper introduces data-driven RSE personas to identify common patterns in Research Software Engineering development using repository mining, applied to 115,174 contributors across 1,284 GitHub repositories.", "motivation": "To help individuals and research software project teams understand their contributions, impact, and repository dynamics, which is important for improving Research Software Engineering practices.", "method": "Combines software repository mining and data-driven personas applied to research software repositories on GitHub, analyzing collaborative interaction behaviors of contributors to mid-sized public repositories (10-300 committers).", "result": "Identified and characterized seven distinct RSE personas from low to high interactivity: Ephemeral Contributor, Occasional Contributor, Project Organiser, Moderate Contributor, Low-Process Closer, Low-Coding Closer, and Active Contributor.", "conclusion": "The method successfully analyzes large datasets despite challenges in comparing software projects with different management factors, research domains, and contributor backgrounds, demonstrating its effectiveness for characterizing RSE development patterns."}}
{"id": "2510.05441", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05441", "abs": "https://arxiv.org/abs/2510.05441", "authors": ["Yiannis Charalambous", "Claudionor N. Coelho Jr", "Luis Lamb", "Lucas C. Cordeiro"], "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification", "comment": null, "summary": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent\nsystem designed to generate unit tests for legacy code, enhancing test coverage\nand critical value testing. UnitTenX leverages a combination of AI agents,\nformal methods, and Large Language Models (LLMs) to automate test generation,\naddressing the challenges posed by complex and legacy codebases. Despite the\nlimitations of LLMs in bug detection, UnitTenX offers a robust framework for\nimproving software reliability and maintainability. Our results demonstrate the\neffectiveness of this approach in generating high-quality tests and identifying\npotential issues. Additionally, our approach enhances the readability and\ndocumentation of legacy code.", "AI": {"tldr": "UnitTenX is an open-source AI multi-agent system that automatically generates unit tests for legacy code using AI agents, formal methods, and LLMs to improve test coverage and critical value testing.", "motivation": "Address challenges of complex and legacy codebases where manual test generation is difficult, and improve software reliability and maintainability despite LLM limitations in bug detection.", "method": "Leverages combination of AI agents, formal methods, and Large Language Models (LLMs) to automate test generation for legacy code.", "result": "Effective in generating high-quality tests, identifying potential issues, and enhancing readability and documentation of legacy code.", "conclusion": "UnitTenX provides a robust framework for improving software reliability and maintainability through automated test generation for legacy systems."}}
{"id": "2510.05450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05450", "abs": "https://arxiv.org/abs/2510.05450", "authors": ["Saul Goldman", "Hong Yi Lin", "Jirat Pasuksmit", "Patanamon Thongtanunam", "Kla Tantithamthavorn", "Zhe Wang", "Ray Zhang", "Ali Behnaz", "Fan Jiang", "Michael Siers", "Ryan Jiang", "Mike Buller", "Minwoo Jeong", "Ming Wu"], "title": "What Types of Code Review Comments Do Developers Most Frequently Resolve?", "comment": "The paper has been accepted the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "Large language model (LLM)-powered code review automation tools have been\nintroduced to generate code review comments. However, not all generated\ncomments will drive code changes. Understanding what types of generated review\ncomments are likely to trigger code changes is crucial for identifying those\nthat are actionable. In this paper, we set out to investigate (1) the types of\nreview comments written by humans and LLMs, and (2) the types of generated\ncomments that are most frequently resolved by developers. To do so, we\ndeveloped an LLM-as-a-Judge to automatically classify review comments based on\nour own taxonomy of five categories. Our empirical study confirms that (1) the\nLLM reviewer and human reviewers exhibit distinct strengths and weaknesses\ndepending on the project context, and (2) readability, bugs, and\nmaintainability-related comments had higher resolution rates than those focused\non code design. These results suggest that a substantial proportion of\nLLM-generated comments are actionable and can be resolved by developers. Our\nwork highlights the complementarity between LLM and human reviewers and offers\nsuggestions to improve the practical effectiveness of LLM-powered code review\ntools.", "AI": {"tldr": "LLM-generated code review comments are often actionable, with readability, bug, and maintainability comments having higher resolution rates than design-focused comments. LLM and human reviewers show complementary strengths.", "motivation": "To understand which types of LLM-generated code review comments are most likely to trigger actual code changes by developers, and to compare the effectiveness of LLM vs human reviewers.", "method": "Developed an LLM-as-a-Judge system to automatically classify review comments into five categories based on a custom taxonomy, then analyzed resolution rates across comment types.", "result": "Readability, bugs, and maintainability-related comments had higher resolution rates than code design comments. LLM and human reviewers exhibit distinct strengths depending on project context.", "conclusion": "A substantial proportion of LLM-generated comments are actionable and can be resolved by developers. The work highlights complementarity between LLM and human reviewers and provides suggestions for improving LLM-powered code review tools."}}
{"id": "2510.05604", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05604", "abs": "https://arxiv.org/abs/2510.05604", "authors": ["Rintaro Kanaji", "Brittany Reid", "Yutaro Kashiwa", "Raula Gaikovina Kula", "Hajimu Iida"], "title": "An Empirical Study of Security-Policy Related Issues in Open Source Projects", "comment": "Accepted in PROFES 2025", "summary": "GitHub recommends that projects adopt a SECURITY.md file that outlines\nvulnerability reporting procedures. However, the effectiveness and operational\nchallenges of such files are not yet fully understood. This study aims to\nclarify the challenges that SECURITY.md files face in the vulnerability\nreporting process within open-source communities. Specifically, we classified\nand analyzed the content of 711 randomly sampled issues related to SECURITY.md.\nWe also conducted a quantitative comparative analysis of the close time and\nnumber of responses for issues concerning six community health files, including\nSECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues\nwere requests to add the file, and reports that included links were closed,\nwith a median time that was 2 days shorter. These findings offer practical\ninsights for improving security reporting policies and community management,\nultimately contributing to a more secure open-source ecosystem.", "AI": {"tldr": "Analysis of SECURITY.md files in open-source projects reveals that 79.5% of related issues are requests to add the file, and reports with links are resolved 2 days faster.", "motivation": "To understand the effectiveness and operational challenges of SECURITY.md files in vulnerability reporting within open-source communities.", "method": "Classified and analyzed content of 711 randomly sampled SECURITY.md-related issues, and conducted quantitative comparative analysis of close time and response counts for six community health files.", "result": "79.5% of SECURITY.md issues were requests to add the file; reports with links closed 2 days faster on median; SECURITY.md had shorter close times compared to other community files.", "conclusion": "Findings provide practical insights for improving security reporting policies and community management, contributing to a more secure open-source ecosystem."}}
{"id": "2510.05705", "categories": ["cs.SE", "cs.DL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.05705", "abs": "https://arxiv.org/abs/2510.05705", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment", "comment": null, "summary": "In the ever-changing realm of research software development, it is crucial\nfor the scientific community to grasp current trends to identify gaps that can\npotentially hinder scientific progress. The adherence to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles can serve as a proxy to\nunderstand those trends and provide a mechanism to propose specific actions.\n  The Software Observatory at OpenEBench\n(https://openebench.bsc.es/observatory) is a novel web portal that consolidates\nsoftware metadata from various sources, offering comprehensive insights into\ncritical research software aspects. Our platform enables users to analyse\ntrends, identify patterns and advancements within the Life Sciences research\nsoftware ecosystem, and understand its evolution over time. It also evaluates\nresearch software according to FAIR principles for research software, providing\nscores for different indicators.\n  Users have the ability to visualise this metadata at different levels of\ngranularity, ranging from the entire software landscape to specific communities\nto individual software entries through the FAIRsoft Evaluator. Indeed, the\nFAIRsoft Evaluator component streamlines the assessment process, helping\ndevelopers efficiently evaluate and obtain guidance to improve their software's\nFAIRness.\n  The Software Observatory represents a valuable resource for researchers and\nsoftware developers, as well as stakeholders, promoting better software\ndevelopment practices and adherence to FAIR principles for research software.", "AI": {"tldr": "The Software Observatory at OpenEBench is a web portal that aggregates software metadata to analyze trends in Life Sciences research software and evaluate adherence to FAIR principles.", "motivation": "To help the scientific community understand current trends in research software development and identify gaps that could hinder scientific progress by using FAIR principles as a proxy.", "method": "Developed a web portal that consolidates software metadata from various sources and includes a FAIRsoft Evaluator component to assess software according to FAIR principles for research software.", "result": "The platform enables analysis of trends, identification of patterns and advancements in Life Sciences research software ecosystem, and provides FAIRness scores for different indicators at various granularity levels.", "conclusion": "The Software Observatory serves as a valuable resource for researchers, developers, and stakeholders to promote better software development practices and adherence to FAIR principles."}}
{"id": "2510.05768", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05768", "abs": "https://arxiv.org/abs/2510.05768", "authors": ["Robin Kimmel", "Judith Michael", "Andreas Wortmann", "Jingxi Zhang"], "title": "Digital Twins for Software Engineering Processes", "comment": null, "summary": "Digital twins promise a better understanding and use of complex systems. To\nthis end, they represent these systems at their runtime and may interact with\nthem to control their processes. Software engineering is a wicked challenge in\nwhich stakeholders from many domains collaborate to produce software artifacts\ntogether. In the presence of skilled software engineer shortage, our vision is\nto leverage DTs as means for better rep- resenting, understanding, and\noptimizing software engineering processes to (i) enable software experts making\nthe best use of their time and (ii) support domain experts in producing\nhigh-quality software. This paper outlines why this would be beneficial, what\nsuch a digital twin could look like, and what is missing for realizing and\ndeploying software engineering digital twins.", "AI": {"tldr": "Digital twins can improve software engineering processes by better representing, understanding, and optimizing them to help software experts use their time efficiently and support domain experts in producing high-quality software.", "motivation": "Software engineering is complex with stakeholders from many domains collaborating, and there's a shortage of skilled software engineers. Digital twins can help address these challenges by providing better process representation and optimization.", "method": "The paper outlines what a software engineering digital twin could look like and discusses the requirements for realizing and deploying such digital twins.", "result": "The paper presents a vision for how digital twins could benefit software engineering processes but does not report on implemented results since it's outlining the concept and requirements.", "conclusion": "Digital twins show promise for improving software engineering processes, but there are missing components and challenges that need to be addressed for their successful realization and deployment."}}
{"id": "2510.05788", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05788", "abs": "https://arxiv.org/abs/2510.05788", "authors": ["Nikita Pavlichenko", "Iurii Nazarov", "Ivan Dolgov", "Ekaterina Garanina", "Dmitry Ustalov", "Ivan Bondyrev", "Kseniia Lysaniuk", "Evgeniia Vu", "Kirill Chekmenev", "Joseph Shtok", "Yaroslav Golubev", "Anton Semenkin", "Uladzislau Sazanovich"], "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding", "comment": "11 pages, 4 figures, 3 tables", "summary": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.", "AI": {"tldr": "Mellum models are 4B-parameter code completion models designed for JetBrains IDEs, featuring careful data curation, staged training, and editor-critical capabilities to meet interactive use requirements.", "motivation": "To create high-quality, cost-effective code completion models that can operate within the latency constraints of interactive IDE use while maintaining production quality for hundreds of thousands of users.", "method": "End-to-end industrial pipeline including disciplined data governance, multi-stage training (pre-training on 4T tokens, fill-in-the-middle, project context via supervised fine-tuning), and alignment via direct preference optimization using real-world feedback.", "result": "Mellum models demonstrate that careful data curation and staged training significantly improve quality, and compact task-focused models can meet cost/latency constraints while providing high-quality suggestions in production environments.", "conclusion": "The Mellum project provides a pragmatic blueprint for taking focused, open models from research prototype to large-scale production deployment, with models released under Apache-2.0 license for community use."}}
{"id": "2510.05878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05878", "abs": "https://arxiv.org/abs/2510.05878", "authors": ["Darja Smite", "Franz Zieris", "Lars-Ola Damm"], "title": "A Wave of Resignations in the Aftermath of Remote Onboarding", "comment": "9 pages, submitted to the Journal of Systems and Software, In\n  Practice track", "summary": "The COVID-19 pandemic has permanently altered workplace structures,\nnormalizing remote work. However, critical evidence highlights challenges with\nfully remote arrangements, particularly for software teams. This study\ninvestigates employee resignation patterns at Ericsson, a global developer of\nsoftware-intensive systems, before, during, and after the pandemic. Using HR\ndata from 2016-2025 in Ericsson Sweden, we analyze how different work\nmodalities (onsite, remote, and hybrid) influence employee retention. Our\nfindings show a marked increase in resignations from summer 2021 to summer\n2023, especially among employees with less than five years of tenure. Employees\nonboarded remotely during the pandemic were significantly more likely to resign\nwithin their first three years, even after returning to the office. Exit\nsurveys suggest that remote onboarding may fail to establish the necessary\norganizational attachment, the feeling of belonging and long-term retention. By\ncontrast, the company's eventual successful return to pre-pandemic retention\nrates illustrates the value of differentiated work policies and supports\nreconsidering selective return-to-office (RTO) mandates. Our study demonstrates\nthe importance of employee integration practices in hybrid environments where\nthe requirement for in-office presence for recent hires shall be accompanied by\nin-office presence from their team members and more senior staff whose\nmentoring and social interactions contribute to integration into the corporate\nwork environment. We hope these actionable insights will inform HR leaders and\npolicymakers in shaping post-pandemic work practices, demonstrating that\ncarefully crafted hybrid models anchored in organizational attachment and\nmentorship can sustain retention in knowledge-intensive companies.", "AI": {"tldr": "Study at Ericsson shows remote onboarding during COVID-19 led to higher resignation rates, especially for employees with <5 years tenure. Hybrid work with in-office presence for new hires and mentors improves retention.", "motivation": "COVID-19 normalized remote work but created challenges for software teams. Need to understand how different work modalities (onsite, remote, hybrid) affect employee retention patterns.", "method": "Analyzed HR data from Ericsson Sweden (2016-2025) to examine resignation patterns before, during, and after pandemic. Studied work modalities and used exit surveys to understand organizational attachment.", "result": "Significant increase in resignations from 2021-2023, especially among employees with <5 years tenure. Remote-onboarded employees were more likely to resign within first 3 years, even after returning to office. Exit surveys indicated remote onboarding failed to establish organizational attachment.", "conclusion": "Hybrid models with selective in-office requirements for new hires, supported by team members and senior staff mentorship, can sustain retention. Organizational attachment and integration practices are crucial in hybrid environments."}}
{"id": "2510.05968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05968", "abs": "https://arxiv.org/abs/2510.05968", "authors": ["Scott Frees"], "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications", "comment": null, "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.", "AI": {"tldr": "Patterns for building LLM-powered reporting systems that decouple query generation from data retrieval using dual-response patterns with ResourceLink.", "motivation": "Context window limitations prevent direct deployment of LLMs in reporting systems where complete datasets exceed available tokens, and practical implementation patterns for scalable architectures are undocumented.", "method": "Introduces dual-response pattern extending ResourceLink to support iterative query refinement and out-of-band data access, with patterns for multi-tenant security and resource lifecycle management.", "result": "Provides practical patterns that address fundamental challenges in LLM-driven reporting applications.", "conclusion": "These patterns offer practical guidance for developers building scalable LLM-powered reporting systems that overcome context window limitations."}}
{"id": "2510.06000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06000", "abs": "https://arxiv.org/abs/2510.06000", "authors": ["Daniel Otten", "Trevor Stalnaker", "Nathan Wintersgill", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools", "comment": null, "summary": "The integration of generative artificial intelligence (GenAI) tools has\nfundamentally transformed software development. Although prompt engineering has\nemerged as a critical skill, existing research focuses primarily on individual\ntechniques rather than software developers' broader workflows. This study\npresents a systematic investigation of how software engineers integrate GenAI\ntools into their professional practice through a large-scale survey examining\nprompting strategies, conversation patterns, and reliability assessments across\nvarious software engineering tasks.\n  We surveyed 91 software engineers, including 72 active GenAI users, to\nunderstand AI usage patterns throughout the development process. Our 14 key\nfindings show that while code generation is nearly universal, proficiency\nstrongly correlates with using AI for more nuanced tasks such as debugging and\ncode review, and that developers prefer iterative multi-turn conversations to\nsingle-shot prompting. Documentation tasks are perceived as most reliable,\nwhile complex code generation and debugging present sizable challenges. Our\ninsights provide an empirical baseline of current developer practices, from\nsimple code generation to deeper workflow integration, with actionable insights\nfor future improvements.", "AI": {"tldr": "A systematic study of how software engineers integrate GenAI tools into their workflows, revealing that while code generation is universal, proficiency correlates with using AI for nuanced tasks like debugging and code review.", "motivation": "Existing research focuses on individual prompt engineering techniques rather than software developers' broader workflows with GenAI tools.", "method": "Large-scale survey of 91 software engineers (72 active GenAI users) examining prompting strategies, conversation patterns, and reliability assessments across various software engineering tasks.", "result": "14 key findings show code generation is nearly universal, proficiency correlates with using AI for nuanced tasks, developers prefer iterative multi-turn conversations, documentation tasks are most reliable, while complex code generation and debugging present challenges.", "conclusion": "Provides an empirical baseline of current developer practices from simple code generation to deeper workflow integration, with actionable insights for future improvements."}}
{"id": "2510.06104", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06104", "abs": "https://arxiv.org/abs/2510.06104", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations", "comment": null, "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates", "AI": {"tldr": "LLMs can translate fault-prediction metrics into human-readable risk explanations to help OSS contributors make safer code changes.", "motivation": "OSS contributors face challenges interpreting static analysis metrics when making code changes, especially in complex object-oriented systems with interdependencies.", "method": "Investigate using LLMs to translate fault-prediction metrics into three types of explanations: descriptive, contextual, and actionable guidance.", "result": "The paper outlines explanation types and proposes a task-based study to assess usefulness compared to metric-only baselines.", "conclusion": "LLMs show promise for improving OSS contribution safety by providing clear risk explanations and actionable guidance for code modifications."}}
{"id": "2510.06187", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06187", "abs": "https://arxiv.org/abs/2510.06187", "authors": ["Griffin Pitts", "Aum Pandya", "Darsh Rank", "Tirth Bhatt", "Muntasir Hoq", "Bita Akram"], "title": "Automated Program Repair of Uncompilable Student Code", "comment": null, "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.", "AI": {"tldr": "LLMs can fix uncompilable student code while preserving structural intent, enabling better student modeling in CS1 courses.", "motivation": "Many student programming submissions are uncompilable and excluded from analysis, limiting understanding of student learning processes.", "method": "Used LLMs (GPT-5, Claude 3.5, Gemini 2.5) with high/low-context prompting to repair uncompilable code, evaluating compilability, edit distance, and structural preservation.", "result": "All LLMs produced compilable repairs but differed in preserving students' control flow and code structure, affecting pedagogical utility.", "conclusion": "Automated program repair enables richer analysis of learners' coding processes by recovering uncompilable submissions for student modeling."}}
