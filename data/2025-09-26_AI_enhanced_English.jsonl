{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM introduces two fine-tuned LLMs that generate expert OpenACC directives for data-parallel loops, achieving 87% correct directive type and 50% exact pragma generation on test data.", "motivation": "GPU programming complexity remains high despite directive-based standards like OpenACC, requiring significant expertise for effective directive usage.", "method": "Fine-tuned large language models on a supervised dataset of 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories.", "result": "Fine-tuned models generate valid pragmas with correct directive type for 87% of loops and exact pragmas for 50% of cases, significantly outperforming base LLMs.", "conclusion": "ACCeLLiuM lowers GPU programming barriers by automating directive generation and establishes a reproducible benchmark for LLM-powered OpenACC pragma generation."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "This paper presents a systematic review of software security visualization techniques, categorizing them into four types (graph-based, notation-based, matrix-based, metaphor-based) and analyzing over 60 recent research papers to identify key issues, advancements, and future directions.", "motivation": "Traditional text-based and numerical methods for analyzing security concerns are becoming ineffective as software systems grow more complex and threat landscapes evolve, necessitating better visualization techniques to transform complex security data into digestible formats.", "method": "Systematic literature review of over 60 recent research papers in software security visualization, creating a comprehensive taxonomy and categorizing techniques into four main types.", "result": "Identified two main areas: extensive software development visualization (focusing on software architecture depiction) and operational/cybersecurity visualization. The review highlights key issues, recent advancements, and the need for adaptive visualization techniques.", "conclusion": "There is a critical need for innovative visualization techniques that can adapt to evolving security landscapes, with practical implications for enhancing threat detection, improving security response strategies, and guiding future research directions."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct enables ReAct agents to efficiently handle large MCP tool sets by addressing tool selection challenges when dealing with hundreds/thousands of tools that exceed LLM memory limits.", "motivation": "Traditional approaches struggle with extensive tool sets where loading all tools simultaneously is computationally infeasible, limiting the scalability of AI agents in diverse environments.", "method": "Proposes five distinct architectures that progressively refine tool selection, culminating in a search-and-load mechanism for intelligent tool selection with minimal computational overhead.", "result": "Experimental results show up to 50% reduction in tool loading while maintaining task completion accuracy.", "conclusion": "The approach advances towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments by solving the tool selection scalability problem."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "The paper proposes a knowledge graph-based framework to address discrimination in software systems by formalizing fairness requirements specification and verification, moving beyond traditional focus on algorithmic flaws and biased data.", "motivation": "Discrimination in software systems is often attributed to algorithmic design flaws or biased data, but the paper argues that the root cause is actually the lack of well-specified fairness requirements and verification mechanisms. Experts' implicit knowledge about fairness makes it difficult to specify precise requirements.", "method": "The authors propose developing a knowledge graph-based framework for fairness, inspired by successful applications in security engineering. Knowledge graphs can formalize domain knowledge to assist in requirements specification and verification.", "result": "The paper presents a research roadmap discussing the challenges and research questions involved in creating this framework, but does not yet present empirical results from implementation.", "conclusion": "A knowledge graph-based approach can provide formal mechanisms for specifying and verifying fairness requirements, addressing a critical gap in current fairness engineering practices that focuses too narrowly on algorithms and data."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "Online-Optimized RAG addresses embedding misalignment in retrieval-augmented generation systems by continuously adapting retrieval embeddings using live interactions and minimal feedback, improving tool selection accuracy and task success.", "motivation": "Embedding misalignment in RAG systems due to imperfect embedding models or noisy descriptions leads to incorrect retrieval and task failure, which needs to be addressed for robust tool use.", "method": "A deployment-time framework that applies lightweight online gradient updates to adapt retrieval embeddings from live interactions using minimal feedback (e.g., task success), requiring no changes to the underlying LLM and supporting various retrieval scenarios.", "result": "The method consistently improves tool selection accuracy and end-task success across diverse tool-use and document-retrieval scenarios with negligible per-query latency.", "conclusion": "Online-Optimized RAG provides a simple, practical path to robust, self-improving RAG systems that can handle embedding misalignment through continuous online optimization."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "Stipula is a domain-specific language for legal contracts with enforceable properties. The paper presents a methodology to verify Stipula contracts by translating them into Java code with JML specifications and using KeY for automatic verification.", "motivation": "To ensure the correctness of legal contracts modeled in Stipula, especially those involving asset transfers and obligations, by providing formal verification methods.", "method": "Translate Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications, and use the KeY deductive verification tool for automatic verification of partial and total correctness.", "result": "The translation and verification process is fully automatic for a large subset of Stipula contracts (those with disjoint cycles), demonstrating that general-purpose deductive verification tools can be effectively used in this context.", "conclusion": "The approach successfully verifies Stipula contracts, showing that translation-based verification using tools like KeY is feasible and effective for ensuring contract correctness."}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "SpecDetect4AI is a tool-based approach for detecting AI-specific code smells in AI-based systems using a declarative DSL and static analysis, achieving high precision and recall on large codebases.", "motivation": "AI-based systems introduce new software issues that existing detection tools miss, particularly AI-specific code smells that can lead to problems like unreproducibility, silent failures, or poor model generalization.", "method": "Combines a high-level declarative Domain-Specific Language (DSL) for rule specification with an extensible static analysis tool that interprets and detects these rules for AI-based systems. Specified 22 AI-specific code smells.", "result": "Evaluated on 826 AI-based systems (20M lines of code), achieving precision of 88.66% and recall of 88.89%, outperforming existing detection tools. Demonstrated efficiency and extensibility with SUS score of 81.7/100.", "conclusion": "SpecDetect4AI effectively supports specification and detection of AI-specific code smells through dedicated rules and can analyze large AI-based systems efficiently."}}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems.", "AI": {"tldr": "First large-scale empirical study of LLM-specific technical debt (SATD) in software integrations, revealing prompt design as the primary source of debt across major LLM APIs.", "motivation": "LLMs are increasingly embedded in software via APIs, but these integrations create their own form of technical debt that needs systematic study and understanding.", "method": "Analyzed 93,142 Python files across major LLM APIs to identify SATD origins, prevalence, and mitigation strategies, with focus on OpenAI and LangChain integrations.", "result": "54.49% of SATD instances stem from OpenAI integrations; prompt design is the primary source (6.61% of debt), with instruction-based prompts (38.60%) and few-shot prompts (18.13%) being most vulnerable due to instruction clarity and example quality dependencies.", "conclusion": "The study provides the first comprehensive analysis of LLM-specific technical debt, releases a SATD dataset for reproducibility, and offers practical guidance for managing technical debt in LLM-powered systems."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "AI-Python chatbot for programming education that combines static code analysis, dynamic execution tracing, and LLMs to help students debug errors and learn programming concepts, achieving 85% error resolution success rate.", "motivation": "Traditional IDEs and static analyzers lack interactive guidance, while AI code assistants like GitHub Copilot focus on code completion rather than learning. This research aims to bridge the gap by creating an educational AI tool that prioritizes learning over mere code generation.", "method": "Hybrid architecture using CodeLlama for code embedding, GPT-4 for natural language interactions, and Docker-based sandboxing for secure execution. Combines static code analysis with dynamic execution tracing to provide practical advice.", "result": "85% error resolution success rate (outperforming pylint's 62% and GPT-4's 73%), 59.3% reduction in debugging time, 34% improvement in coding proficiency, with particular gains in recursion and exception handling. Positive qualitative feedback from 120 students.", "conclusion": "The chatbot successfully demonstrates how AI can augment programming education by balancing technical innovation with pedagogical empathy, fostering deeper conceptual understanding and prioritizing educational equity over code completion."}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "FaR-Loc is a novel framework that enhances method-level fault localization by integrating LLMs with retrieval-augmented generation (RAG), outperforming state-of-the-art baselines on Defects4J benchmark.", "motivation": "Current LLM-based fault localization methods struggle with complex systems due to lack of project-specific knowledge and difficulty navigating large projects.", "method": "Three-component framework: LLM Functionality Extraction generates natural language descriptions of failing behavior, Semantic Dense Retrieval embeds descriptions and methods in shared semantic space, and LLM Re-ranking reorders methods based on contextual relevance.", "result": "FaR-Loc outperforms SoapFL and AutoFL by 14.6% and 9.1% in Top-1 accuracy, and 19.2% and 22.1% in Top-5 accuracy respectively. Pre-trained code embedding models with structure improve performance by up to 49.0% in Top-1 accuracy.", "conclusion": "The framework effectively addresses limitations of existing LLM-based fault localization methods and shows significant improvements without requiring re-training."}}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering.", "AI": {"tldr": "A novel programming language topic classification workflow using multi-label SVM with sliding window and voting strategy to localize language concepts in source code, achieving high accuracy on IBM Project CodeNet dataset.", "motivation": "As software systems grow in scale and complexity, understanding programming language topic distribution in source code is crucial for technical decisions, onboarding, tooling, and education.", "method": "Combines multi-label Support Vector Machine (SVM) with sliding window and voting strategy for fine-grained localization of core language concepts like operator overloading, virtual functions, inheritance, and templates.", "result": "Trained on IBM Project CodeNet dataset, the model achieves average F1 score of 0.90 across topics and 0.75 in code-topic highlight.", "conclusion": "Provides empirical insights and a reusable pipeline for researchers and practitioners in code analysis and data-driven software engineering."}}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges.", "AI": {"tldr": "This study examines engagement patterns in hybrid meetings among software professionals, finding comparable engagement levels between onsite and remote participants, though remote participants show lower engagement in long meetings.", "motivation": "The shift to hybrid work post-COVID-19 has created new challenges in communication and collaboration, with remote participation potentially leading to isolation and decreased engagement among team members.", "method": "Researchers studied professionals from three software companies using multimodal measurements including self-reported questionnaires and physiological biometric devices during hybrid meetings over several weeks.", "result": "Regression analyses showed comparable engagement levels between onsite and remote participants, but remote participants had lower engagement in long meetings. Active roles correlated with higher engagement, while larger meetings and afternoon sessions were associated with lower engagement.", "conclusion": "The findings provide insights into engagement factors in hybrid meetings and offer improvement recommendations relevant for software teams and other knowledge-intensive organizations facing hybrid collaboration challenges."}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "Current synthetic data generation for code LLMs faces a verification ceiling where data quality is limited by synthetic verifiers. The paper shows that rigid 100% pass criteria are too restrictive, and that relaxed verification thresholds combined with diverse test suites can improve model performance by 2-4 points in pass@1.", "motivation": "To address the verification bottleneck in synthetic data generation for code LLMs, where the capabilities of synthetic verifiers fundamentally constrain training data quality and diversity.", "method": "Systematic study of verification design strategies: (i) analyzing test complexity vs quantity, (ii) exploring relaxed pass thresholds and LLM-based soft verification, (iii) comparing formally correct vs incorrect solutions with human evaluation.", "result": "Richer test suites improve code generation (+3 pass@1), relaxed thresholds recover valuable training data (2-4 point improvement), and retaining diverse correct solutions yields consistent generalization gains.", "conclusion": "Verification is necessary but too rigid in current practice. By combining calibrated verification with diverse problem-solution pairs, we can break the verification ceiling and develop stronger code generation models."}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "PseudoBridge introduces pseudo-code as an intermediate modality to bridge the semantic gap between natural language queries and programming language code, improving code retrieval accuracy and robustness to code style variations.", "motivation": "Existing PLM-based code search methods struggle with the fundamental semantic gap between human intent and machine execution logic, and have limited robustness to diverse code styles.", "method": "A two-stage framework: 1) Use LLM to synthesize pseudo-code for explicit NL-pseudo-code alignment, 2) Generate stylistically diverse but logically equivalent code implementations with pseudo-code alignment to enhance robustness.", "result": "PseudoBridge consistently outperforms baselines across 10 PLMs and 6 programming languages, achieving significant gains in retrieval accuracy and generalization, especially in zero-shot domain transfer scenarios.", "conclusion": "The explicit logical alignment via pseudo-code is effective, making PseudoBridge a robust and generalizable solution for code retrieval."}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "CodeHinter is an AI-assisted debugging tool that combines traditional debugging tools with LLM-based techniques to help novice programmers fix semantic errors while promoting active engagement in the debugging process.", "motivation": "Current AI-assisted debugging tools foster over-reliance on AI and don't actively engage students in the debugging process. The goal is to design an intuitive debugging assistant that helps novices while promoting their active participation.", "method": "CodeHinter combines traditional debugging tools with LLM-based techniques. The tool was tested with undergraduate students in its second design iteration, focusing on features like error localization.", "result": "Students found the tool highly effective in resolving semantic errors and significantly easier to use than the first version. Error localization was identified as the most valuable feature.", "conclusion": "AI-assisted debugging tools should be personalized based on user profiles to optimize interactions with students, rather than fostering passive reliance on AI."}}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed.", "AI": {"tldr": "This paper analyzes quantum software engineering challenges by classifying Stack Overflow questions using transformer models, achieving 95% accuracy with BERT-based approaches and providing model interpretability through SHAP analysis.", "motivation": "Quantum developers face challenges in optimizing quantum computing and need better ways to categorize and understand QSE challenges discussed on platforms like Stack Overflow, where current tags focus on technical aspects rather than developer challenges.", "method": "Extracted 2829 quantum-related questions from Q&A platforms, conducted content analysis and grounded theory to identify common challenges (Tooling, Theoretical, Learning, Conceptual, Errors, API Usage), validated annotations with ChatGPT, and compared fine-tuned transformer models (BERT, DistilBERT, RoBERTa) against traditional D&ML classifiers (FNN, CNN, LSTM).", "result": "Transformer-based models achieved 95% average accuracy, outperforming D&ML classifiers (89%, 86%, 84% for FNN, CNN, LSTM respectively) by 6% without data augmentation. SHAP analysis provided model interpretability for linguistic feature importance.", "conclusion": "The approach successfully classifies quantum developer challenges and can help vendors and forums better organize discussions. However, empirical evaluation with actual developers and vendors is needed for validation."}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR is a chain-of-thought fine-tuning approach that enhances LLMs' code review capabilities by training them to analyze multiple dimensions simultaneously using long COT techniques with Maximum Entropy modeling.", "motivation": "Current LLM-based code review methods are limited by vague fine-tuning information and cannot analyze multiple dimensions simultaneously like human reviewers, leading to suboptimal performance.", "method": "Proposes MelcotCR - combines chain-of-thought fine-tuning with Maximum Entropy modeling and pre-defined reasoning pathways to handle long COT prompts effectively, addressing context loss and reasoning logic issues.", "result": "A 14B Qwen2.5 model fine-tuned with MelcotCR surpasses state-of-the-art methods in code issue detection accuracy and performs comparably to the 671B DeepSeek-R1 model.", "conclusion": "MelcotCR enables smaller LLMs to achieve human-level multi-dimensional code review analysis through structured reasoning pathways, making high-quality automated code review more accessible."}}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy.", "AI": {"tldr": "This paper introduces an automated approach using BERTopic with seed words and LLM validation to classify citizen contributions on digital platforms like Brasil Participativo, enabling governments to process large volumes of input efficiently.", "motivation": "Governments face challenges in utilizing citizen engagement on digital platforms due to the sheer volume of contributions, which makes manual classification infeasible, requires expert involvement, and needs alignment with official taxonomies.", "method": "The approach combines BERTopic with seed words and automatic validation by large language models to generate coherent and institutionally aligned topics with minimal human effort.", "result": "Initial results show that the generated topics are coherent and institutionally aligned, successfully transforming large volumes of citizen input into actionable data.", "conclusion": "This methodology enables governments to effectively process citizen contributions for public policy purposes, overcoming scalability challenges through automated topic modeling and validation."}}
