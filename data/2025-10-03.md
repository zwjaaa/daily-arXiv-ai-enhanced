<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: PerfOrch is a multi-stage LLM orchestration framework that dynamically routes coding tasks to the most suitable LLMs based on empirical performance profiling, achieving higher correctness rates and runtime performance than single-model approaches.


<details>
  <summary>Details</summary>
Motivation: Current single-model approaches ignore the heterogeneous computational strengths that different LLMs exhibit across programming languages, algorithmic domains, and development stages, limiting their effectiveness in automated code generation.

Method: A multi-stage, performance-guided orchestration framework that uses empirical profiling of 17 LLMs across 5 programming languages, with dynamic routing through a generate-fix-refine workflow using stage-wise validation and rollback mechanisms.

Result: Achieved average correctness rates of 96.22% and 91.37% on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and 49.11%. Also improved execution time for 58.76% of problems with median speedups ranging from 17.67% to 27.66% across languages.

Conclusion: The framework offers a paradigm for production-grade automated software engineering that adapts to the rapidly evolving generative AI landscape through its plug-and-play architecture for seamless integration of new LLMs.

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: Study examines the prevalence and reasons behind GitHub's 'wontfix' label usage across 3,132 popular repositories, finding 30% of projects use it, primarily for bug reports and feature requests, with eight identified themes explaining the reasoning.


<details>
  <summary>Details</summary>
Motivation: The 'wontfix' label is widely used but poorly understood in GitHub repositories, and its impact on project management and community dynamics in open-source software development needs clarification.

Method: Mixed-method approach combining quantitative analysis of 'wontfix' label prevalence and qualitative analysis using open coding and thematic analysis to categorize reasons for using the label across 3,132 GitHub repositories.

Result: About 30% of GitHub projects use the 'wontfix' label, primarily on user-submitted bug reports and feature requests. Eight common themes were identified for labeling issues as wontfix, including user-specific factors and maintainer decisions.

Conclusion: The 'wontfix' label is crucial for resource management and guiding contributor efforts but can discourage community involvement and reduce transparency. Understanding these reasons helps project managers make informed decisions and foster efficient collaboration.

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC is a framework that integrates diverse personality traits into gaming agents to enable different gaming strategies for similar situations, improving test coverage and interaction diversity in game testing.


<details>
  <summary>Details</summary>
Motivation: Traditional automated testing agents in games often produce repetitive solutions because they don't account for diverse human player strategies based on different personalities, limiting their ability to trigger varied interactions and uncover edge cases.

Method: The MIMIC framework integrates diverse personality traits into gaming agents, allowing them to adopt different gaming strategies for similar situations by mimicking various human playstyles.

Result: MIMIC achieves higher test coverage and richer in-game interactions across different games, and outperforms state-of-the-art agents in Minecraft with higher task completion rates and more diverse solutions.

Conclusion: MIMIC demonstrates significant potential for effective game testing by enabling agents to mimic varied human gaming strategies, leading to more comprehensive testing outcomes.

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain is a blockchain-based web platform that automates OSS license compliance for derivative works, covering 14 licenses and showing promising results in initial evaluation.


<details>
  <summary>Details</summary>
Motivation: OSS license compliance is complex and license incompatibilities can lead to legal disputes. Blockchain technology offers transparency and immutable recording of software changes.

Method: Designed and implemented FOSS-chain, a web platform integrating blockchain with license management to automate license compliance process for derivative works.

Result: Preliminary evaluation through small-scale user study showed promising results, demonstrating potential for adaptation to realistic software systems.

Conclusion: Blockchain integration with license management can effectively address OSS license compatibility issues and automate compliance processes.

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA is a tool that integrates hardware-based energy measurement for Android apps directly into IDEs, making accurate energy consumption analysis more accessible and reproducible for developers and researchers.


<details>
  <summary>Details</summary>
Motivation: Hardware-based energy measurement for Android apps is time-consuming, difficult to adapt/reproduce, and lacks open-source tools, creating barriers for developers and researchers who need accurate energy consumption analysis.

Method: ARENA is implemented as an IntelliJ and Android Studio plugin that connects to physical measurement devices, executes test scenarios on Android apps, and automatically handles data collection, aggregation, statistical analysis, reporting, and visualization.

Result: The tool enables developers to compare energy consumption between different apps or versions directly within their IDE, providing reliable hardware-based measurements without complex setup processes.

Conclusion: ARENA successfully addresses the gap in open-source tools for hardware-based energy measurement by providing an integrated solution that makes accurate energy analysis accessible and reproducible during Android app development.

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair is the first non-autoregressive program repair model that addresses patch quality issues through three novel components: repair action predictor, inter-token dependency extractor, and two-stage decoder, achieving state-of-the-art performance in both speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive APR techniques suffer from huge time delays due to token-by-token generation, especially with large models. Non-autoregressive methods can generate code in parallel but compromise patch quality when applied naively.

Method: NARRepair uses three key components: 1) repair action predictor to prevent over-correction, 2) inter-token dependency extractor to capture token relationships, and 3) two-stage decoder to incorporate contextual information.

Result: NARRepair achieves best performance within limited repair time compared to other APR techniques, with 1.4-6.4x speed improvement over AR-based methods in GPU environments, while maintaining high accuracy.

Conclusion: NARRepair demonstrates state-of-the-art comprehensive performance in program repair, successfully balancing repair speed and accuracy through its customized non-autoregressive approach.

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter is a refactoring-aware tool that reduces false positives in semantic interference detection by filtering out behavior-preserving code refactorings, improving precision by 32% with minimal impact on recall.


<details>
  <summary>Details</summary>
Motivation: Current lightweight static analysis techniques for semantic interference detection suffer from high false positive rates, primarily due to inability to distinguish behavior-preserving refactorings from actual interfering changes.

Method: RefFilter builds on existing static techniques by incorporating automated refactoring detection to filter out behavior-preserving code changes from interference reports.

Result: Experimental evaluation shows RefFilter reduces false positives by nearly 32% on a labeled dataset of 99 scenarios, with only a non-significant increase in false negatives, resulting in significant overall precision improvement.

Conclusion: Refactoring-aware interference detection is a practical and effective strategy for improving merge support in modern development workflows, as the precision gains significantly outweigh minor recall trade-offs.

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST is a novel technique that refines unit tests to improve their semantic clarity for better in-context learning in LLM-based test generation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Poor quality in-context examples in LLM-based unit test generation lead to suboptimal outputs, so there's a need to systematically improve test clarity and structure.

Method: CLAST decomposes complex tests into logically clearer ones and improves semantic clarity through program analysis and LLM-based rewriting.

Result: CLAST fully preserves test effectiveness while UTgen reduces key metrics by 4-36%. Over 85% of users preferred CLAST's clarity. Using CLAST-refined tests improved test generation metrics by 26-46% compared to UTgen.

Conclusion: CLAST effectively enhances unit test semantic clarity and improves LLM-based test generation, showing significant practical impact and research potential.

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: The paper proposes using Model-Driven Engineering (MDE) to systematically derive reoptimization problems from original optimization specifications, addressing challenges in adapting solutions when contextual factors change.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reoptimization where solutions need adaptation when contextual factors change, while minimizing impact and handling constraints on changing past allocations.

Method: Uses Model-Driven Engineering (MDE) with declarative modeling languages and model transformations for high-level specification of optimization problems, implemented via GIPS tool for combinatorial reoptimization problems.

Result: Provides initial categorization of changing problems and strategies for deriving reoptimization specifications, with proof-of-concept implementation applied to teaching assistant allocation problem.

Conclusion: MDE offers systematic opportunities for deriving reoptimization problems from original specifications, enabling minimal-impact solution adaptations in changing contexts.

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: Introduction of a new ACM SIGSOFT SEN column (SEN-ESE) focused on meta-aspects of empirical software engineering research, aiming to address implicit knowledge gaps and improve research practices through community discussions.


<details>
  <summary>Details</summary>
Motivation: Empirical software engineering has evolved but faces challenges like research reproducibility, limited external validity, subjective reviews, and difficulty transferring results to industry. Many ESE research facets remain undocumented, making it hard for newcomers to learn.

Method: Establish a regular column featuring expert interviews, focus groups, surveys, and position pieces to discuss ESE meta-topics including replication packages, statistical methods, interview tools, and interdisciplinary publishing.

Result: Creation of SEN-ESE column as a dedicated venue for sparking conversations on implicit ESE topics and encouraging reflection on research practices.

Conclusion: The column aims to improve how ESE research is conducted, communicated, taught, and ultimately enhanced, while inviting community feedback on challenging or underexplored topics.

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: A multimodal system using ViViT and AST models with Tensor Fusion Network for detecting public transport fraud through CCTV and audio analysis, achieving 89.5% accuracy and significant improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address fraud and fare evasion in public transportation by leveraging multimodal data (CCTV and audio) for more accurate detection, reducing revenue loss and improving passenger safety.

Method: Uses Vision Transformer for Video (ViViT) for video feature extraction, Audio Spectrogram Transformer (AST) for audio analysis, and implements Tensor Fusion Network (TFN) architecture with 2-fold Cartesian product to model unimodal and bimodal interactions.

Result: Achieved 89.5% accuracy, 87.2% precision, and 84.0% recall in detecting fraudulent activities, significantly outperforming early fusion baselines and exceeding state-of-the-art systems. Ablation studies showed 7.0% F1 score improvement and 8.8% recall boost over traditional methods.

Conclusion: The multimodal tensor fusion approach enables effective real-time fraud detection in public transport, supporting operators in reducing revenue loss, improving safety, and ensuring operational compliance.

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE is a community-driven framework that replaces static dataset cards with machine-readable Confidence Cards providing verifiable statistical guarantees for code dataset quality.


<details>
  <summary>Details</summary>
Motivation: Current code datasets lack verifiable quality guarantees, static dataset cards are not auditable or statistically rigorous, and teams build isolated cleaning pipelines which fragments effort and raises costs.

Method: SIEVE framework turns per-property checks into Confidence Cards - machine-readable, verifiable certificates with anytime-valid statistical bounds.

Result: The paper presents a research plan to mature SIEVE, replacing narrative cards with anytime-verifiable certification.

Conclusion: This shift to verifiable certification is expected to lower quality-assurance costs and increase trust in code datasets.

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: The paper introduces TAIBOM, a framework extending SBOM principles to AI systems to address unique challenges in AI dependency management, integrity verification, and trust attestation.


<details>
  <summary>Details</summary>
Motivation: Current SBOM frameworks are inadequate for AI systems due to their dynamic, data-driven nature and complex dependencies across datasets, models, and software components, compounded by fragmented governance and lack of integrity tools.

Method: TAIBOM provides a structured dependency model for AI components, mechanisms for propagating integrity statements across AI pipelines, and a trust attestation process for verifying component provenance.

Result: TAIBOM demonstrates advantages over existing standards like SPDX and CycloneDX in supporting assurance, security, and compliance across AI workflows.

Conclusion: This work establishes foundations for trustworthy and verifiable AI systems through structured software transparency.

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: AI-driven strategies reduce false positives in automated fuzz driver generation by using constraint-based generation and context-based crash validation, cutting reported crashes by over 50%.


<details>
  <summary>Details</summary>
Motivation: Automatically generated fuzz drivers frequently cause false positive crashes, especially for functions with structured inputs and complex state requirements, which undermines trust in fuzzing systems like OSS-Fuzz-Gen.

Method: Two AI-driven strategies: 1) Constraint-based fuzz driver generation that proactively enforces input and state constraints, 2) Context-based crash validation that reactively analyzes function callers to determine crash feasibility from program entry points.

Result: Using 1,500 benchmark functions from OSS-Fuzz, the strategies reduced spurious crashes by up to 8% and cut reported crashes by more than half, demonstrating that frontier LLMs can serve as reliable program analysis agents.

Conclusion: The results highlight the promise and challenges of integrating AI into large-scale fuzzing pipelines, showing effective reduction of false positives while maintaining fuzzing effectiveness.

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>
