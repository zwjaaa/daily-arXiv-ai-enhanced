<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: CoRaCMG is a retrieval-augmented framework that enhances commit message generation by using similar diff-message pairs to guide LLMs, achieving significant performance improvements across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Commit messages are often low-quality and incomplete, limiting their usefulness. While LLMs show promise in automating commit message generation, their performance remains limited and needs enhancement.

Method: CoRaCMG uses a three-phase approach: (1) Retrieve similar diff-message pairs, (2) Augment by combining them with query diff into structured prompts, (3) Generate commit messages via LLMs. This helps LLMs learn project-specific terminologies and writing styles.

Result: CoRaCMG significantly boosts LLM performance across BLEU, Rouge-L, METEOR, and CIDEr metrics. DeepSeek-R1 achieved 76% BLEU and 71% CIDEr improvements with single example. GPT-4o showed 89% BLEU improvement. Performance plateaus after 3 examples.

Conclusion: The framework effectively enhances commit message generation by enabling LLMs to capture human-written message patterns from retrieved examples, with diminishing returns beyond 3 examples.

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [2] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: The paper proposes using sentiment analysis of developer prompts as a scalable method to evaluate developer satisfaction with conversational AI assistants, showing it can capture implicit feedback 13x more frequently than explicit ratings.


<details>
  <summary>Details</summary>
Motivation: Traditional user studies are unscalable for evaluating developer satisfaction with AI assistants, while large-scale quantitative signals like logs or ratings are often too shallow or sparse to be reliable.

Method: The authors analyzed industrial usage logs of 372 professional developers and applied sentiment analysis to developer prompts to identify implicit signals of user satisfaction.

Result: The approach identified satisfaction signals in ~8% of all interactions, which is more than 13 times higher than explicit user feedback rates, with reasonable accuracy using off-the-shelf sentiment analysis.

Conclusion: Sentiment analysis of developer prompts provides a practical, scalable approach to complement existing feedback channels for understanding developer experience with AI assistants at scale.

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [3] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: SC2Tools is a modular toolset for creating and working with large datasets, particularly for StarCraft 2 research, designed to simplify data collection and preprocessing for gaming and esports studies.


<details>
  <summary>Details</summary>
Motivation: To address the need for simplified scientific workloads and tooling in gaming and esports research, making it easier for less technically proficient researchers to engage in AI/ML applications in this domain.

Method: Developed a modular toolset called SC2Tools with multiple submodules for dataset creation and processing, including PyTorch and PyTorch Lightning APIs for easy data access.

Result: Created one of the largest StarCraft 2 tournament datasets to date, with tools that can also be used with other types of data beyond StarCraft 2.

Conclusion: Alleviating data collection and preprocessing burdens is essential for broader researcher engagement in gaming/esports research, and SC2Tools provides foundational work toward normalizing experiment workflows in StarCraft 2.

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [4] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: This paper explores the use of Landlock, a new Linux kernel security feature, to secure scientific applications by removing network access after MPI initialization but before processing user inputs.


<details>
  <summary>Details</summary>
Motivation: Science Gateways require network access during MPI startup but need to restrict access for security before handling user-supplied parameter files. Landlock provides a way to dynamically revoke resource access for running processes.

Method: The researchers modified three mature scientific codes (Einstein Toolkit, Octo-Tiger, FUKA) to implement Landlock restrictions and created a fully-functioning FUKA science gateway that uses Landlock instead of traditional user authentication for security.

Result: The study demonstrates successful implementation of Landlock in scientific applications, showing that it can effectively secure processes by removing unnecessary network access after required initialization phases.

Conclusion: Landlock proves to be a useful security feature for scientific applications, enabling dynamic access control that enhances security without compromising functionality, particularly for Science Gateways handling sensitive user inputs.

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [5] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval is a benchmark for evaluating LLMs on iterative code generation under stepwise requirement refinement, addressing the gap in existing static benchmarks by simulating real-world development workflows.


<details>
  <summary>Details</summary>
Motivation: Existing code generation benchmarks are static and single-turn, failing to capture the iterative nature of real-world software development with evolving requirements, limiting understanding of LLMs' practical utility.

Method: SR-Eval uses a multi-agent-based requirement generation method to simulate development processes and recover multi-round interactions from final requirements, with semantic-aware discriminative test cases for consistent evaluation across 443 multi-turn tasks.

Result: Evaluation of 11 LLMs shows iterative code generation remains highly challenging - best model achieves only 22.67% completion on function-level and 20.00% on repository-level tasks, with performance significantly influenced by prompting strategies.

Conclusion: Iterative code generation under stepwise requirement refinement is a difficult task for current LLMs, highlighting the need for advanced methods and better prompting strategies to support real-world development workflows.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [6] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: This paper investigates using LLM agents to execute natural language test cases for GUI applications, addressing challenges of unsoundness and inconsistency through guardrail mechanisms and specialized agents.


<details>
  <summary>Details</summary>
Motivation: Natural language test cases are emerging as an alternative to manual test scripts, but they face issues with unsoundness (false failures) and inconsistent execution outcomes when using LLM agents.

Method: Proposed algorithm with guardrail mechanisms and specialized agents that dynamically verify each test step execution. Introduced evaluation measures for LLM capabilities and execution consistency, and defined weak unsoundness concept.

Result: Experimental evaluation with 8 LLMs (3B-70B parameters) showed Meta Llama 3.1 70B achieves acceptable NL test execution capabilities with high consistency (above 3-sigma level).

Conclusion: LLM agents show potential for GUI testing but have current limitations; the proposed approach addresses key challenges and demonstrates feasibility with certain LLM models.

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [7] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper presents the first large-scale empirical study of testing practices in AI agent ecosystems, revealing that traditional testing methods dominate while novel agent-specific testing is rare, and identifies a critical blind spot in prompt testing.


<details>
  <summary>Details</summary>
Motivation: Foundation model-based AI agents face testing challenges due to non-determinism and non-reproducibility, but there's limited understanding of how developers verify internal correctness during development.

Method: Conducted empirical study analyzing 39 open-source agent frameworks and 439 agentic applications to identify testing patterns and map them to architectural components.

Result: Found that traditional testing patterns dominate (70% effort on deterministic components), while FM-based components receive minimal testing (<5%), with prompts being particularly neglected (1% of tests).

Conclusion: Framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore adoption barriers to build more robust AI agents.

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>
