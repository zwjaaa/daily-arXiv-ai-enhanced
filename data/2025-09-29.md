<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens improves issue localization in large codebases by extracting conceptual knowledge to create semantic concerns that guide LLMs, achieving significant performance gains over state-of-the-art tools.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches struggle with concern mixing (relevant logic buried in large functions) and concern scattering (related logic dispersed across files) in large-scale repositories, limiting their effectiveness in bug localization.

Method: RepoLens operates in two stages: offline stage extracts and enriches conceptual knowledge into a repository-wide knowledge base; online stage retrieves issue-specific terms, clusters and ranks concerns by relevance, and integrates them via prompt enhancements to guide LLMs.

Result: RepoLens improved three state-of-the-art tools (AgentLess, OpenHands, mini-SWE-agent) by average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization. It generalized across models with Hit@1 and Recall@10 gains up to 504% and 376% respectively.

Conclusion: RepoLens effectively addresses concern mixing and scattering in large repositories through conceptual knowledge abstraction, significantly enhancing LLM-based issue localization performance across different models and tools.

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [2] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: This paper proposes a multicultural research project to investigate challenges faced by women with software engineering backgrounds re-entering academia after career breaks, comparing them with industry opportunities and providing policy recommendations.


<details>
  <summary>Details</summary>
Motivation: Academia offers limited opportunities for women returning after career breaks compared to industry, which has supportive pathways like returnship programs. Career disruptions due to pregnancy, immigration, or lack of flexible work options create barriers for women seeking academic roles.

Method: A diverse multicultural research project conducted in multiple universities across different countries to capture location-specific challenges and policies through comparative analysis.

Result: The research aims to identify specific challenges women face when re-entering academic roles, understand institutional perspectives, and analyze existing policies across different countries.

Conclusion: The study will provide recommendations to support transparent hiring practices and better support systems for women returning to academic and research roles after career breaks.

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [3] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: First framework for automatically generating Excel tutorials from natural language descriptions, eliminating manual labor and reducing time costs to 1/20 of expert authoring.


<details>
  <summary>Details</summary>
Motivation: Excel's complexity creates demand for tutorials, but existing manual approaches are labor-intensive, require frequent updates, and prior work hasn't achieved full automation.

Method: Framework that instantiates tasks, uses Execution Agent to plan/execute solutions in Excel, collects intermediate artifacts, and transforms them into structured documents and video demonstrations.

Result: Improves task execution success rates by 8.5% over state-of-the-art baselines, generates tutorials with superior readability and instructional effectiveness approaching expert-authored materials.

Conclusion: Automated pipeline makes scalable, high-quality tutorial generation practical for the first time, eliminating manual labor and significantly reducing time costs.

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [4] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: A domain-specific framework for software analytics that enables querying, modeling, and integration of heterogeneous software repositories using multi-layered abstraction with domain-specific operators.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing heterogeneous software repositories by providing a unified approach for querying, modeling, and integration.

Method: Multi-layered abstraction mechanism consisting of domain-specific operators, demonstrated through a case study.

Result: The framework successfully enables integration and analysis of diverse software repositories through its domain-specific approach.

Conclusion: The proposed domain-specific framework with multi-layered abstraction and specialized operators provides an effective solution for software analytics across heterogeneous repositories.

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [5] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: AgentPack is a new corpus of 1.3M code edits co-authored by AI agents (Claude Code, OpenAI Codex, Cursor Agent) from public GitHub projects, showing that agent-generated commits are higher quality than traditional human commits for training code-editing models.


<details>
  <summary>Details</summary>
Motivation: Traditional commit-based training data for code editing models is noisy due to terse messages, commingled edits, and bot-generated commits. AI agents produce more focused, well-documented changes that are human-filtered through code reviews.

Method: Collected 1.3M code edits from public GitHub projects where AI agents (Claude Code, OpenAI Codex, Cursor Agent) co-authored changes. Developed identification and curation pipeline to filter and analyze these agent-generated commits.

Result: Models fine-tuned on AgentPack outperform models trained on prior human-only commit corpora. Agent-generated commits are more narrowly scoped with clearer goals and detailed rationale.

Conclusion: Public data from software engineering agents represents a high-quality training resource for future code-editing models, offering superior performance over traditional human-only commit data.

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [6] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: This paper challenges the conventional belief that accurate surrogate models are always better for configuration tuning, revealing that model accuracy can be misleading. It introduces a new theory for assessing model usefulness and proposes Model4Tune, an automated tool that predicts optimal model-tuner pairs for unseen systems.


<details>
  <summary>Details</summary>
Motivation: Prior work discovered that surrogate model accuracy can be misleading in configuration tuning, leaving unanswered questions about the model's true role. This paper aims to systematically explore what makes surrogate models useful for configuration tuning beyond just accuracy.

Method: The authors use fitness landscape analysis to assess model usefulness and conduct an extensive empirical study with up to 27,000 cases. They propose Model4Tune, an automated predictive tool that estimates optimal model-tuner pairs without expensive tuner profiling.

Result: Model4Tune performs significantly better than random guessing in 79%-82% of cases, demonstrating its effectiveness in predicting which model-tuner pairs work best for unforeseen systems.

Conclusion: The research provides new insights into surrogate model roles in configuration tuning and offers a practical tool (Model4Tune) that helps practitioners evaluate the most useful models, opening new research directions in this area.

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [7] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: SecureAgentBench is a benchmark for evaluating code agents' secure code generation capabilities, revealing current agents struggle with security despite functional correctness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook realistic vulnerability contexts and fail to comprehensively evaluate both functional correctness and security vulnerabilities in LLM-generated code.

Method: Created 105 coding tasks with realistic multi-file edits in large repositories, aligned with real-world vulnerabilities, and comprehensive evaluation combining functionality testing, vulnerability checking via exploits, and static analysis for new vulnerabilities.

Result: Current agents perform poorly on secure coding - best agent (SWE-agent with DeepSeek-V3.1) achieved only 15.2% correct-and-secure solutions. Some agents produce functional but vulnerable code, including new vulnerabilities. Security instructions don't significantly help.

Conclusion: SecureAgentBench establishes rigorous evaluation for secure code generation, highlighting the need for further research to improve LLM-powered code agents' security reliability.

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [8] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: SK2Decompile is a two-phase LLM-based decompilation approach that first recovers program structure (skeleton) then generates meaningful identifiers (skin), significantly outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based decompilers are limited in effectively presenting source-level structure with original identifiers, motivating a more structured approach to decompilation.

Method: Two-phase approach: 1) Structure Recovery model translates binary to IR with generic placeholders using reinforcement learning for syntactic/semantic correctness, 2) Identifier Naming model generates meaningful identifiers using reinforcement learning for semantic similarity.

Result: Significant improvements over SOTA: 21.6% average re-executability rate gain over GPT-5-mini on HumanEval and 29.4% average R2I improvement over Idioms on GitHub2025 benchmark.

Conclusion: The two-phase decompilation process facilitates independent advancement of correctness and readability, making SK2Decompile a highly effective approach for binary decompilation.

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [9] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN is an LLM-driven agent framework for intelligent MMORPG testing that achieves high task completion rates (95%) and superior bug detection compared to existing approaches, detecting four previously unknown bugs.


<details>
  <summary>Details</summary>
Motivation: Traditional automated game testing struggles with high state coverage and efficiency in complex MMORPGs, while existing LLM-based approaches have limited reasoning ability for complex game state-action spaces and long tasks.

Method: TITAN incorporates four key components: (1) perceiving and abstracting high-dimensional game states, (2) proactively optimizing and prioritizing actions, (3) long-horizon reasoning with action trace memory and reflective self-correction, and (4) LLM-based oracles for bug detection with diagnostic reports.

Result: TITAN achieves 95% task completion rate and superior bug detection on two commercial MMORPGs, detects four previously unknown bugs, and has been deployed in eight real-world game QA pipelines.

Conclusion: TITAN demonstrates significant practical impact as an LLM-driven game testing framework and provides guidance for advancing intelligent, general-purpose testing systems.

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [10] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: Systematic study shows LLMs are highly vulnerable to library hallucinations triggered by realistic user prompt variations, with one-character misspellings causing up to 26% hallucinations and fake names accepted in up to 99% of tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs increasingly generate code but frequently hallucinate non-existent libraries, which can mislead developers, break builds, and expose systems to supply chain threats like slopsquatting. Little is known about how real-world prompt variations affect hallucination rates.

Method: Evaluated six diverse LLMs across two hallucination types: library name hallucinations (invalid imports) and library member hallucinations (invalid calls from valid libraries). Investigated realistic user language from developer forums and user errors including one/multi-character misspellings and fake names/members.

Result: Revealed systemic vulnerabilities: one-character misspellings in library names trigger hallucinations in up to 26% of tasks, fake library names accepted in up to 99% of tasks, time-related prompts lead to hallucinations in up to 84% of tasks. Prompt engineering shows promise but remains inconsistent and LLM-dependent.

Conclusion: LLMs are fragile to natural prompt variation, highlighting urgent need for safeguards against library-related hallucinations and their potential exploitation.

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [11] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: Green Prompt Engineering explores how linguistic complexity in prompts affects energy consumption and performance in language models, showing that simpler prompts can reduce energy costs without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Address environmental concerns from language model inference by examining linguistic complexity as a sustainability factor, which has received little attention compared to hardware choices and prompt length.

Method: Empirical study on requirement classification using open-source Small Language Models, varying the readability of prompts to measure impact on energy consumption and performance.

Result: Readability affects both environmental sustainability and performance, revealing trade-offs between them. Simpler prompts can reduce energy costs without significant F1-score loss.

Conclusion: Green Prompt Engineering opens a path toward sustainable prompt design guidelines within the Green AI agenda, benefiting both practitioners (energy savings) and researchers (new study directions).

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [12] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: GPU-accelerated Loopy Belief Propagation for program analysis with flexible update strategies and optimized message grouping to minimize warp divergence.


<details>
  <summary>Details</summary>
Motivation: LBP faces computational challenges in large-scale program analysis, and existing GPU approaches lack flexible update strategies and integration with logical constraints.

Method: Proposed unified representation for user-defined update strategies with dependency analysis, and grouped messages to minimize warp divergence for better GPU utilization.

Result: Achieved average speedup of 2.14× over state-of-the-art sequential approach and 5.56× over state-of-the-art GPU-based approach on datarace analysis over eight real-world Java programs.

Conclusion: The GPU-accelerated LBP algorithm effectively addresses computational bottlenecks in program analysis while maintaining high accuracy.

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [13] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: This paper presents an empirical study comparing four testing modalities (SiL, ViL, MR, and real-world) for autonomous driving systems, evaluating their effectiveness in bridging the reality gap across actuation, perception, and behavioral fidelity dimensions.


<details>
  <summary>Details</summary>
Motivation: To address the reality gap challenge in autonomous driving system testing, where discrepancies between simulated and real-world behavior limit the transferability of test results to deployed systems.

Method: Used a small-scale physical vehicle with real sensors (camera and LiDAR) and its digital twin to implement four testing modalities (SiL, ViL, MR, and real-world), evaluating two ADS architectures (modular and end-to-end) across diverse indoor driving scenarios with real obstacles and environments.

Result: SiL and ViL setups simplify critical aspects of real-world dynamics and sensing, while MR testing improves perceptual realism without compromising safety or control. The study identified conditions where failures don't transfer across modalities and isolated the underlying dimensions of the reality gap responsible for these discrepancies.

Conclusion: The findings provide actionable insights into the strengths and limitations of each testing modality and outline a path toward more robust and transferable validation of autonomous driving systems.

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [14] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: Context-specific instruction (pairing concrete bug-localization steps with problem-specific details) outperforms abstract guidelines and general concrete steps in teaching bug localization to novices, achieving faster skill acquisition and better retention.


<details>
  <summary>Details</summary>
Motivation: Novices often lack systematic approaches to bug localization, and the impact of context-specific instruction compared to abstract guidelines or general concrete steps is unclear.

Method: Eight-week longitudinal study with 44 undergraduates across four conditions: no instruction, abstract guidelines, concrete steps, and context-specific instruction. Participants completed 2-3 debugging tasks per session over five sessions to identify minimal code elements containing logical faults.

Result: Context-specific instruction group (G4) achieved 80% correctness after one session (vs. 20-44% for others) and maintained 80% after three weeks, with time to completion stabilizing at 13-15 minutes vs. 22-27 minutes for other groups. Also showed lower stress and higher satisfaction.

Conclusion: Context-specific instruction yields faster skill acquisition and stronger retention than abstract guidelines or context-agnostic steps. Integrating contextual examples with abstract principles bridges theory-practice gaps in bug-localization education.

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [15] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind integrates LLMs with Monte Carlo Tree Search to automatically reproduce Android app crashes from bug reports, outperforming existing methods by combining semantic reasoning with strategic UI exploration.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with incomplete bug reports and complex UI interactions, lacking goal-directed reasoning to infer missing steps and navigate the vast UI space for crash reproduction.

Method: Combines LLMs with customized MCTS: uses Expander agent to generate promising actions and Simulator agent to estimate success likelihood, incorporating multi-modal UI inputs and feedback-aware navigation.

Result: Outperforms four state-of-the-art baselines on 93 real-world Android bug reports, achieving higher reproduction success rates.

Conclusion: Integrating LLM reasoning with MCTS-based planning is a compelling direction for automated bug reproduction, effectively handling incomplete reports and complex UI interactions.

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [16] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: AFD enhances pointer analysis by automatically identifying and modeling custom allocation functions in C/C++ programs using a hybrid approach of value-flow analysis and LLMs, achieving significant precision improvements with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Pointer analysis is hindered by imprecise modeling of heap allocations, especially for pervasive user-defined allocation functions in C/C++ programs that existing approaches largely overlook, leading to coarse aliasing and reduced analysis precision.

Method: AFD uses a hybrid approach: value-flow analysis to detect straightforward wrapper functions and Large Language Models (LLMs) to reason about complex allocation patterns with side effects, enabling precise modeling of heap objects at each call site.

Result: Evaluation on 15 real-world C projects identified over 600 custom allocation functions. Integration with baseline pointer analysis yielded 26x more modeled heap objects, 39% reduction in alias set sizes with only 1.4x runtime overhead, plus improved indirect call resolution and discovery of 17 previously undetected memory bugs.

Conclusion: Precise modeling of custom allocation functions offers a scalable and practical path to improving pointer analysis in large software systems, achieving context-sensitivity-like benefits without the associated overhead.

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>
