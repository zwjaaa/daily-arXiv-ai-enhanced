<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX is an adaptive program repair method that uses fast and slow thinking approaches to handle different complexity levels of bugs, achieving state-of-the-art performance on SWE-bench Lite.


<details>
  <summary>Details</summary>
Motivation: To enhance large language model-based agents' capabilities on complex tasks like program repair by leveraging dual thinking modes for better efficiency and accuracy.

Method: Uses slow thinking bug fix agent for complex repairs and fast thinking workflow decision components to classify issue descriptions. Adaptively selects easy, middle, and hard repair modes based on problem complexity, employing fast generalization for simple problems and test-time scaling for complex ones.

Result: Achieves 60.67% pass@1 performance on SWE-bench Lite using Claude-4 Sonnet model, reaching state-of-the-art levels among open-source methods.

Conclusion: SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair through adaptive thinking modes.

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [2] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: Analysis of 140,000 research articles and code repositories reveals that software development contributions are poorly recognized in scholarly credit systems, with 30% of articles having non-author code contributors and frequent coders showing lower h-indices despite similar publication counts.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between software development activities and traditional scholarly credit metrics in scientific research, as software has become essential but its contribution recognition remains unclear.

Method: Created a dataset of 140,000 paired research articles and code repositories, developed a predictive model to match article authors with software repository developers, and analyzed how software contributions affect credit allocation.

Result: 30% of articles have non-author code contributors; code-contributing authors show only 4.2% citation increase (non-significant when controlled); first authors are more likely to be code contributors; frequent coders have progressively lower h-indices than non-coding colleagues even when controlling for publication count and other factors.

Conclusion: There is a significant disconnect between software contributions and scholarly credit, with implications for institutional reward structures and science policy that need to better recognize software development work.

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [3] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD is a large-scale, language-agnostic dataset with over 7 million parsed source files across 10 programming languages, providing unified AST representations for cross-language code analysis.


<details>
  <summary>Details</summary>
Motivation: To create a unified dataset that enables consistent cross-language reasoning and structural learning, addressing limitations of existing corpora that focus on token-level code or isolated parsers.

Method: Developed a universal Abstract Syntax Tree schema to normalize syntactic representations across languages, with complete pipelines for dataset reproduction, grammar compilation, and visualization tools.

Result: Empirical analyses show strong cross-language structural regularities, demonstrating that syntactic graphs from diverse languages can be aligned under a shared schema.

Conclusion: MLCPD establishes an open, reproducible foundation for future research in cross-language representation learning and program analysis, with publicly released dataset and codebase.

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [4] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt is a novel framework that uses static program analysis and LLMs to improve automated code optimization by precisely identifying optimizable code segments and retrieving relevant optimization strategies, significantly outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing code optimization approaches using LLMs often fail because semantically equivalent optimizations can appear in syntactically different code, making current retrieval methods ineffective at finding relevant examples.

Method: SemOpt has three LLM-powered components: (1) strategy library builder that extracts and clusters optimization strategies from real-world code modifications, (2) rule generator that creates Semgrep static analysis rules to identify optimization conditions, and (3) optimizer that uses the strategy library to generate optimized code.

Result: On 151 optimization tasks, SemOpt increased successful optimizations by 1.38 to 28 times compared to baselines. On large-scale C/C++ projects, it improved performance metrics by 5.04% to 218.07%.

Conclusion: SemOpt effectively addresses the limitations of current retrieval-based optimization approaches by leveraging static analysis and demonstrates significant practical utility in real-world code optimization scenarios.

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [5] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: The paper proposes a Code Digital Twin framework to bridge AI capabilities with enterprise software development realities by modeling both physical and conceptual layers of software, preserving tacit knowledge, and enabling co-evolution with codebases.


<details>
  <summary>Details</summary>
Motivation: Enterprise software development faces challenges beyond routine coding, relying heavily on tacit knowledge, design decisions, and historical trade-offs. Current LLMs lack alignment with practical enterprise development realities.

Method: Proposes Code Digital Twin - a living framework with hybrid knowledge representations, multi-stage extraction pipelines, incremental updates, LLM-empowered applications, and human-in-the-loop feedback to transform fragmented knowledge into explicit representations.

Result: The framework provides a systematic approach to address challenges in enterprise software evolution by integrating AI capabilities with structured knowledge frameworks for enhanced decision-making in tasks like issue localization and impact analysis.

Conclusion: The Code Digital Twin serves as a bridge between AI advancements and enterprise software realities, offering a roadmap for sustainable, intelligent, and resilient development of ultra-complex systems.

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [6] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: This study analyzes continuous fuzzing's role in vulnerability detection using OSS-Fuzz data, finding high early detection rates, increasing coverage over time, and coverage changes contributing to bug detection.


<details>
  <summary>Details</summary>
Motivation: To understand how continuous fuzzing contributes to vulnerability detection despite widespread adoption, as it's unclear how effective it is in practice.

Method: Collected issue reports, coverage reports, and fuzzing logs from OSS-Fuzz (Google's continuous fuzzing service), analyzing approximately 1.12 million fuzzing sessions from 878 projects.

Result: (i) Many fuzzing bugs existed before continuous fuzzing integration, leading to high early detection rates; (ii) Code coverage increases progressively with continuous fuzzing; (iii) Coverage changes contribute to fuzzing bug detection.

Conclusion: Continuous fuzzing effectively contributes to vulnerability detection through increasing coverage and coverage changes, providing empirical insights for future strategies and tool development.

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [7] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: Examining challenges of using LLMs for qualitative synthesis in systematic reviews through collaborative autoethnography trials.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for supporting systematic reviews including qualitative synthesis, but applying them to unevenly reported stages carries risks of amplifying weaknesses and eroding confidence in findings.

Method: Conducted collaborative autoethnography involving two trials, evaluating each for methodological rigor and practical usefulness, interpreted through technical lens of LLM limitations.

Result: Identified challenges in using LLMs for qualitative synthesis due to uneven reporting and variable conduct in systematic reviews.

Conclusion: Careful consideration needed when applying LLMs to qualitative synthesis in systematic reviews to avoid amplifying existing weaknesses and maintain confidence in findings.

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [8] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval is the first large-scale benchmark for evaluating LLM-based code readability assessment, showing that developer-guided prompting improves alignment with human judgments and enhances explanation quality, though with increased score variability.


<details>
  <summary>Details</summary>
Motivation: Code readability is crucial for software maintenance but difficult to assess at scale. Traditional static metrics fail to capture subjective human judgments, and LLMs' behavior as readability evaluators remains underexplored.

Method: Created CoReEval benchmark with 1.4M evaluations across 10 LLMs, spanning 3 programming languages, 2 code types, 4 prompting strategies, 9 decoding settings, and developer-guided prompts. Compared LLM outputs against human annotations and static models using numerical alignment metrics and justification quality analysis.

Result: Developer-guided prompting grounded in human-defined readability dimensions improves alignment in structured contexts, enhances explanation quality, and enables lightweight personalization through persona framing. However, increased score variability highlights trade-offs between alignment, stability, and interpretability.

Conclusion: CoReEval provides a robust foundation for prompt engineering, model alignment studies, and human-in-the-loop evaluation, with applications in education, onboarding, and CI/CD pipelines where LLMs can serve as explainable, adaptable reviewers.

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [9] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: This study compares the impact of hyperparameter tuning across two software defect prediction scenarios (IVDP and CVDP), finding that performance gains are significantly larger in IVDP than CVDP, and that these gains don't consistently apply across all ML algorithms or dataset sizes.


<details>
  <summary>Details</summary>
Motivation: To provide comprehensive insights into how hyperparameter tuning impacts different SDP scenarios, enhancing the robustness and practicality of SDP modeling for quality assurance.

Method: Contrasted hyperparameter tuning impact across IVDP and CVDP scenarios using 28 ML algorithms, 53 datasets, two tuning algorithms, and five optimization metrics with statistical analytics.

Result: SDP gains in IVDP are significantly larger than in CVDP; performance gains don't hold consistently across 24/28 ML algorithms; small datasets show larger performance impact differences.

Conclusion: Researchers and practitioners should consider the effect of the selected SDP scenario when expecting performance gains from hyperparameter tuning.

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [10] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanBench is a benchmark for evaluating LLMs on quantum code generation, covering 44 tasks across quantum algorithms, state preparation, gate decomposition, and quantum machine learning. Current LLMs show limited capability with overall accuracy below 40% and frequent semantic errors.


<details>
  <summary>Details</summary>
Motivation: Large language models have shown good performance in general code generation, but their capabilities in quantum code generation remain insufficiently studied, creating a need for systematic evaluation.

Method: Created QuanBench with 44 programming tasks, each with executable canonical solutions, evaluated using functional correctness (Pass@K) and quantum semantic equivalence (Process Fidelity). Evaluated several recent LLMs including general-purpose and code-specialized models.

Result: Current LLMs have limited capability in generating correct quantum code, with overall accuracy below 40%. Common failure cases include outdated API usage, circuit construction errors, and incorrect algorithm logic.

Conclusion: QuanBench provides a basis for future work on improving quantum code generation with LLMs, highlighting the current limitations and need for better quantum programming capabilities in language models.

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [11] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: LLM-powered coding agents face high costs due to inefficient turn usage. This paper analyzes turn-control strategies and finds dynamic-turn allocation reduces costs by 12%-24% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Practical deployment of LLM coding agents is hindered by unpredictable costs from quadratic token growth, expensive models, and inefficient turn usage. Existing research focuses on individual turns but neglects strategic turn control.

Method: Comprehensive empirical study on SWE-bench using three state-of-the-art models, evaluating three turn-control strategies: unrestricted baseline, fixed-turn limit with reminders, and novel dynamic-turn strategy with on-demand extensions.

Result: Fixed-turn limit at 75th percentile reduces costs by 24%-68% with minimal solve rate impact. Dynamic-turn strategy outperforms fixed limits, achieving comparable/better solve rates while further reducing costs by 12%-24% through intelligent resource allocation.

Conclusion: Dynamic resource allocation is a superior, easy-to-implement approach for deploying economically viable coding agents, providing systematic guidelines for balancing cost and efficacy.

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [12] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: Many-shot prompting doesn't always improve code translation performance - functional correctness peaks with just 5-25 examples, while more examples can degrade performance despite modest improvements in similarity metrics.


<details>
  <summary>Details</summary>
Motivation: To investigate whether providing many examples (many-shot prompting) actually enhances performance for complex tasks like code translation, challenging the common assumption that "more is better" for in-context learning with large language models.

Method: Large-scale empirical study of over 90,000 code translations, systematically evaluating scaling from zero-shot to many-shot configurations (up to 625 examples) with prompts spanning 100,000 to 800,000 tokens.

Result: Revealed a "many-shot paradox" - while static similarity metrics modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples) and degrades with substantially more examples.

Conclusion: For code translation, quality of a few well-chosen examples outweighs sheer quantity, challenging universal efficacy of "more is better" for ICL and highlighting task-dependent nature of optimal prompting strategies.

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [13] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: LLMs generate Chrome extensions with high vulnerability rates (18%-50%), particularly in Authentication & Identity (83%) and Cookie Management (78%) scenarios, exposing sensitive browser data despite advanced reasoning models performing worse than simpler ones.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly drive software development, developers focus on functionality over security, missing hidden implementation vulnerabilities in framework-constrained programs like Chrome extensions with complex security models.

Method: Built ChromeSecBench dataset with 140 prompts from known vulnerable extensions, used 9 state-of-the-art LLMs to generate Chrome extensions, analyzed vulnerabilities across scenario types, model differences, and vulnerability categories.

Result: LLMs produced vulnerable programs at alarmingly high rates (18%-50%), with highest vulnerability rates in Authentication & Identity (83%) and Cookie Management (78%) scenarios. Most vulnerabilities exposed sensitive browser data like cookies, history, or bookmarks to untrusted code.

Conclusion: There's a critical gap between LLMs' coding skills and their ability to write secure framework-constrained programs, with advanced reasoning models performing worse than simpler models in generating secure code.

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [14] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: AI models (GPT-4o and Gemini 2.5 Flash) show promise in usability inspection but cannot replace human inspectors due to higher false positives and redundant reports. The best results come from combining AI with human expertise.


<details>
  <summary>Details</summary>
Motivation: Usability inspection is costly and requires specialized knowledge. Advances in AI offer opportunities to support this task through generative models that can interpret interfaces more efficiently.

Method: Evaluated a software prototype using four human specialists and two AI models (GPT-4o and Gemini 2.5 Flash), comparing performance using precision, recall, and F1-score metrics.

Result: Human inspectors achieved highest precision and overall coverage. AIs demonstrated high individual performance and discovered novel defects but had higher false positive rates and redundant reports. The combination of AI and humans produced best results.

Conclusion: AI cannot replace human inspectors but serves as valuable augmentation tool to improve efficiency and expand defect coverage. Results support complementary use of AI in software quality assessment.

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [15] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: This paper introduces a Model-Driven Development (MDD) approach for quantum system engineering, enabling automatic code generation for multiple quantum programming languages to improve development efficiency and platform consistency.


<details>
  <summary>Details</summary>
Motivation: Despite the emergence of quantum supremacy and numerous quantum programming languages, Model-Driven Development remains largely unexplored in quantum system engineering, creating a gap for structured development approaches.

Method: The authors propose an MDD-based framework that supports structured design and implementation of quantum systems, with automatic generation of quantum code for multiple quantum programming languages.

Result: The effectiveness and practicality of the approach have been demonstrated through multiple case studies, showing enhanced development efficiency and consistency across heterogeneous quantum platforms.

Conclusion: The MDD-based approach successfully addresses the gap in quantum system engineering by providing a structured development methodology that supports multiple quantum programming languages through automatic code generation.

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [16] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER is a self-exploring reasoning framework that addresses limitations in Chain-of-Thought code generation by exploring diverse reasoning paths, training quality-aware models, and enabling adaptive reasoning switching.


<details>
  <summary>Details</summary>
Motivation: Current CoT approaches for code generation have limitations: limited exploration of diverse reasoning paths, lack of quality assessment for intermediate steps, and potential negative impact of 'overthinking' leading to complex incorrect solutions.

Method: SEER frames CoT code generation as decision making with three components: (1) Diverse reasoning path exploration without manual experts, (2) Training policy model for generating reasoning steps and value model for quality assessment, (3) Adaptive switching between direct generation and step-by-step reasoning.

Result: The framework enables accurate and adaptive reasoning for code generation by addressing the three key limitations of existing approaches.

Conclusion: SEER provides a comprehensive solution for improving Chain-of-Thought reasoning in code generation through self-exploration, quality assessment, and adaptive reasoning strategies.

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [17] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: Peace is a hybrid framework for project-level code efficiency optimization through automatic code editing, addressing limitations of previous function-level approaches by considering inter-function interactions and ensuring project correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong code generation capabilities but their potential in code efficiency optimization is underexplored. Previous approaches only focused on function-level optimization and ignored interactions between functions, failing to generalize to real-world scenarios.

Method: Peace integrates three phases: dependency-aware optimizing function sequence construction, valid associated edits identification, and efficiency optimization editing iteration. It uses a hybrid framework for project-level optimization while ensuring overall correctness and integrity.

Result: Peace achieves 69.2% correctness rate (pass@1), +46.9% optimization rate, and 0.840 speedup in execution efficiency on the PeacExec benchmark with 146 real-world optimization tasks from 47 GitHub Python projects, significantly outperforming all baselines.

Conclusion: Peace demonstrates superior performance in project-level code efficiency optimization, particularly excelling in complex tasks with multiple functions, and validates the effectiveness of its hybrid framework design through extensive experiments.

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [18] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: TREAT is an evaluation framework for assessing the trustworthiness and reliability of large foundation models in software engineering tasks, addressing limitations in existing benchmarks through multi-task, multi-language, multi-modality, and robustness assessments.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating code foundation models have limited task scope and fail to assess critical aspects like robustness and reliability, creating a gap in comprehensive trustworthiness evaluation for real-world software engineering scenarios.

Method: The TREAT framework provides holistic assessment through four main improvements: multi-task evaluation across diverse software engineering activities, multi-language and multi-modality assessment, robustness evaluation under code transformations, and rigorous methodology with diverse prompts and adaptive solution extraction.

Result: Evaluation of 26 state-of-the-art models revealed substantial performance variation across programming tasks and identified specific limitations in multi-modal models for UI code generation and editing.

Conclusion: The TREAT framework successfully addresses key limitations in existing evaluation approaches and provides comprehensive insights into model trustworthiness, revealing both strengths and limitations of current code foundation models.

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [19] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: This study investigates how software testing professionals use LLMs in practice and proposes a preliminary guideline for integrating LLMs into testing workflows.


<details>
  <summary>Details</summary>
Motivation: The adoption of large language models in software testing is growing rapidly but often relies on informal experimentation rather than structured guidance, creating a need for practitioner-informed guidelines.

Method: Qualitative study with 15 software testers from diverse roles and domains using semi-structured interviews and grounded theory-based thematic analysis.

Result: Testers described an iterative process involving defining objectives, prompt engineering, refinement, output evaluation, and learning over time, with emphasis on human oversight due to LLM limitations like hallucinations.

Conclusion: LLM adoption in software testing is growing but shaped by evolving practices and risk caution; this study provides a starting point for structured LLM use and invites future research refinement.

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [20] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW is a tool supporting the ACIMOV methodology for agile ontology development on GitHub, using W3C standards to assist modular ontology development through various interfaces.


<details>
  <summary>Details</summary>
Motivation: Agile and collaborative approaches are needed for ontology design to ensure they remain user-driven, up-to-date, and evolve with systems, requiring continuous validation tooling.

Method: OLIVAW relies on W3C Standards and supports development through GitHub Composite Actions, pre-commit hooks, or a command line interface.

Result: OLIVAW was tested on several ontology projects and demonstrated usefulness, genericity, and reusability. A template repository is available.

Conclusion: OLIVAW provides a practical tool for continuous validation and agile development of ontologies using the ACIMOV methodology on GitHub.

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [21] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack improves code generation by incorporating backtracking to avoid distorting model output intent while maintaining constraint compliance, achieving significant improvements over constrained decoding methods.


<details>
  <summary>Details</summary>
Motivation: Constrained decoding techniques for code generation often distort the model's output intent, producing code that satisfies constraints but doesn't match development intent, leading to incorrect results.

Method: AdapTrack incorporates backtracking into the generation process to avoid distorting the model's output intent while ensuring constraint compliance. It provides theoretical proof that the distribution aligns with the model's distribution given generated tokens.

Result: Achieved up to 360.87% improvement on synthetic API completion dataset, 38.93% on real-world API completion dataset, 7.84% on HumanEval, and 6.42% on MBPP benchmarks compared to constrained decoding.

Conclusion: AdapTrack significantly improves code generation by better adhering to the model's output intent while maintaining constraint compliance, providing more semantically aligned results than constrained decoding methods.

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [22] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: Developed a Scalable CI/CD Pipeline to address Japan's 2025 cliff problem by modernizing legacy IT systems through automated, containerized development environments using GitHub, Jenkins, AWS, and Docker.


<details>
  <summary>Details</summary>
Motivation: Address the Japan 2025 cliff problem where legacy core IT systems reaching end-of-service life threaten to increase maintenance costs by up to 12 trillion yen annually and hinder Digital Transformation progress, as experienced by Asahi with outdated systems and manual workflows.

Method: Implemented a Scalable CI/CD Pipeline using GitHub for source control, Jenkins for automation, AWS for scalable infrastructure, and Docker for containerization, enabling dynamic creation and deletion of isolated development environments.

Result: Developers can now freely test maintenance procedures and experiment with new technologies in isolated environments, reducing maintenance costs and driving Digital Transformation.

Conclusion: The Scalable CI/CD Pipeline successfully addresses legacy system modernization challenges by providing flexible, automated development environments that reduce costs and enable digital transformation progress.

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>
