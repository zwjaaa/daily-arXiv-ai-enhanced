<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust is a compiler that automatically inserts flush and fence operations to eliminate missing flush bugs in persistent memory code, achieving minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: Manual flush operations for persistent memory are error-prone and challenging for developers, while existing tools require bug-revealing test cases and cannot guarantee absence of missing flush bugs.

Method: PMRobust employs a novel static analysis with optimizations targeting newly allocated objects to automatically insert necessary flush and fence operations during compilation.

Result: Evaluation on persistent memory libraries and data structures showed a geometric mean overhead of only 0.26% compared to hand-placed flush operations.

Conclusion: PMRobust effectively eliminates missing flush bugs in persistent memory code with negligible performance impact, providing automated correctness guarantees.

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [2] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: A framework integrating reasoning LLMs with AFL++ fuzzer to address semantic limitations of traditional mutation-based fuzzing through prompt-based few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzers lack semantic reasoning for complex protocol logic and domain-specific constraints, while LLMs can understand input formats and perform targeted mutations like human experts, but supervised fine-tuning is impractical without ground truth.

Method: Open-source microservices framework integrating reasoning LLMs with AFL++ on Google's FuzzBench, addressing asynchronous execution and hardware divergence between GPU-intensive LLMs and CPU-intensive fuzzers.

Result: Deepseek-r1-Distill-Llama-70B performed best among tested models. Mutation effectiveness depends more on prompt complexity and model choice than shot count. Response latency and throughput are key bottlenecks.

Conclusion: Reasoning LLMs show promise for improving fuzzing quality through semantic mutations, but performance depends on model selection and prompt engineering, with latency/throughput challenges requiring future optimization.

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [3] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: LLMs can effectively recover user stories from source code with F1 scores around 0.8 for code up to 200 lines, and smaller models can match larger ones when given a single example.


<details>
  <summary>Details</summary>
Motivation: User stories are often missing in legacy systems, and this research explores whether LLMs can automatically generate them from source code to improve documentation.

Method: Evaluated 5 state-of-the-art LLMs across 6 prompting strategies using 1,750 annotated C++ code snippets of varying complexity.

Result: All models achieved average F1 score of 0.8 for code up to 200 NLOC. Single example enabled 8B model to match 70B model performance. Chain-of-Thought provided only marginal gains for larger models.

Conclusion: LLMs are effective for user story recovery from code, with smaller models performing well when properly prompted, making this approach practical for real-world applications.

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [4] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: Evaluation of four FIM LLMs (Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, StarCoder) for generating assertion messages in Java unit tests, with Codestral-22B achieving the best performance but still below human-written quality.


<details>
  <summary>Details</summary>
Motivation: Assertion messages are crucial for understanding test failures but are often omitted by developers and automated tools. LLMs have not been systematically evaluated for generating informative assertion messages.

Method: Evaluated four state-of-the-art FIM LLMs on 216 Java test methods containing developer-written assertion messages using human-like evaluation approach. Conducted ablation study on the impact of descriptive test comments.

Result: Codestral-22B achieved highest quality score of 2.76/5 (vs 3.24 for manual messages). Including test comments improved Codestral's performance to 2.97. Models frequently replicated developers' linguistic patterns.

Conclusion: Context is critical for generating clear assertion messages. Current models and conventional metrics have limitations in capturing diverse assertion message structures. The study provides foundation for advancing automated, context-aware assertion message generation.

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [5] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: Real-world evaluation of AI-assisted software development tools at enterprise scale shows significant productivity improvements, including 31.8% reduction in PR review cycle time and strong developer adoption with 85% satisfaction.


<details>
  <summary>Details</summary>
Motivation: To provide empirical evidence from production environments about the transformative potential and practical deployment challenges of integrating AI into enterprise software development workflows, moving beyond controlled benchmark evaluations.

Method: Longitudinal study over one year with 300 engineers across multiple teams using an in-house AI platform (DeputyDev) that combines code generation and automated review capabilities. Used rigorous cohort analysis to track adoption patterns and productivity metrics.

Result: Statistically significant productivity improvements: 31.8% reduction in PR review cycle time, 61% increase in code volume for top adopters, with 30-40% of production code shipped through the tool. Strong adoption scaling from 4% to 83% peak usage, stabilizing at 60% active engagement with 85% satisfaction for code review features.

Conclusion: AI-assisted development tools demonstrate transformative potential in enterprise settings, with substantial productivity gains and high developer satisfaction, though practical deployment challenges exist that differ from controlled benchmark evaluations.

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [6] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen is a multi-agent system that improves code generation across multiple programming languages by using data-driven bridging languages and iterative error correction.


<details>
  <summary>Details</summary>
Motivation: Current LLMs have varying proficiency across programming languages, especially for less common languages like Rust, Perl, OCaml, and Erlang. Existing approaches treat each language in isolation, missing opportunities for cross-language knowledge sharing.

Method: Uses a coordinated multi-agent architecture with intermediate representation, code generation, translation, and automated repair. Features data-driven bridging language selection based on transfer matrices that identify optimal intermediate languages through demonstrated translation success rather than raw generation accuracy.

Result: Achieves 13 percentage-point gains over the strongest fine-tuned baseline and up to 30 percentage points over existing single-language multi-agent methods. Ablation studies show compatibility-guided bridging significantly outperforms LLM-based heuristics.

Conclusion: The approach demonstrates the value of cumulative cross-language knowledge transfer, with data-driven bridging language selection proving more effective than heuristic methods.

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [7] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: The paper introduces Neural Network Bill of Material (NNBOM) as a specialized dataset construct for analyzing neural network software evolution, addressing limitations of traditional SBOMs and conceptual AIBOMs for NN repositories.


<details>
  <summary>Details</summary>
Motivation: Existing Software Bill of Materials (SBOMs) are ill-suited for neural network software due to distinct component structures and reuse patterns involving pre-trained models and modules. Conceptual AIBOMs lack practical implementations for large-scale evolutionary analysis.

Method: Created a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories, cataloging their third-party libraries (TPLs), pre-trained models (PTMs), and modules. Conducted comprehensive empirical study of NN software evolution across software scale, component reuse, and inter-domain dependency.

Result: Developed a comprehensive NNBOM dataset enabling evolutionary analysis of neural network software. Built two prototype applications: Multi repository Evolution Analyzer and Single repository Component Assessor and Recommender to demonstrate practical value.

Conclusion: NNBOM provides a tailored solution for analyzing neural network software evolution, offering maintainers and developers a holistic view of long-term trends in NN repository development and component reuse patterns.

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [8] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym is a comprehensive benchmark for evaluating code LLMs in visual game development, addressing the gap between current code generation metrics and practical game development requirements.


<details>
  <summary>Details</summary>
Motivation: Current code LLM benchmarks focus on syntax correctness and execution accuracy but overlook critical game-specific metrics like playability, visual aesthetics, and user engagement needed for real-world game development.

Method: Created a benchmark with 2,219 samples across 100 thematic clusters from real-world repositories using clustering-based curation. Introduced a multimodal evaluation framework with automated LLM-driven pipeline for visual code synthesis in UI sandbox environments.

Result: V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation.

Conclusion: The benchmark addresses the limitations of current code evaluation methods by incorporating comprehensive game development metrics, making it suitable for assessing LLMs' capabilities in practical visual game development scenarios.

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [9] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: This paper proposes using LLMs for data augmentation in requirements traceability to address training data scarcity, achieving up to 28.59% F1 score improvement.


<details>
  <summary>Details</summary>
Motivation: Existing automated traceability methods are limited by scarce training data and semantic gaps between requirements and code artifacts.

Method: Uses prompt-based techniques with four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, GPT-4) for data augmentation, employing zero-shot and few-shot templates, and optimizes the encoder component of the tracing model.

Result: Experimental results show significant performance enhancement with up to 28.59% improvement in F1 score.

Conclusion: The approach effectively addresses data scarcity in requirements traceability and demonstrates practical application potential.

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [10] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: Evaluation of LLMs for generating web API integration code reveals significant challenges with hallucinated endpoints and incorrect arguments, with open-source models solving less than 40% of tasks.


<details>
  <summary>Details</summary>
Motivation: API integration is crucial but challenging, and while LLMs are popular in software development, their effectiveness in generating web API invocation code remains unexplored.

Method: Created a dataset and evaluation pipeline to assess LLMs' ability to generate web API integration code, testing several open-source models.

Result: Generating API invocations proved highly challenging - models produced hallucinated endpoints, incorrect argument usage, and other errors. No open-source model solved more than 40% of tasks.

Conclusion: Current open-source LLMs struggle significantly with web API integration code generation, indicating this remains a difficult problem requiring further research.

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [11] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: VCD-RNK is a discriminator model for Verilog code reranking that addresses LLMs' limitations in Verilog generation by formulating it as a semantic alignment problem between requirements and implementations.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with Verilog generation due to limited domain knowledge, and while sampling improves pass@k metrics, hardware engineers need one trustworthy solution rather than multiple uncertain candidates.

Method: VCD-RNK incorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment, simulating these reasoning processes during inference to avoid computationally intensive test execution.

Result: The proposed method effectively avoids computationally intensive test execution used in existing methods while providing reliable Verilog code ranking.

Conclusion: VCD-RNK bridges the gap between sampling-based approaches and hardware engineers' need for trustworthy single solutions by providing efficient semantic alignment between requirements and Verilog implementations.

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [12] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: A zero-knowledge proof (ZKP)-based approach for verifiable execution of business processes while preserving confidentiality, using zkVMs integrated into BPM engines.


<details>
  <summary>Details</summary>
Motivation: Ensuring business process integrity without disclosing confidential information in inter-organizational processes is a major challenge.

Method: Integrate ZK virtual machines (zkVMs) into business process management engines through system architecture and implementation, supporting chained verifiable computations via proof compositions.

Result: Demonstrated on product carbon footprinting example, showing organizations can prove and verify process integrity without exposing sensitive information. Evaluated different ZKP proving variants for efficiency.

Conclusion: Experiment-driven evaluation demonstrates automation of process verification under confidentiality constraints, enabling practical integration of ZKPs throughout the BPM lifecycle.

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [13] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: A novel protocol testing approach using I/O grammars that combines input generation and output checking in a single framework, enabling complete specification of protocol syntax and semantics.


<details>
  <summary>Details</summary>
Motivation: Address two fundamental problems in software testing: generating diverse, correct inputs and having an oracle to check outputs, particularly challenging in protocol testing where inputs are messages and outputs are responses.

Method: Introduces I/O grammars to completely specify protocol syntax/semantics (messages, states, interactions). Implementation uses FANDANGO framework with user-defined constraints and k-path guidance for systematic coverage.

Result: Applied to DNS, FTP, and SMTP protocols, demonstrating correct specification of advanced features and enabling output validation. Systematic coverage achieves faster coverage of input/response spaces compared to random-based approaches.

Conclusion: I/O grammars provide a versatile framework that can act as test generator, mock object, and oracle, offering unified systematic coverage superior to existing random-based methods.

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [14] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: This study examines GitHub Copilot's impact on developer productivity using mixed-methods analysis of 26,317 commits from 703 repositories over two years, comparing 25 Copilot users with 14 non-users.


<details>
  <summary>Details</summary>
Motivation: To understand the real-world effects of generative AI tools like GitHub Copilot on developer activity and perceived productivity in an agile public sector organization.

Method: Mixed-methods case study analyzing commit-based activity metrics from GitHub repositories, supplemented by surveys on roles/perceived productivity and 13 interviews with developers.

Result: Copilot users were consistently more active than non-users even before adoption. No statistically significant changes in commit-based activity were found after Copilot adoption, though minor increases were observed, indicating a discrepancy between objective metrics and subjective productivity perceptions.

Conclusion: GitHub Copilot adoption did not produce statistically significant changes in measurable developer activity metrics, despite users' subjective perceptions of increased productivity, suggesting the need for better productivity measurement tools that capture AI tool benefits.

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>
