<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges](https://arxiv.org/abs/2509.15283)
*Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo*

Main category: cs.SE

TL;DR: Evaluation of open-source LLMs on competitive programming tasks using enhanced FACE framework shows local models perform at half the accuracy of proprietary models like GPT-4 and Gemini 1.5.


<details>
  <summary>Details</summary>
Motivation: To assess the performance gap between open-source locally hosted LLMs and proprietary models in handling complex competitive programming tasks with extended contexts.

Method: Enhanced the FACE framework to work offline via Ollama runtime, consolidated directory structure into JSON files, added checkpointing, and tested 8 code-oriented models (6.7-9B parameters) on 3,589 Kattis problems.

Result: Open-source models achieved modest pass@1 accuracy, with best models performing at approximately 50% of proprietary models' acceptance rates.

Conclusion: Significant gap exists between local and proprietary LLMs, but open-source models show rapid progress and the framework enables reproducible evaluation on in-house hardware.

Abstract: This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.

</details>


### [2] [LoCaL: Countering Surface Bias in Code Evaluation Metrics](https://arxiv.org/abs/2509.15397)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: The paper critically evaluates reference-based code evaluation metrics (CEMs), revealing their surface-level bias, and introduces LoCaL benchmark with 3117 code pairs to test CEM robustness.


<details>
  <summary>Details</summary>
Motivation: Current reference-based CEMs show weak correlation with functional correctness, but the causes are assumed and solutions unexplored. There's a need for better evaluation datasets that include challenging code pairs.

Method: Proposes LoCaL benchmark with 3117 code pairs labeled with functional similarity scores calculated through differential fuzzing, which executes more tests than prior work without predefined test cases.

Result: All four state-of-the-art CEMs show significant performance degradation on LoCaL compared to baselines, confirming their strong bias towards surface-level features rather than functionality.

Conclusion: Exposing CEMs to LoCaL-like data can help develop metrics robust to surface bias, addressing the gap in current CEM evaluation practices.

Abstract: With the increasing popularity of large language models (LLMs) and LLM-based
agents, reliable and effective code evaluation metrics (CEMs) have become
crucial for progress across several software engineering tasks. While popular
benchmarks often provide test cases to assess the correctness of generated
code, crafting and executing test cases is expensive. Reference-based CEMs
provide a cheaper alternative by scoring a candidate program based on its
functional similarity to a reference. Although prior research has focused on
reporting the weak correlation between these CEMs and functional correctness,
the causes are only assumed, and plausible solutions remain unexplored. In this
work, we critically evaluate four state-of-the-art reference-based CEMs,
revealing their strong bias towards surface-level features rather than code
functionality. Despite this surface bias, current evaluation datasets for these
CEMs rarely include code pairs that are surface-similar yet functionally
dissimilar, or functionally similar yet surface-dissimilar. To mitigate this
gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117
code pairs at both the method and program levels. Each pair is labeled with a
functional similarity score and aims to target regions where CEMs are likely to
perform poorly. The functional similarity scores are calculated through
differential fuzzing, which eliminates the need for predefined test cases and,
at the same time, improves the reliability of the scores by executing an order
of magnitude more tests than prior work. We find that all four CEMs show
significant performance degradation on LoCaL, compared to the baselines.
Finally, based on our findings, we draw the implication that exposing CEMs to
LoCaL-like data might facilitate the development of metrics that are robust to
surface bias.

</details>


### [3] [Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation](https://arxiv.org/abs/2509.15567)
*Hongyu Kuang,Ning Zhang,Hui Gao,Xin Zhou,Wesley K. G. Assunção,Xiaoxing Ma,Dong Shao,Guoping Rong,He Zhang*

Main category: cs.SE

TL;DR: This paper proposes a novel approach for automatic commit message generation by condensing code changes into structured text templates before generation, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Developers often neglect writing high-quality commit messages, making automated generation crucial for software maintenance. Existing methods focus on representing code changes but lack effective condensation strategies.

Method: The approach uses three-part text templates (summarized changes, elicited comments, emphasized identifiers) created with ChangeScribe tool, then fine-tunes CodeLlama-7B on template-message pairs.

Result: Outperformed six baselines with average improvements of 51.7% (BLEU-Norm), 78.7% (METEOR), and 62.5% (ROUGE-L). Ablation studies and human evaluation confirmed effectiveness.

Conclusion: The proposed template-based condensation approach better utilizes pre-trained language models while maintaining brevity and readability, providing a complementary solution for commit message generation.

Abstract: Commit messages are valuable resources for describing why code changes are
committed to repositories in version control systems (e.g., Git). They
effectively help developers understand code changes and better perform software
maintenance tasks. Unfortunately, developers often neglect to write
high-quality commit messages in practice. Therefore, a growing body of work is
proposed to generate commit messages automatically. These works all
demonstrated that how to organize and represent code changes is vital in
generating good commit messages, including the use of fine-grained graphs or
embeddings to better represent code changes. In this study, we choose an
alternative way to condense code changes before generation, i.e., proposing
brief yet concise text templates consisting of the following three parts: (1)
summarized code changes, (2) elicited comments, and (3) emphasized code
identifiers. Specifically, we first condense code changes by using our proposed
templates with the help of a heuristic-based tool named ChangeScribe, and then
fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding
commit messages. Our proposed templates better utilize pre-trained language
models, while being naturally brief and readable to complement generated commit
messages for developers. Our evaluation based on a widely used dataset showed
that our approach can outperform six baselines in terms of BLEU-Norm, METEOR,
and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,
respectively. The ablation study and human evaluation also provide further
insights into the effectiveness of our approach.

</details>


### [4] [How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches](https://arxiv.org/abs/2509.15777)
*Haoran Xu,Zhi Chen,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: A novel two-stage framework combining version-driven filtering and LLM-based multi-round dialogue voting for accurate vulnerability patch detection in open-source software.


<details>
  <summary>Details</summary>
Motivation: Traditional manual methods are unscalable and error-prone, while existing automated approaches suffer from limited accuracy, poor generalization, and practical deployment constraints.

Method: Two-stage framework: 1) version-driven candidate filtering to reduce search space, 2) large language model-based multi-round dialogue voting for accurate patch identification.

Result: Extensive experiments on 750 real vulnerabilities show the method outperforms current approaches.

Conclusion: The proposed framework effectively addresses scalability and accuracy challenges in vulnerability patch detection through search space reduction and semantic understanding.

Abstract: Open-source software vulnerability patch detection is a critical component
for maintaining software security and ensuring software supply chain integrity.
Traditional manual detection methods face significant scalability challenges
when processing large volumes of commit histories, while being prone to human
errors and omissions. Existing automated approaches, including heuristic-based
methods and pre-trained model solutions, suffer from limited accuracy, poor
generalization capabilities, and inherent methodological constraints that
hinder their practical deployment. To address these fundamental challenges,
this paper conducts a comprehensive empirical study of existing vulnerability
patch detection methods, revealing four key insights that guide the design of
effective solutions: the critical impact of search space reduction, the
superiority of pre-trained semantic understanding over architectural
complexity, the temporal limitations of web crawling approaches, and the
advantages of knowledge-driven methods. Based on these insights, we propose a
novel two-stage framework that combines version-driven candidate filtering with
large language model-based multi-round dialogue voting to achieve accurate and
efficient vulnerability patch identification. Extensive experiments on a
dataset containing 750 real vulnerabilities demonstrate that our method
outperforms current approaches.

</details>


### [5] [Failure Modes and Effects Analysis: An Experience from the E-Bike Domain](https://arxiv.org/abs/2509.15893)
*Andrea Bombarda,Federico Conti,Marcello Minervini,Aurora Zanenga,Claudio Menghi*

Main category: cs.SE

TL;DR: Industrial study on using Simulink Fault Analyzer for FMEA in e-Bike CPS, showing that simulation-driven FMEA helps identify unexpected fault effects and improves model accuracy.


<details>
  <summary>Details</summary>
Motivation: Software failures in Cyber-Physical Systems can have catastrophic consequences, and industries need evidence of effectiveness for simulation-driven FMEA approaches to increase practical adoption.

Method: Used Simulink Fault Analyzer to analyze safety of an e-Bike CPS, identified 13 realistic faults, modeled them, and analyzed their effects with expert feedback to assess model appropriateness and fault detection effectiveness.

Result: Models were accurate or contained minor imprecision that were corrected; FMEA helped engineers improve models as 38.4% (5 out of 13) of faults revealed unexpected effects that didn't match engineers' expectations.

Conclusion: FMEA with simulation support effectively helps engineers discover unexpected fault effects and improve their models, with findings useful for Simulink engineers, safety analysts, and users of Simulink Fault Analyzer.

Abstract: Software failures can have catastrophic and costly consequences. Functional
Failure Mode and Effects Analysis (FMEA) is a standard technique used within
Cyber-Physical Systems (CPS) to identify software failures and assess their
consequences. Simulation-driven approaches have recently been shown to be
effective in supporting FMEA. However, industries need evidence of the
effectiveness of these approaches to increase practical adoption. This
industrial paper presents our experience with using FMEA to analyze the safety
of a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial
tool that supports engineers with FMEA. We identified 13 realistic faults,
modeled them, and analyzed their effects. We sought expert feedback to analyze
the appropriateness of our models and the effectiveness of the faults in
detecting safety breaches. Our results reveal that for the faults we
identified, our models were accurate or contained minor imprecision that we
subsequently corrected. They also confirm that FMEA helps engineers improve
their models. Specifically, the output provided by the simulation-driven
support for 38.4% (5 out of 13) of the faults did not match the engineers'
expectations, helping them discover unexpected effects of the faults. We
present a thorough discussion of our results and ten lessons learned. Our
findings are useful for software engineers who work as Simulink engineers, use
the Simulink Fault Analyzer, or work as safety analysts.

</details>


### [6] [LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines](https://arxiv.org/abs/2509.15971)
*Owen Truong,Terrence Zhang,Arnav Marchareddy,Ryan Lee,Jeffery Busold,Michael Socas,Eman Abdullah AlOmar*

Main category: cs.SE

TL;DR: A VS Code extension called LeakageDetector that identifies and corrects Data Leakage issues in Jupyter Notebooks for ML code quality improvement.


<details>
  <summary>Details</summary>
Motivation: To help ML engineers enhance code quality by detecting Data Leakage issues that cause misleading performance evaluations when test data information contaminates training data.

Method: Developed a Visual Studio Code extension with detection capabilities for Overlap, Preprocessing, and Multi-test leakage types, plus two correction approaches: manual quick fixes and LLM-driven guidance for ML pipeline best practices.

Result: Created LeakageDetector tool that can identify Data Leakage patterns and provide both conventional and AI-assisted correction mechanisms.

Conclusion: The extension successfully addresses Data Leakage issues in ML development workflows, offering practical solutions for maintaining code quality and accurate model evaluation.

Abstract: In software development environments, code quality is crucial. This study
aims to assist Machine Learning (ML) engineers in enhancing their code by
identifying and correcting Data Leakage issues within their models. Data
Leakage occurs when information from the test dataset is inadvertently included
in the training data when preparing a data science model, resulting in
misleading performance evaluations. ML developers must carefully separate their
data into training, evaluation, and test sets to avoid introducing Data Leakage
into their code. In this paper, we develop a new Visual Studio Code (VS Code)
extension, called LeakageDetector, that detects Data Leakage, mainly Overlap,
Preprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond
detection, we included two correction mechanisms: a conventional approach,
known as a quick fix, which manually fixes the leakage, and an LLM-driven
approach that guides ML developers toward best practices for building ML
pipelines.

</details>


### [7] [Software Development Aspects of Integrating Linear Algebra Libraries](https://arxiv.org/abs/2509.16081)
*Marcel Koch,Tobias Ribizel,Pratik Nayak,Fritz Göbel,Gregor Olenik,Terry Cojean*

Main category: cs.SE

TL;DR: Ginkgo is a sparse linear algebra library that helps scientific applications transition to modern systems and accelerate simulations by providing faster numerical routines. This paper examines the challenges and benefits of integrating Ginkgo into applications from various domains.


<details>
  <summary>Details</summary>
Motivation: Scientific simulation software relies on specialized building blocks from unfamiliar domains. Ginkgo addresses the need for efficient sparse linear algebra routines that can run on different platforms, helping applications modernize and improve performance.

Method: The paper presents case studies from different application domains (CFD, power grid simulation, electro-cardiophysiology) and analyzes the integration approaches from a software engineering perspective, highlighting sustainable development practices.

Result: The integration of Ginkgo enables applications to transition to modern systems more easily and achieve faster simulation speeds through improved numerical linear algebra performance.

Conclusion: Ginkgo provides a valuable building block for scientific applications, offering both performance benefits and sustainable software development approaches that facilitate easier adoption of modern computing platforms.

Abstract: Many scientific discoveries are made through, or aided by, the use of
simulation software. These sophisticated software applications are not built
from the ground up, instead they rely on smaller parts for specific use cases,
usually from domains unfamiliar to the application scientists. The software
library Ginkgo is one of these building blocks to handle sparse numerical
linear algebra on different platforms. By using Ginkgo, applications are able
to ease the transition to modern systems, and speed up their simulations
through faster numerical linear algebra routines. This paper discusses the
challenges and benefits for application software in adopting Ginkgo. It will
present examples from different domains, such as CFD, power grid simulation, as
well as electro-cardiophysiology. For these cases, the impact of the
integrations on the application code is discussed from a software engineering
standpoint, and in particular, the approaches taken by Ginkgo and the
applications to enable sustainable software development are highlighted.

</details>


### [8] [When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes](https://arxiv.org/abs/2509.16140)
*Avinash Patil*

Main category: cs.SE

TL;DR: Analysis of bug resolution anomalies in open-source projects using statistical methods and text mining to identify patterns in long-resolution bugs.


<details>
  <summary>Details</summary>
Motivation: To understand why certain bug reports take unusually long to resolve and identify underlying process inefficiencies in software maintenance.

Method: Used Z-score and IQR for anomaly detection in bug resolution times, combined with TF-IDF for textual feature extraction and KMeans clustering to group similar bug summaries across 7 open-source repositories.

Result: Found consistent patterns where anomalies cluster around test failures, enhancement requests, and user interface issues across all studied projects.

Conclusion: The approach provides actionable insights for project maintainers to prioritize and effectively address long-standing bugs, improving software quality and user satisfaction.

Abstract: Efficient bug resolution is critical for maintaining software quality and
user satisfaction. However, specific bug reports experience unusually long
resolution times, which may indicate underlying process inefficiencies or
complex issues. This study presents a comprehensive analysis of bug resolution
anomalies across seven prominent open-source repositories: Cassandra, Firefox,
Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods
such as Z-score and Interquartile Range (IQR), we identify anomalies in bug
resolution durations. To understand the thematic nature of these anomalies, we
apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature
extraction and KMeans clustering to group similar bug summaries. Our findings
reveal consistent patterns across projects, with anomalies often clustering
around test failures, enhancement requests, and user interface issues. This
approach provides actionable insights for project maintainers to prioritize and
effectively address long-standing bugs.

</details>


### [9] [MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair](https://arxiv.org/abs/2509.16187)
*Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening*

Main category: cs.SE

TL;DR: MatchFixAgent is an LLM-based framework for validating and repairing code translations across multiple programming languages using a multi-agent architecture that combines semantic analysis and test execution.


<details>
  <summary>Details</summary>
Motivation: Existing code translation validation methods struggle with generalization across programming languages due to high engineering overhead and reliance on inadequate test suites, leading to false equivalence claims and ineffective repairs.

Method: Multi-agent architecture dividing equivalence validation into sub-tasks: semantic analysis, test writing/execution, translation repair, and final verdict decision based on semantic analyses and test results.

Result: Achieves 99.2% verdict coverage on 2,219 translation pairs across 6 PL pairs, corrects prior work's errors 60.7% of the time, and repairs 50.6% of inequivalent translations (vs 18.5% for prior work).

Conclusion: MatchFixAgent is significantly more adaptable to multiple programming language pairs than prior approaches while producing highly accurate validation results.

Abstract: Code translation transforms source code from one programming language (PL) to
another. Validating the functional equivalence of translation and repairing, if
necessary, are critical steps in code translation. Existing automated
validation and repair approaches struggle to generalize to many PLs due to high
engineering overhead, and they rely on existing and often inadequate test
suites, which results in false claims of equivalence and ineffective
translation repair. We develop MatchFixAgent, a large language model
(LLM)-based, PL-agnostic framework for equivalence validation and repair of
translations. MatchFixAgent features a multi-agent architecture that divides
equivalence validation into several sub-tasks to ensure thorough and consistent
semantic analysis of the translation. Then it feeds this analysis to test agent
to write and execute tests. Upon observing a test failure, the repair agent
attempts to fix the translation bug. The final (in)equivalence decision is made
by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four
repository-level code translation techniques. We use 2,219 translation pairs
from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub
projects totaling over 900K lines of code. Our results demonstrate that
MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,
with the same equivalence validation result as prior work on 72.8% of them.
When MatchFixAgent's result disagrees with prior work, we find that 60.7% of
the time MatchFixAgent's result is actually correct. In addition, we show that
MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior
work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to
many PL pairs than prior work, while producing highly accurate validation
results.

</details>
