{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces is a human-AI collaborative publishing system that reduced time-to-market by 90% and costs by 80%, publishing 52 books in its first year with high quality metrics.", "motivation": "To create a more efficient publishing system that democratizes access to sophisticated publishing capabilities and makes niche markets viable through human-AI collaboration.", "method": "Used configuration-driven architecture, multi-model AI integration, continuous ideation pipeline with tournament-style evaluation, novel codex design, comprehensive automation, publisher personas, and automated verification with human oversight.", "result": "Achieved 90% reduction in time-to-market (2-4 weeks vs 6-12 months), 80% cost reduction, published 52 books in first year, with 99% citation accuracy and 100% validation success after corrections.", "conclusion": "The system demonstrates new paradigms for human-AI collaboration in publishing, making sophisticated publishing capabilities more accessible and enabling previously unviable niche markets."}}
{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "The paper introduces VisCode-Multi-679K dataset, VisPlotBench benchmark, and VisCoder2 models to address limitations in visualization coding agents, achieving 82.4% execution pass rate with iterative self-debug.", "motivation": "Existing LLM-based coding agents fail in practical visualization workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms, constrained by narrow datasets and benchmarks.", "method": "Created three resources: VisCode-Multi-679K (679K validated visualization samples with multi-turn dialogues across 12 languages), VisPlotBench (evaluation benchmark with executable tasks), and VisCoder2 models trained on the dataset with iterative self-debug capabilities.", "result": "VisCoder2 significantly outperforms open-source baselines and approaches GPT-4.1 performance, reaching 82.4% overall execution pass rate at 32B scale, especially in symbolic or compiler-dependent languages.", "conclusion": "The introduced resources and models effectively address key limitations in visualization coding agents, enabling reliable multi-language visualization generation with iterative correction capabilities."}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "Agentsway is a novel software development framework designed for AI agent collaboration, addressing the limitations of traditional human-centric methodologies in agentic AI environments.", "motivation": "Traditional software development methodologies are inadequate for environments where autonomous AI agents contribute to development tasks, creating a methodological gap that needs to be addressed.", "method": "Agentsway introduces a structured lifecycle with distinct roles for planning, prompting, coding, testing, and fine-tuning agents, using fine-tuned LLMs and privacy-preserving collaboration.", "result": "The framework enhances domain-specific reasoning, explainable decision-making, and integrates responsible AI principles through coordinated use of multiple fine-tuned LLMs.", "conclusion": "Agentsway represents a foundational step toward AI-native, self-improving software development methodologies and is the first dedicated methodology for AI agent-based software engineering teams."}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen enhances code security in LLMs by combining RAG with guided self-reflection, achieving significant security improvements across multiple models without requiring resource-intensive fine-tuning.", "motivation": "Security remains a major challenge in code generation using LLMs, and traditional approaches like fine-tuning or creating secure code datasets are resource-intensive.", "method": "RefleXGen integrates Retrieval-Augmented Generation (RAG) with guided self-reflection mechanisms, allowing iterative optimization through self-assessment and reflection without extensive resources.", "result": "Substantial security improvements: 13.6% with GPT-3.5 Turbo, 6.7% with GPT-4o, 4.5% with CodeQwen, and 5.8% with Gemini.", "conclusion": "Improving model self-reflection quality is an effective and practical strategy for enhancing AI-generated code security."}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow is a test-driven agentic workflow that frames repository-scale software engineering as test-resolution, achieving 88.8% pass rate on SWE-Bench Lite and 94.3% on SWE-Bench Verified with minimal test hacking.", "motivation": "To solve human-written tests in repository-scale software engineering by decomposing the complex task into manageable sub-tasks, reducing long-context burden and focusing each sub-agent on specific functions.", "method": "TDFlow uses precisely engineered sub-agents and tightly constrained tools to repeatedly propose, revise, and debug repository-scale patches through four components: patch proposing, debugging, patch revision, and optional test generation.", "result": "Achieved 88.8% pass rate on SWE-Bench Lite (27.8% absolute improvement over next best system) and 94.3% on SWE-Bench Verified, with only 7 instances of test hacking found in 800 runs.", "conclusion": "Modern LLMs in narrowly engineered, test-driven workflows achieve human-level test resolution, with the main remaining challenge being accurate generation of valid reproduction tests for fully autonomous repository repair."}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "LLMs can enable autonomous system interoperability at runtime without human intervention, with qwen2.5-coder:32b showing the best performance across different strategies and dataset versions.", "motivation": "Address the challenge of interoperability in increasingly dynamic and heterogeneous systems of systems, reducing development time efforts for building interoperability artifacts by leveraging LLMs for autonomous runtime interoperability.", "method": "Evaluated 13 open source LLMs using four dataset versions in agricultural interoperability use case, performed three runs per model with two strategies (DIRECT and CODEGEN), and compared effectiveness and consistency across runs.", "result": "qwen2.5-coder:32b was most effective with DIRECT (average pass@1 >= 0.99) and CODEGEN (average pass@1 >= 0.89) in three dataset versions. In fourth version with unit conversion, DIRECT failed but CODEGEN succeeded with qwen2.5-coder:32b achieving average pass@1 = 0.75.", "conclusion": "Some LLMs can achieve autonomous system interoperability, but further evaluation across domains and research on reliability strategies is needed."}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "This paper introduces an alerting extension for OXN observability tool that enables engineers to test and validate alert rules during development, addressing challenges in alert design and reducing runtime issues.", "motivation": "Alert design is challenging due to the need to balance early issue detection with minimizing false alarms, and alerts often cover rare faults that are rarely executed or checked. Current tools lack systematic support for alert validation.", "method": "The paper proposes a new alerting extension for the OXN observability experimentation tool that allows engineers to experiment with alerts during development, tune rules at design time, and routinely validate alert firing behavior.", "result": "The OXN extension enables engineers to test alert rules early in development, helping them tune rules and validate firing behavior before deployment, thus avoiding future runtime problems.", "conclusion": "Systematic testing of alerting code during development using tools like OXN's alerting extension can improve alert reliability and reduce production issues by catching problems early in the development cycle."}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "A lifecycle-aware framework that incorporates intermediate software engineering artifacts (requirements analysis, state machine modeling, pseudocode) into LLM training and inference, improving code generation quality through structured multi-step reasoning.", "motivation": "Most current LLM-based code generation approaches rely on direct single-step translation from problem descriptions to code, disregarding structured software engineering practices and intermediate development phases.", "method": "Introduces a framework that systematically incorporates intermediate artifacts (requirements analysis, state machine modeling, pseudocode) into both training and inference stages, aligning code generation with standard software development lifecycle phases.", "result": "Lifecycle-level fine-tuning improves code correctness by up to 75% over same model before fine-tuning. Multi-step inference consistently surpasses single-step generation. Fine-tuned open-source LLMs match or outperform models pretrained on code. DeepSeek-Coder-1.3B achieves CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and 22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B respectively.", "conclusion": "The framework demonstrates that incorporating structured software engineering practices and intermediate artifacts significantly improves code generation quality, with state machine modeling having the most substantial impact. The approach is robust with reduced training data and enables open-source models to compete with larger proprietary models."}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "Empirical study of ML observability practices through focus groups, cataloging captured information and usage patterns for validation, fault detection, and degradation explanation, while identifying gaps for future tooling.", "motivation": "ML systems fail silently through wrong decisions rather than crashes, and while observability is critical for ML operations, there's a lack of empirical evidence about what practitioners actually capture.", "method": "Conducted seven focus group sessions across several domains to study ML observability practices empirically.", "result": "Cataloged the information practitioners systematically capture across ML systems and their environment, and mapped how they use it for model validation, fault detection/diagnosis, and degradation explanation.", "conclusion": "Identified gaps in current practice and outlined implications for tooling design and research to establish ML observability practices."}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "LLM-generated software shows significant software aging issues including memory growth, increased response times, and performance instability during sustained 50-hour load tests.", "motivation": "To investigate the long-term reliability of LLM-generated software under sustained execution, as automatically generated code is increasingly adopted but little is known about its aging characteristics.", "method": "Generated four service-oriented applications using Bolt platform and Baxbench prompts, then subjected them to 50-hour load tests while monitoring resource usage, response time, and throughput.", "result": "Significant evidence of software aging across all applications: progressive memory growth, increased response time, and performance instability. Statistical analysis confirmed these trends with variability in severity based on application type.", "conclusion": "Software aging must be considered in automatically generated software, and this study provides foundation for future research on mitigation strategies and long-term reliability evaluation."}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "MAGNET is a multi-graph attentional framework for code clone detection that jointly leverages AST, CFG, and DFG representations with residual graph neural networks and attention mechanisms, achieving state-of-the-art performance.", "motivation": "Existing code clone detection methods rely on singular representations (AST, CFG, DFG) that capture only partial code semantics, and hybrid approaches have ineffective handcrafted fusion strategies.", "method": "MAGNET integrates residual graph neural networks with node-level self-attention, introduces gated cross-attention for inter-graph interactions, and uses Set2Set pooling to fuse multi-graph embeddings into unified program-level representations.", "result": "Achieves state-of-the-art performance with F1 scores of 96.5% on BigCloneBench and 99.2% on Google Code Jam datasets.", "conclusion": "The multi-graph fusion and attentional components are critical for performance, demonstrating MAGNET's effectiveness in capturing comprehensive code semantics for clone detection."}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "Survey of 415 developers shows limited overall productivity gains from GenAI tools, revealing a productivity paradox where developers work faster but don't necessarily produce better software or feel more fulfilled.", "motivation": "To investigate how GenAI adoption affects different dimensions of developer productivity, as evidence regarding where and when these tools actually enhance productivity is unclear.", "method": "Surveyed 415 software practitioners to capture their perceptions of productivity changes associated with AI-assisted development using the SPACE framework (Satisfaction and well-being, Performance, Activity, Communication and collaboration, and Efficiency and flow).", "result": "Results disaggregated by frequency of AI usage reveal limited overall productivity change, highlighting the productivity paradox where developers become faster but do not necessarily create better software or feel more fulfilled.", "conclusion": "GenAI tools show limited overall productivity improvements, demonstrating a productivity paradox where increased speed doesn't translate to better software quality or developer satisfaction."}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "PRDBench is a new benchmark for evaluating code agents using 50 real-world Python projects with structured requirements and flexible evaluation metrics beyond unit tests.", "motivation": "Existing code agent benchmarks have high annotation costs and rigid evaluation metrics that primarily rely on unit tests, limiting comprehensive assessment.", "method": "Proposed an agent-driven benchmark construction pipeline with human supervision to generate diverse project-level tasks, and introduced Agent-as-a-Judge paradigm for scoring.", "result": "Created PRDBench with 50 Python projects across 20 domains, featuring structured PRD requirements, comprehensive evaluation criteria, and reference implementations.", "conclusion": "PRDBench provides a scalable and robust framework for evaluating code agents and evaluation agents, addressing limitations of existing benchmarks through flexible metrics and diverse task complexity."}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "LLM-as-a-Judge paradigm uses LLMs for automated evaluation of LLM-generated software artifacts, addressing scalability issues in software engineering evaluation.", "motivation": "The rapid integration of LLMs in software engineering has created a bottleneck in evaluating massive volumes of generated artifacts, as human evaluation is costly and traditional metrics like BLEU are inadequate.", "method": "Literature review of existing SE studies, analysis of limitations, identification of research gaps, and development of a detailed roadmap for advancing LLM-as-a-Judge frameworks.", "result": "Identified that LLM-as-a-Judge research in SE is still in early stages, with current limitations that need to be addressed through systematic research and framework development.", "conclusion": "By 2030, LLM-as-a-Judge frameworks should evolve into reliable, robust, and scalable human surrogates capable of consistent, multi-faceted evaluation of software artifacts, ultimately improving evaluation scalability."}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki is an open-source framework for repository-level documentation that uses hierarchical decomposition, recursive agentic processing, and synthesis of textual/visual artifacts to address limitations of current LLMs in capturing architectural patterns.", "motivation": "Developers spend 58% of time understanding codebases, but existing documentation tools fail at repository level where capturing architectural patterns and cross-module interactions is essential.", "method": "Three innovations: (i) hierarchical decomposition preserving architectural context, (ii) recursive agentic processing with dynamic delegation, (iii) synthesis of textual and visual artifacts including architecture diagrams and data flows.", "result": "Achieves 68.79% quality score with proprietary models and 64.80% with open-source alternatives, outperforming existing closed-source systems on CodeWikiBench benchmark.", "conclusion": "CodeWiki demonstrates scalable, accurate repository-level documentation for real-world repositories across seven programming languages."}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "A cynical reflection on the future of software engineering based on the FUSE 2024 symposium, highlighting three major concerns: incompetent but effective software developers, rapid field evolution causing loss of institutional knowledge, and uncontrolled technology proliferation.", "motivation": "To provide a critical perspective on the current state and future trajectory of software engineering, drawing from discussions at the FUSE 2024 symposium and adding sarcastic commentary.", "method": "Personal reflection and distillation of symposium discussions, incorporating sarcasm and cynicism to analyze the field's challenges.", "result": "Identified three key nightmares facing software engineering: developers who succeed despite incompetence, the field's inability to retain lessons due to rapid change, and exponential technology growth.", "conclusion": "The future of software engineering appears problematic, likened to a slow-motion car crash where problems are visible but unavoidable, suggesting a pessimistic outlook for the field's development."}}
